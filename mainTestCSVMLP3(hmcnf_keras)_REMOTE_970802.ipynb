{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "48f98b91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48f98b91",
        "outputId": "d63c073d-9caa-4831-e06b-d5fb7fe7e8c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/SUMONBDT\n",
            "reading data\n",
            "x.shape: (844538, 22) yOneHot.shape: (844538, 19)\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 11503629369409492942\n",
            "xla_global_id: -1\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11320098816\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 687948869743450733\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "xla_global_id: 416903419\n",
            "]\n",
            "Wed May 18 16:26:18 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    71W / 149W |    151MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#mkdir /content/tmp\n",
        "#%cp -r -f -v /content/drive/MyDrive/SUMONBDT /content/tmp\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/SUMONBDT\n",
        "\n",
        "#用于测试oneHot\n",
        "#也是第一步，读取数据\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "\n",
        "enc = OneHotEncoder()\n",
        "\n",
        "#[2,3,5,9]\n",
        "x1 = [0,0,0,0]\n",
        "x2 = [0,0,0,1]\n",
        "\n",
        "x3 = [1,1,1,2]\n",
        "x4 = [1,1,1,3]\n",
        "x5 = [1,1,2,4]\n",
        "x6 = [1,1,2,5]\n",
        "x7 = [1,2,3,6]\n",
        "x8 = [1,2,3,7]\n",
        "x9 = [1,2,4,8]\n",
        "X = [x1, x2, x3,x4,x5,x6,x7,x8,x9]\n",
        "enc.fit(X)\n",
        "#print(enc.transform(X).toarray())\n",
        "\n",
        "\n",
        "########################读写CSV,并转为oneHot\n",
        "file1 = \"./trainData/dataAllSim.csv\"\n",
        "print(\"reading data\")\n",
        "xyDataTmp = pd.read_csv(file1)\n",
        "#print(xyDataTmp.info())\n",
        "xyData = np.array(xyDataTmp)\n",
        "\n",
        "x = xyData[:,0:22]\n",
        "y = xyData[:,22:26]\n",
        "\n",
        "y = enc.transform(y).toarray()\n",
        "\n",
        "print(\"x.shape:\",x.shape,\"yOneHot.shape:\",y.shape)\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "!nvidia-smi\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03c551ad",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03c551ad",
        "outputId": "540554b1-87a3-4212-d35b-23b3a25e97a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "33/33 [==============================] - 10s 177ms/step - loss: 0.2908 - mae: 0.1632\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 6s 175ms/step - loss: 0.1548 - mae: 0.0922\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 6s 174ms/step - loss: 0.1063 - mae: 0.0647\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 6s 174ms/step - loss: 0.0917 - mae: 0.0565\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 6s 175ms/step - loss: 0.0834 - mae: 0.0515\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 6s 174ms/step - loss: 0.0772 - mae: 0.0480\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 6s 175ms/step - loss: 0.0732 - mae: 0.0455\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 6s 175ms/step - loss: 0.0697 - mae: 0.0435\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 6s 174ms/step - loss: 0.0664 - mae: 0.0417\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 6s 175ms/step - loss: 0.0644 - mae: 0.0406\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 6s 174ms/step - loss: 0.0623 - mae: 0.0393\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 6s 174ms/step - loss: 0.0610 - mae: 0.0385\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 6s 175ms/step - loss: 0.0591 - mae: 0.0374\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 6s 174ms/step - loss: 0.0587 - mae: 0.0371\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 6s 174ms/step - loss: 0.0569 - mae: 0.0361\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 6s 174ms/step - loss: 0.0550 - mae: 0.0351\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 6s 174ms/step - loss: 0.0544 - mae: 0.0346\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0533 - mae: 0.0340\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 6s 177ms/step - loss: 0.0527 - mae: 0.0336\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 6s 174ms/step - loss: 0.0528 - mae: 0.0337\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 6s 174ms/step - loss: 0.0513 - mae: 0.0328\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0506 - mae: 0.0324\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0510 - mae: 0.0326\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0501 - mae: 0.0321\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0505 - mae: 0.0323\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 6s 174ms/step - loss: 0.0489 - mae: 0.0315\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0483 - mae: 0.0311\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0481 - mae: 0.0310\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0479 - mae: 0.0309\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0485 - mae: 0.0311\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 6s 174ms/step - loss: 0.0472 - mae: 0.0305\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 6s 174ms/step - loss: 0.0469 - mae: 0.0303\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 6s 174ms/step - loss: 0.0477 - mae: 0.0307\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0469 - mae: 0.0303\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0469 - mae: 0.0302\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 6s 180ms/step - loss: 0.0459 - mae: 0.0297\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 6s 179ms/step - loss: 0.0457 - mae: 0.0296\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 6s 180ms/step - loss: 0.0457 - mae: 0.0296\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 6s 179ms/step - loss: 0.0457 - mae: 0.0295\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0458 - mae: 0.0295\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0457 - mae: 0.0296\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 6s 174ms/step - loss: 0.0452 - mae: 0.0292\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0447 - mae: 0.0290\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0447 - mae: 0.0289\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0449 - mae: 0.0290\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0435 - mae: 0.0283\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0440 - mae: 0.0285\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 6s 178ms/step - loss: 0.0438 - mae: 0.0285\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0458 - mae: 0.0292\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0436 - mae: 0.0283\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0429 - mae: 0.0280\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0436 - mae: 0.0282\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0436 - mae: 0.0282\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0428 - mae: 0.0278\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0424 - mae: 0.0276\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0424 - mae: 0.0276\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0426 - mae: 0.0276\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0423 - mae: 0.0275\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0423 - mae: 0.0275\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 6s 174ms/step - loss: 0.0422 - mae: 0.0274\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0427 - mae: 0.0277\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 6s 174ms/step - loss: 0.0419 - mae: 0.0273\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0426 - mae: 0.0275\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0435 - mae: 0.0279\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0412 - mae: 0.0269\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0421 - mae: 0.0274\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0415 - mae: 0.0269\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0409 - mae: 0.0267\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0424 - mae: 0.0273\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0414 - mae: 0.0269\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 6s 173ms/step - loss: 0.0417 - mae: 0.0271\n",
            "Epoch 72/100\n",
            "14/33 [===========>..................] - ETA: 3s - loss: 0.0409 - mae: 0.0267"
          ]
        }
      ],
      "source": [
        "#1. 核心为keras220不是pytorch\n",
        "#2. 基于hmcnf\n",
        "#第二步，训练\n",
        "import model_hmcnf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "#hierarchy = [18, 80, 178, 142, 77, 4]\n",
        "hierarchy = [2,3,5,9]\n",
        "features_size = x.shape[1]\n",
        "label_size = y.shape[1]\n",
        "beta = 0.2\n",
        "dropout_rate=0.1\n",
        "relu_size=384\n",
        "\n",
        "\n",
        "\n",
        "def local_model(num_labels, dropout_rate, relu_size):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(relu_size, activation='relu'))\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.Dense(num_labels, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "\n",
        "def global_model(dropout_rate, relu_size):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(relu_size, activation='relu'))\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    return model\n",
        "\n",
        "\n",
        "def sigmoid_model(label_size):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(label_size, activation='sigmoid',name=\"global\"))\n",
        "    return model\n",
        "\n",
        "features = layers.Input(shape=(features_size,))\n",
        "global_models = []\n",
        "local_models = []\n",
        "\n",
        "\n",
        "for i in range(len(hierarchy)):\n",
        "    if i == 0:\n",
        "        global_models.append(global_model(dropout_rate, relu_size)(features))\n",
        "    else:\n",
        "        global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
        "\n",
        "p_glob = sigmoid_model(label_size)(global_models[-1])\n",
        "\n",
        "\n",
        "#显示只有全局模型的情况\n",
        "#modelTmp1 = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
        "#modelTmp1.summary()#\n",
        "#plot_model(modelTmp1, to_file='Flatten1.png', show_shapes=True)\n",
        "\n",
        "\n",
        "for i in range(len(hierarchy)):\n",
        "    local_models.append(local_model(hierarchy[i], dropout_rate, relu_size)(global_models[i]))\n",
        "    \n",
        "#显示只有局部局模型的情况(部分全局)\n",
        "p_loc = layers.concatenate(local_models)\n",
        "#modelTmp2 = tf.keras.Model(inputs=[features], outputs=[p_loc])\n",
        "#modelTmp2.summary()#\n",
        "#plot_model(modelTmp2, to_file='Flatten2.png', show_shapes=True)\n",
        "p_glob1 = layers.Lambda(lambda x: x*beta,name=\"global\")(p_glob)\n",
        "p_loc1 = layers.Lambda(lambda x: x*(1-beta),name=\"local\")(p_loc)\n",
        "\n",
        "labels = layers.add([p_glob1, p_loc1])\n",
        "\n",
        "model = tf.keras.Model(inputs=[features], outputs=[labels])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plot_model(model, to_file='FlattenAll.png', show_shapes=True)\n",
        "\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['mae'])\n",
        "\n",
        "\n",
        "\n",
        "model.fit([x],[y],epochs=100, batch_size=25600)\n",
        "\n",
        "model.save(\"hmcnf.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d7beeed",
      "metadata": {
        "id": "4d7beeed"
      },
      "outputs": [],
      "source": [
        "#第三步，验证\n",
        "import model_hmcnf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#######################0.准备onehot\n",
        "enc = OneHotEncoder()\n",
        "#[2,3,5,9]\n",
        "x1 = [0,0,0,0]\n",
        "x2 = [0,0,0,1]\n",
        "\n",
        "x3 = [1,1,1,2]\n",
        "x4 = [1,1,1,3]\n",
        "x5 = [1,1,2,4]\n",
        "x6 = [1,1,2,5]\n",
        "x7 = [1,2,3,6]\n",
        "x8 = [1,2,3,7]\n",
        "x9 = [1,2,4,8]\n",
        "X = [x1, x2, x3,x4,x5,x6,x7,x8,x9]\n",
        "enc.fit(X)\n",
        "\n",
        "#######################2.准备数据\n",
        "        \n",
        "file1 = \"./trainData/dataAllSim.csv\"\n",
        "print(\"reading data\")\n",
        "xyDataTmp = pd.read_csv(file1)\n",
        "#print(xyDataTmp.info())\n",
        "xyData = np.array(xyDataTmp)\n",
        "\n",
        "x = xyData[:,0:22]\n",
        "y = xyData[:,22:26]\n",
        "ylabel = y\n",
        "y = enc.transform(y).toarray()\n",
        "\n",
        "\n",
        "\n",
        "#######################3.预测模型\n",
        "print(\"3.HMCNF预测模型\")\n",
        "hierarchy = [2,3,5,9]\n",
        "features_size = x.shape[1]\n",
        "label_size = y.shape[1]\n",
        "beta = 0.2\n",
        "\n",
        "model_name =\"hmcnf.h5\" \n",
        "\n",
        "model = keras.models.load_model(model_name)\n",
        "y_out = model.predict([x], batch_size=2560)\n",
        "y_predict = np.where(y_out > 0.5, 1, 0)\n",
        "\n",
        "predict_ok = np.where(np.sum(y_predict - y, axis=1) == 0, 1, 0)\n",
        "\n",
        "\n",
        "print(\"validated {} , {} good out of {} samples\".format(model_name, np.sum(predict_ok), predict_ok.shape[0]))\n",
        "#######################3.层次预测预测模型\n",
        "print(\"3.层次预测预测模型\")\n",
        "y1 = np.where(y_out[:,0:2] > 0.5, 1, 0)\n",
        "y2 = np.where(y_out[:,2:5] > 0.5, 1, 0)\n",
        "y3 = np.where(y_out[:,5:10] > 0.5, 1, 0)\n",
        "y4 = np.where(y_out[:,10:19] > 0.5, 1, 0)\n",
        "for i in range(y4.shape[0]):\n",
        "    tmp1 = y1[i]\n",
        "    tmp2 = y2[i]\n",
        "    tmp3 = y3[i]\n",
        "    tmp4 = y4[i]\n",
        "    if sum(tmp1) == 0:\n",
        "        index=  np.argmax(tmp1)\n",
        "        y1[i,index]=1\n",
        "        \n",
        "    if sum(tmp2) == 0:\n",
        "        index=  np.argmax(tmp2)\n",
        "        y2[i,index]=1\n",
        "        \n",
        "    if sum(tmp3) == 0:\n",
        "        index=  np.argmax(tmp3)\n",
        "        y3[i,index]=1\n",
        "    \n",
        "    if sum(tmp4) == 0:\n",
        "        index=  np.argmax(tmp4)\n",
        "        y4[i,index]=1\n",
        "        #print(i,y4[i],index)\n",
        "y_predict = np.concatenate([y1,y2,y3,y4],axis=1)\n",
        "predict_ok = np.where(np.sum(y_predict - y, axis=1) == 0, 1, 0)\n",
        "print(\"validated {} , {} good out of {} samples\".format(model_name, np.sum(predict_ok), predict_ok.shape[0]))\n",
        "\n",
        "#######################4.评估层次模型\n",
        "#hierarchy = [2,3,5,9]\n",
        "ypredict = enc.inverse_transform(y_predict)\n",
        "\n",
        "##第一层，2\n",
        "print(\"###################################第一层，2\")\n",
        "h1_yp = ypredict[:,0]\n",
        "h1_yl = ylabel[:,0]\n",
        "tmp1 = classification_report(h1_yl,h1_yp)\n",
        "tmp2 = confusion_matrix(h1_yl,h1_yp,normalize='true')\n",
        "tmp3 = confusion_matrix(h1_yl,h1_yp,normalize='pred')\n",
        "print(tmp1)\n",
        "print(np.around(tmp2, decimals=3))\n",
        "print(np.around(tmp3, decimals=3))\n",
        "\n",
        "\n",
        "##第二层，3\n",
        "print(\"################################第二层，3\")\n",
        "h2_yp = ypredict[:,1]\n",
        "h2_yl = ylabel[:,1]\n",
        "tmp1 = classification_report(h2_yl,h2_yp)\n",
        "tmp2 = confusion_matrix(h2_yl,h2_yp,normalize='true')\n",
        "tmp3 = confusion_matrix(h2_yl,h2_yp,normalize='pred')\n",
        "print(tmp1)\n",
        "print(np.around(tmp2, decimals=3))\n",
        "print(np.around(tmp3, decimals=3))\n",
        "\n",
        "\n",
        "\n",
        "##第三层，5\n",
        "print(\"#############################第三层，5\")\n",
        "h3_yp = ypredict[:,2]\n",
        "h3_yl = ylabel[:,2]\n",
        "tmp1 = classification_report(h3_yl,h3_yp)\n",
        "tmp2 = confusion_matrix(h3_yl,h3_yp,normalize='true')\n",
        "tmp3 = confusion_matrix(h3_yl,h3_yp,normalize='pred')\n",
        "print(tmp1)\n",
        "print(np.around(tmp2, decimals=3))\n",
        "print(np.around(tmp3, decimals=3))\n",
        "\n",
        "\n",
        "##第四层，9\n",
        "print(\"#############################第四层，9\")\n",
        "h4_yp = ypredict[:,3]\n",
        "h4_yl = ylabel[:,3]\n",
        "tmp1 = classification_report(h4_yl,h4_yp)\n",
        "tmp2 = confusion_matrix(h4_yl,h4_yp,normalize='true')\n",
        "tmp3 = confusion_matrix(h4_yl,h4_yp,normalize='pred')\n",
        "print(tmp1)\n",
        "print(np.around(tmp2, decimals=3))\n",
        "print(np.around(tmp3, decimals=3))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b6f76df",
      "metadata": {
        "id": "1b6f76df"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/SUMONBDT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/SUMONBDT/trainData/\n",
        "!unzip dataAllSim1000.zip  -d /content/drive/MyDrive/SUMONBDT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w49dSZIVHhE",
        "outputId": "2f6939a5-48fc-488d-d5c4-8db27194c3c7"
      },
      "id": "6w49dSZIVHhE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/SUMONBDT/trainData\n",
            "Archive:  dataAllSim1000.zip\n",
            "  inflating: /content/drive/MyDrive/SUMONBDT/trainData/dataAllSim.csv  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "mainTestCSVMLP3(hmcnf_keras).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}