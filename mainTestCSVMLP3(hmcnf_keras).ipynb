{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16626293",
   "metadata": {
    "id": "48f98b91"
   },
   "outputs": [],
   "source": [
    "#mkdir /content/tmp\n",
    "#%cp -r -f -v /content/drive/MyDrive/SUMONBDT /content/tmp\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#%cd /content/drive/MyDrive/SUMONBDT\n",
    "#%cd /home/liuli/github/SUMONBDT\n",
    "#!nvidia-smi\n",
    "#用于测试oneHot\n",
    "#############################################################也是第一步，读取数据\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "#[2,3,5,9]\n",
    "x1 = [0,0,0,0]\n",
    "x2 = [0,0,0,1]\n",
    "\n",
    "x3 = [1,1,1,2]\n",
    "x4 = [1,1,1,3]\n",
    "x5 = [1,1,2,4]\n",
    "x6 = [1,1,2,5]\n",
    "x7 = [1,2,3,6]\n",
    "x8 = [1,2,3,7]\n",
    "x9 = [1,2,4,8]\n",
    "X = [x1, x2, x3,x4,x5,x6,x7,x8,x9]\n",
    "enc.fit(X)\n",
    "#print(enc.transform(X).toarray())\n",
    "\n",
    "\n",
    "########################读写CSV,并转为oneHot\n",
    "file1 = \"./trainData/dataAllSim1000.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "ylabel = y\n",
    "y = enc.transform(y).toarray()\n",
    "\n",
    "print(\"x.shape:\",x.shape,\"yOneHot.shape:\",y.shape)\n",
    "\n",
    "\n",
    "\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bded2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "03c551ad",
    "outputId": "e1da57c0-4dd7-440a-bf3f-6194c50c0c1c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "################################################################第二步，训练\n",
    "#1. 核心为keras220不是pytorch\n",
    "#2. 基于hmcnf\n",
    "import model_hmcnf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "#hierarchy = [18, 80, 178, 142, 77, 4]\n",
    "hierarchy = [2,3,5,9]\n",
    "features_size = x.shape[1]\n",
    "label_size = y.shape[1]\n",
    "beta = 0.2\n",
    "dropout_rate=0.1\n",
    "relu_size=384\n",
    "\n",
    "\n",
    "\n",
    "def local_model(num_labels, dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(num_labels, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_model(dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    return model\n",
    "\n",
    "\n",
    "def sigmoid_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='sigmoid',name=\"global\"))\n",
    "    return model\n",
    "\n",
    "features = layers.Input(shape=(features_size,))\n",
    "global_models = []\n",
    "local_models = []\n",
    "\n",
    "\n",
    "for i in range(len(hierarchy)):\n",
    "    if i == 0:\n",
    "        global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "    else:\n",
    "        global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "\n",
    "p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "\n",
    "\n",
    "#显示只有全局模型的情况\n",
    "#modelTmp1 = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "#modelTmp1.summary()#\n",
    "#plot_model(modelTmp1, to_file='Flatten1.png', show_shapes=True)\n",
    "\n",
    "\n",
    "for i in range(len(hierarchy)):\n",
    "    local_models.append(local_model(hierarchy[i], dropout_rate, relu_size)(global_models[i]))\n",
    "    \n",
    "#显示只有局部局模型的情况(部分全局)\n",
    "p_loc = layers.concatenate(local_models)\n",
    "#modelTmp2 = tf.keras.Model(inputs=[features], outputs=[p_loc])\n",
    "#modelTmp2.summary()#\n",
    "#plot_model(modelTmp2, to_file='Flatten2.png', show_shapes=True)\n",
    "p_glob1 = layers.Lambda(lambda x: x*beta,name=\"global\")(p_glob)\n",
    "p_loc1 = layers.Lambda(lambda x: x*(1-beta),name=\"local\")(p_loc)\n",
    "\n",
    "labels = layers.add([p_glob1, p_loc1])\n",
    "\n",
    "model = tf.keras.Model(inputs=[features], outputs=[labels])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_model(model, to_file='FlattenAll.png', show_shapes=True)\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['mae'])\n",
    "model.fit([x],[y],epochs=1000, batch_size=25600*1)\n",
    "model.save(\"hmcnf10000.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1b25d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d7beeed",
    "outputId": "d6908600-8597-41c2-800f-afc59a088154",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##################################################################第三步，验证\n",
    "#%cd /content/drive/MyDrive/SUMONBDT\n",
    "import model_hmcnf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#######################0.准备onehot\n",
    "enc = OneHotEncoder()\n",
    "#[2,3,5,9]\n",
    "x1 = [0,0,0,0]\n",
    "x2 = [0,0,0,1]\n",
    "\n",
    "x3 = [1,1,1,2]\n",
    "x4 = [1,1,1,3]\n",
    "x5 = [1,1,2,4]\n",
    "x6 = [1,1,2,5]\n",
    "x7 = [1,2,3,6]\n",
    "x8 = [1,2,3,7]\n",
    "x9 = [1,2,4,8]\n",
    "X = [x1, x2, x3,x4,x5,x6,x7,x8,x9]\n",
    "enc.fit(X)\n",
    "\n",
    "#######################2.准备数据\n",
    "        \n",
    "file1 = \"./trainData/dataAllSim10000.csv\"\n",
    "file1 = \"./trainData/dataAllSim.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "ylabel = y\n",
    "y = enc.transform(y).toarray()\n",
    "\n",
    "\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "#######################3.预测模型\n",
    "print(\"3.HMCNF预测模型\")\n",
    "hierarchy = [2,3,5,9]\n",
    "features_size = x.shape[1]\n",
    "label_size = y.shape[1]\n",
    "beta = 0.2\n",
    "\n",
    "model_name =\"hmcnf.h5\" \n",
    "\n",
    "model = keras.models.load_model(model_name)\n",
    "y_out = model.predict([x], batch_size=2560)\n",
    "y_predict = np.where(y_out > 0.5, 1, 0)\n",
    "\n",
    "predict_ok = np.where(np.sum(y_predict - y, axis=1) == 0, 1, 0)\n",
    "\n",
    "\n",
    "print(\"validated {} , {} good out of {} samples\".format(model_name, np.sum(predict_ok), predict_ok.shape[0]))\n",
    "del y_predict #节省内存\n",
    "del predict_ok #节省内存\n",
    "#######################3.层次预测预测模型\n",
    "print(\"3.层次预测预测模型\")\n",
    "y1 = np.where(y_out[:,0:2] > 0.5, 1, 0)\n",
    "y2 = np.where(y_out[:,2:5] > 0.5, 1, 0)\n",
    "y3 = np.where(y_out[:,5:10] > 0.5, 1, 0)\n",
    "y4 = np.where(y_out[:,10:19] > 0.5, 1, 0)\n",
    "for i in range(y4.shape[0]):\n",
    "    tmp1 = y1[i]\n",
    "    tmp2 = y2[i]\n",
    "    tmp3 = y3[i]\n",
    "    tmp4 = y4[i]\n",
    "    if sum(tmp1) == 0:\n",
    "        index=  np.argmax(tmp1)\n",
    "        y1[i,index]=1\n",
    "        \n",
    "    if sum(tmp2) == 0:\n",
    "        index=  np.argmax(tmp2)\n",
    "        y2[i,index]=1\n",
    "        \n",
    "    if sum(tmp3) == 0:\n",
    "        index=  np.argmax(tmp3)\n",
    "        y3[i,index]=1\n",
    "    \n",
    "    if sum(tmp4) == 0:\n",
    "        index=  np.argmax(tmp4)\n",
    "        y4[i,index]=1\n",
    "        #print(i,y4[i],index)\n",
    "y_predict = np.concatenate([y1,y2,y3,y4],axis=1)\n",
    "predict_ok = np.where(np.sum(y_predict - y, axis=1) == 0, 1, 0)\n",
    "print(\"validated {} , {} good out of {} samples\".format(model_name, np.sum(predict_ok), predict_ok.shape[0]))\n",
    "\n",
    "#onehot 2 label\n",
    "ypredict = enc.inverse_transform(y_predict)\n",
    "del y_predict #节省内存\n",
    "del predict_ok #节省内存\n",
    "del y1,y2,y3,y4\n",
    "#######################4.评估层次模型\n",
    "#hierarchy = [2,3,5,9]\n",
    "\n",
    "##第一层，2\n",
    "print(\"###################################第一层，2\")\n",
    "h1_yp = ypredict[:,0]\n",
    "h1_yl = ylabel[:,0]\n",
    "tmp1 = classification_report(h1_yl,h1_yp)\n",
    "tmp2 = confusion_matrix(h1_yl,h1_yp,normalize='true')\n",
    "tmp3 = confusion_matrix(h1_yl,h1_yp,normalize='pred')\n",
    "print(tmp1)\n",
    "print(np.around(tmp2, decimals=3))\n",
    "print(np.around(tmp3, decimals=3))\n",
    "\n",
    "\n",
    "##第二层，3\n",
    "print(\"################################第二层，3\")\n",
    "h2_yp = ypredict[:,1]\n",
    "h2_yl = ylabel[:,1]\n",
    "tmp1 = classification_report(h2_yl,h2_yp)\n",
    "tmp2 = confusion_matrix(h2_yl,h2_yp,normalize='true')\n",
    "tmp3 = confusion_matrix(h2_yl,h2_yp,normalize='pred')\n",
    "print(tmp1)\n",
    "print(np.around(tmp2, decimals=3))\n",
    "print(np.around(tmp3, decimals=3))\n",
    "\n",
    "\n",
    "\n",
    "##第三层，5\n",
    "print(\"#############################第三层，5\")\n",
    "h3_yp = ypredict[:,2]\n",
    "h3_yl = ylabel[:,2]\n",
    "tmp1 = classification_report(h3_yl,h3_yp)\n",
    "tmp2 = confusion_matrix(h3_yl,h3_yp,normalize='true')\n",
    "tmp3 = confusion_matrix(h3_yl,h3_yp,normalize='pred')\n",
    "print(tmp1)\n",
    "print(np.around(tmp2, decimals=3))\n",
    "print(np.around(tmp3, decimals=3))\n",
    "\n",
    "\n",
    "##第四层，9\n",
    "print(\"#############################第四层，9\")\n",
    "h4_yp = ypredict[:,3]\n",
    "h4_yl = ylabel[:,3]\n",
    "tmp1 = classification_report(h4_yl,h4_yp)\n",
    "tmp2 = confusion_matrix(h4_yl,h4_yp,normalize='true')\n",
    "tmp3 = confusion_matrix(h4_yl,h4_yp,normalize='pred')\n",
    "print(tmp1)\n",
    "print(np.around(tmp2, decimals=3))\n",
    "print(np.around(tmp3, decimals=3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4278841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#第四步，根据混淆矩阵进行聚类。第一列代表识别为类别1的样本真实的类别分布\n",
    "import numpy as np\n",
    "import copy\n",
    "#########################################手动准备模拟数据\n",
    "mat1 = np.array([[0.952,0.004,0.015,0.008],\n",
    " [0.018,0.923,0.016,0.032],\n",
    " [0.016,0.036,0.934,0.047],\n",
    " [0.014,0.037,0.035,0.913]])\n",
    "accy = [mat1[0,0],mat1[1,1],mat1[2,2],mat1[3,3]]\n",
    "print(\"accuracy\",accy)\n",
    "matT1 = mat1\n",
    "mat1[:,0] = matT1[:,0]*1000\n",
    "mat1[:,1] = matT1[:,1]*1000\n",
    "mat1[:,2] = matT1[:,2]*1000\n",
    "mat1[:,3] = matT1[:,3]*1000\n",
    "sumTmp =  sum(mat1)\n",
    "print(sumTmp)\n",
    "print(mat1)\n",
    "\n",
    "##########################################计算最佳合并位置，根据最大的正确率提高\n",
    "def computeAccuracyDiff(mat1,accy):\n",
    "    h,w = mat1.shape\n",
    "    tmp = np.zeros((h-1,w-1))\n",
    "    matTmp={}\n",
    "    ##从0到最后，行列合并\n",
    "    for index in range(h-1):\n",
    "\n",
    "        tmp = np.zeros((h-1,w))\n",
    "        num = 0\n",
    "        ####行合并\n",
    "        for i in range(h):#行合并\n",
    "            if i == index:\n",
    "                tmp[num]=mat1[i]+mat1[i+1]\n",
    "                num=num+1\n",
    "                continue\n",
    "            if i== index+1:\n",
    "                continue\n",
    "\n",
    "            tmp[num]=mat1[i]\n",
    "            num=num+1\n",
    "\n",
    "        ####列合并   \n",
    "        mat2=tmp\n",
    "        tmp = np.zeros((h-1,w-1))\n",
    "        num = 0\n",
    "        for j in range(w):#列合并\n",
    "            if j == index:\n",
    "                tmp[:,num] = mat2[:,j]+mat2[:,j+1]\n",
    "                num=num+1\n",
    "                continue\n",
    "            if j== index+1:\n",
    "                continue\n",
    "\n",
    "            tmp[:,num] = mat2[:,j]\n",
    "            num=num+1\n",
    "        matTmp[index] = tmp\n",
    "        \n",
    "        #print(\"合并后的所有矩阵\")\n",
    "        #print(index,matTmp[index])#合并后的所有矩阵\n",
    "    matTmp1 = copy.deepcopy(matTmp)\n",
    "   \n",
    "    ##归一化   \n",
    "    maxDiffMat = np.zeros((len(matTmp),1))\n",
    "    for i in range(len(matTmp)):\n",
    "        tmp = matTmp[i]\n",
    "        sumTmp =  sum(tmp)\n",
    "        for j in range(tmp.shape[1]):\n",
    "            tmp[:,j] = tmp[:,j]/(sumTmp[j])\n",
    "\n",
    "        accyNow = tmp[i,i]\n",
    "        maxDiffMat[i]= max(accyNow-accy[i],accyNow-accy[i+1])\n",
    "    maxIndex = np.argmax(maxDiffMat)\n",
    "    maxDiff = max(maxDiffMat)\n",
    "    #print(matTmp1)\n",
    "    return maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1\n",
    "\n",
    "#为了思考，不用for循环，直接一步一步做\n",
    "#4到3\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(matTmp1[maxIndex])\n",
    "\n",
    "#3到2\n",
    "mat3to2=matTmp1[maxIndex]\n",
    "accy3to2 = [matTmp[maxIndex][0,0],matTmp[maxIndex][1,1],matTmp[maxIndex][2,2]]\n",
    "maxIndex3to2,maxDiff3to2,maxDiffMat3to2,matTmp3to2,matTmp3to2Origin =computeAccuracyDiff(mat3to2,accy3to2)\n",
    "chosedMat = matTmp3to2Origin[maxIndex3to2]\n",
    "print(\"最佳合并点和矩阵\",maxIndex3to2,maxDiff3to2)\n",
    "print(matTmp3to2Origin[maxIndex3to2])\n",
    "print(matTmp3to2[maxIndex3to2])\n",
    "\n",
    "\n",
    "#########################################采用数据进行分析\n",
    "print(\"\\n\\n\\n###################################################\")\n",
    "print(\"\\n\\n\\n 用第5层数据进行分析\")\n",
    "h3_yp = ypredict[:,2]\n",
    "h3_yl = ylabel[:,2]\n",
    "mat1 = confusion_matrix(h3_yl,h3_yp)\n",
    "p1 = confusion_matrix(h3_yl,h3_yp,normalize='pred')\n",
    "sumTmp = sum(mat1)\n",
    "print(mat1)\n",
    "print(sumTmp)\n",
    "print(np.around(p1, decimals=3))\n",
    "\n",
    "########5->4\n",
    "accy = [p1[0,0],p1[1,1],p1[2,2],p1[3,3],p1[4,4]]\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"\\n\\n\\n5->4,最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(matTmp1[maxIndex])\n",
    "print(np.around(matTmp[maxIndex], decimals=3))\n",
    "\n",
    "########4->3\n",
    "mat1=matTmp1[maxIndex]\n",
    "accy= [matTmp[maxIndex][0,0],matTmp[maxIndex][1,1],matTmp[maxIndex][2,2],matTmp[maxIndex][3,3]]\n",
    "\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"\\n\\n\\n4->3,最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(matTmp1[maxIndex])\n",
    "print(np.around(matTmp[maxIndex], decimals=3))\n",
    "\n",
    "\n",
    "########3->2\n",
    "mat1=matTmp1[maxIndex]\n",
    "accy= [matTmp[maxIndex][0,0],matTmp[maxIndex][1,1],matTmp[maxIndex][2,2]]\n",
    "\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"\\n\\n\\n3->2,最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(matTmp1[maxIndex])\n",
    "print(np.around(matTmp[maxIndex], decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f88c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##测试最简注意力机制，Attention Channel ,SEAttention\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils.vis_utils import plot_model\n",
    "np.random.seed(1337)  # for reproducibility\n",
    " \n",
    "from keras.models import *\n",
    "from keras.layers import Input, Dense,Multiply,Activation\n",
    " \n",
    "input_dim = 4\n",
    "\n",
    "\n",
    "def get_data(n, input_dim, attention_column=1):\n",
    "\n",
    "    x = np.random.standard_normal(size=(n, input_dim))\n",
    "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
    "    x[:, attention_column] = y[:, 0]\n",
    "    return x, y\n",
    "\n",
    " \n",
    " \n",
    "def Att(att_dim,inputs,name):\n",
    "    V = inputs\n",
    "    QK = Dense(att_dim,bias=None)(inputs)\n",
    "    QK = Activation(\"softmax\",name=name)(QK)\n",
    "    MV = Multiply()([V, QK])\n",
    "    return(MV)\n",
    " \n",
    " \n",
    "def build_model():\n",
    "    inputs = Input(shape=(input_dim,))\n",
    " \n",
    "    atts1 = Att(input_dim,inputs,\"attention_vec\")\n",
    " \n",
    "    x = Dense(16)(atts1)\n",
    "    atts2 = Att(16,x,\"attention_vec1\")\n",
    " \n",
    " \n",
    "    output = Dense(1, activation='sigmoid')(atts2)\n",
    "    model = Model(input=inputs, output=output)\n",
    "    return model\n",
    "\n",
    "N = 10000\n",
    "inputs_1, outputs = get_data(N, input_dim) \n",
    "print(inputs_1)\n",
    " \n",
    "m = build_model()\n",
    "plot_model(m, to_file='attMap.png', show_shapes=True)\n",
    "#m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#print(m.summary())\n",
    "#m.fit(inputs_1, outputs, epochs=20, batch_size=128, validation_split=0.2)testing_inputs_1, testing_outputs = get_data(1, input_dim)\n",
    "\n",
    "\n",
    "#原文链接：https://blog.csdn.net/xiaosongshine/article/details/90579679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7d2516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.csdn.net/SKIp121whats112/article/details/122265766\n",
    "#https://scikit-learn.org/stable/modules/tree.html\n",
    "##############测试决策树\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "Input= x\n",
    "Output = ylabel[:,2]\n",
    "print(x)\n",
    "print(Output)\n",
    "dt = tree.DecisionTreeClassifier(max_depth=5,min_samples_split=100,min_samples_leaf=100,min_impurity_decrease=0.001)\n",
    "dt = dt.fit(Input, Output)\n",
    "tree.plot_tree(dt)\n",
    "data=tree.export_graphviz(dt, out_file=None,class_names=['0','1','2','3','4'],filled=True) \n",
    "graph = graphviz.Source(data)\n",
    "graph.render(\"now\")\n",
    "\n",
    "data=tree.export_graphviz(dt, out_file=None,class_names=['0','1','2','3','4'],filled=True,proportion=True) \n",
    "graph = graphviz.Source(data)\n",
    "graph.render(\"nowPercent\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "OutPredict = dt.predict(Input)\n",
    "\n",
    "tmp1 = classification_report(Output,OutPredict )\n",
    "tmp2 = confusion_matrix(Output,OutPredict ,normalize='true')\n",
    "tmp3 = confusion_matrix(Output,OutPredict ,normalize='pred')\n",
    "print(tmp1)\n",
    "print(np.around(tmp2, decimals=3))\n",
    "print(np.around(tmp3, decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c051664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试决策树的特征\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "###测试权重\n",
    "nSamples =5000\n",
    "input_dim = 10\n",
    "#x = np.random.standard_normal(size=(nSamples, input_dim))\n",
    "x = np.random.randint(low=0, high=10, size=(nSamples, input_dim))\n",
    "y1 = np.zeros((nSamples, 1))#>50\n",
    "y1A = np.zeros((nSamples, 1))#>50 and <60\n",
    "y1B = np.zeros((nSamples, 1))#>=60\n",
    "sumX = np.sum(x,axis=1)\n",
    "index=np.where(sumX>40)\n",
    "y1[index]=1\n",
    "index=np.where((sumX>50)& (sumX<70))\n",
    "y1A[index]=1\n",
    "index=np.where(sumX>=70)\n",
    "y1B[index]=1\n",
    "\n",
    "##数据来源2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "file1 = \"./trainData/dataAllSim1000.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "xyData = np.array(xyDataTmp)\n",
    "nSamples, nDims= xyData.shape\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "ylabel = y\n",
    "y1= y[:,0]\n",
    "\n",
    "\n",
    "##################################################################\n",
    "#测试决策树\n",
    "def dtFitAndSave(x,y,class_names1,saveName):\n",
    "    dt = tree.DecisionTreeClassifier(max_depth=7,min_samples_split=100,min_samples_leaf=100,min_impurity_split=0.06,ccp_alpha=0.001)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    data=tree.export_graphviz(dt, out_file=None,class_names=class_names1,filled=True) \n",
    "    graph = graphviz.Source(data)\n",
    "    graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(tmp1)\n",
    "    text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    \n",
    "    yPredict = dt.predict_proba(x[0:3,:])\n",
    "    print(yPredict[:,1])\n",
    "    d_path = dt.decision_path(x[0:3,:]).todense()\n",
    "    print(d_path)\n",
    "    print(\"impurity\",dt.tree_.impurity)\n",
    "    print(\"feature\",dt.tree_.feature)\n",
    "    print(\"threshold\",dt.tree_.threshold)\n",
    "    \n",
    "    #左节点编号  :  clf.tree_.children_left\n",
    "    #右节点编号  :  clf.tree_.children_right\n",
    "    #分割的变量  :  clf.tree_.feature\n",
    "    #分割的阈值  :  clf.tree_.threshold\n",
    "    #不纯度(gini) :  clf.tree_.impurity\n",
    "    #样本个数      :  clf.tree_.n_node_samples\n",
    "    #样本分布      :  clf.tree_.value\n",
    "    #https://blog.csdn.net/ywj_1991/article/details/122985778\n",
    "    #https://www.javaroad.cn/questions/54003\n",
    "\n",
    "    w,h = d_path.shape\n",
    "    for i in range(h):\n",
    "       path = d_path[i]\n",
    "       v,ind = np.where(path>0)\n",
    "       xtmp = x[i]\n",
    "       #print(\"path\",path,ind,np.array(ind)[-1])\n",
    "    \n",
    "       print(\"\\n index\",index)\n",
    "       print(\"impurity\",dt.tree_.impurity[ind])\n",
    "       print(\"feature\",dt.tree_.feature[ind])\n",
    "       print(\"threshold\",dt.tree_.threshold[ind])\n",
    "       print(\"x[index]\",xtmp[ind])\n",
    "       \n",
    "      \n",
    "       print(\"the leaf node:\",np.array(ind)[-1],\"the simplest rule is\")\n",
    "       for jj in ind:\n",
    "           if dt.tree_.feature[jj] == -2:\n",
    "                print(\"label,proba is\",yPredict[i,0],yPredict[i,1])\n",
    "                break\n",
    "                \n",
    "           if xtmp[jj]<=dt.tree_.threshold[jj]:\n",
    "              print(\" x[%d]<=%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "           else:\n",
    "              print(\" x[%d]>%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "                    \n",
    "       finalPos = np.array(ind)[-1]\n",
    "       print(dt.tree_.impurity[finalPos])\n",
    "       print(dt.tree_.feature[finalPos])\n",
    "       print(dt.tree_.threshold[finalPos])\n",
    "\n",
    "dtFitAndSave(x,y1,[\"0\",\"1\"],\"bigger\")\n",
    "\n",
    "###################################################################################\n",
    "#测试神经网络\n",
    "def kerasFitAndSave(x,y,num_labels):\n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 384\n",
    "    dropout_rate =0.1\n",
    "    models=[]\n",
    "    \n",
    "    build_model = tf.keras.Sequential()\n",
    "   \n",
    "    build_model.add(layers.Dense(relu_size, activation='relu',name=\"layer1\",input_shape=(features_size,)))\n",
    "    build_model.add(layers.Dropout(dropout_rate,name=\"Dropout1-2\"))\n",
    "    build_model.add(layers.Dense(num_labels, activation='sigmoid',name=\"layer2\"))\n",
    "    \n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    plot_model(build_model, to_file='AKeras.png', show_shapes=True)\n",
    "    \n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(y)  \n",
    "    yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    build_model.fit([x],[yOnehot],epochs=100, batch_size=80000*1)\n",
    "    build_model.save(\"Akeras.h5\")\n",
    "    plot_model(build_model, to_file='AKeras.png', show_shapes=True)\n",
    "    \n",
    "    return build_model,models\n",
    "\n",
    "def kerasFitAndSaveSimple(x,y,num_labels):\n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 382\n",
    "    models=[]\n",
    "    \n",
    "    build_model = tf.keras.Sequential()\n",
    "    build_model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    build_model.add(layers.Dropout(dropout_rate))\n",
    "    build_model.add(layers.Dense(num_labels, activation='sigmoid'))\n",
    "    build_model.add(layers.Dense(num_labels, activation='sigmoid',name=\"layer1\",input_shape=(features_size,)))\n",
    "    \n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    plot_model(build_model, to_file='AKerasSimple.png', show_shapes=True)\n",
    "    \n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(y)  \n",
    "    yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    build_model.fit([x],[yOnehot],epochs=10000, batch_size=80000*1)\n",
    "    build_model.save(\"Akeras.h5\")\n",
    "    plot_model(build_model, to_file='AKeras.png', show_shapes=True)\n",
    "    \n",
    "    return build_model,models\n",
    "\n",
    "y1 = np.array(y1)\n",
    "y1= y1.reshape(nSamples,-1)\n",
    "print(y1)\n",
    "#kerasFitAndSave(x,y1,2)\n",
    "#kerasFitAndSaveSimple(x,y1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f76571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################融合决策树和多层神经网络###########################################################\n",
    "\n",
    "#######################################第一步读取数据\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "file1 = \"./trainData/dataAllSim1000.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "xyData = np.array(xyDataTmp)\n",
    "nSamples, nDims= xyData.shape\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "ylabel = y\n",
    "y1Level= y[:,0]#01\n",
    "y2Level= y[:,1]#012\n",
    "y3Level= y[:,2]#01234\n",
    "\n",
    "print(\"x.shape:\",x.shape,\"y.shape:\",y.shape)\n",
    "\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129ed4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################融合决策树和多层神经网络###########################################################\n",
    "\n",
    "#######################################第二步基于神经网络训练，这里采用简单神经网络，RESNET类似和HNCF三种方法进行训练\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "###简单模型1，没有隐藏层\n",
    "def kerasFitAndSaveSimple1(x,yOneHot,num_labels):\n",
    "    nSamples,features_size = x.shape\n",
    "    build_model = tf.keras.Sequential()\n",
    "    build_model.add(layers.Dense(num_labels, activation='sigmoid',name=\"layer1\",input_shape=(features_size,)))\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    \n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    build_model.fit([x],[yOneHot],epochs=10000, batch_size=80000*1)\n",
    "    build_model.save(\"kerasSimple1.h5\")\n",
    "    plot_model(build_model, to_file='KerasSimple1_noHiddenLayer.png', show_shapes=True)\n",
    "    return build_model\n",
    "\n",
    "###简单模型2，有隐藏层\n",
    "def kerasFitAndSaveSimple2(x,yOneHot,num_labels):\n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 512\n",
    "    dropout_rate = 0.05\n",
    "    build_model = tf.keras.Sequential()\n",
    "    build_model.add(layers.Dense(relu_size, activation='relu',name=\"layer1\",input_shape=(features_size,)))\n",
    "    build_model.add(layers.Dropout(dropout_rate,name=\"Dropout1-2\"))\n",
    "    build_model.add(layers.Dense(relu_size/2, activation='relu',name=\"layer2\"))\n",
    "    build_model.add(layers.Dropout(dropout_rate,name=\"Dropout2-3\"))\n",
    "    build_model.add(layers.Dense(num_labels, activation='sigmoid',name=\"layer3\"))\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    build_model.fit([x],[yOneHot],epochs=10000, batch_size=80000*1)\n",
    "    build_model.save(\"kerasSimple2.h5\")\n",
    "    plot_model(build_model, to_file='KerasSimple2_HiddenLayer.png', show_shapes=True)\n",
    "    return build_model\n",
    "\n",
    "###简单模型3，resnet_like\n",
    "def global_model(dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    return model\n",
    "\n",
    "def sigmoid_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='sigmoid',name=\"global\"))\n",
    "    return model\n",
    "\n",
    "def kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels):\n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 512\n",
    "    dropout_rate = 0.05\n",
    "    hierarchy = [1,1,1]\n",
    "    global_models = []\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(len(hierarchy)):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "\n",
    "    p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    build_model.fit([x],[yOneHot],epochs=10000, batch_size=80000*1)\n",
    "    build_model.save(\"KerasSimple3_likeResnet.h5\")\n",
    "    plot_model(build_model, to_file='KerasSimple3_likeResnet.png', show_shapes=True)\n",
    "    return build_model\n",
    "    print(\"HMCNF is not implemented\")\n",
    "    return False\n",
    "\n",
    "nSamples,features_size = x.shape\n",
    "num_labels = 5\n",
    "enc = OneHotEncoder()\n",
    "y3Level = np.array(y3Level)\n",
    "y3Level= y3Level.reshape(nSamples,-1)\n",
    "print(y3Level)\n",
    "enc.fit(y3Level)  \n",
    "\n",
    "###开始训练\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y3Level, test_size = 0.5)\n",
    "\n",
    "\n",
    "x = x_train\n",
    "yOneHot=enc.transform(y_train).toarray()\n",
    "print(yOneHot)\n",
    "#simpleMode1 = kerasFitAndSaveSimple1(x,yOneHot,num_labels)\n",
    "simpleMode2 = kerasFitAndSaveSimple2(x,yOneHot,num_labels)\n",
    "#simpleMode3 = kerasFitAndSaveSimple3(x,yOneHot,num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9931b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################融合决策树和多层神经网络###########################################################\n",
    "#######################################第三步根据识别结果，进行聚类聚类\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "\n",
    "####################################################################\n",
    "file1 = \"./trainData/dataAllSim1000.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "ylabel = y\n",
    "print(\"x.shape:\",x.shape,\"y.shape:\",y.shape)\n",
    "\n",
    "\n",
    "\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "#################################################################\n",
    "\n",
    "if 0:#采用keras\n",
    "    model_name =\"kerasSimple2.h5\" \n",
    "    model = keras.models.load_model(model_name)\n",
    "    yP5= model.predict([x], batch_size=2560)\n",
    "\n",
    "    ###需要将预测出的值，转换01整数,并转为数字式\n",
    "    for i in range(yP5.shape[0]):\n",
    "        tmp = yP5[i]\n",
    "        index=  np.argmax(tmp)\n",
    "        yP5[i] = [0,0,0,0,0]\n",
    "        yP5[i,index]=1\n",
    "    print(yP5)\n",
    "\n",
    "    ###\n",
    "    enc = OneHotEncoder()\n",
    "    yl5= y[:,2]#01234\n",
    "    yl5 = np.array(yl5)\n",
    "    yl5= yl5.reshape(nSamples,-1)\n",
    "    print(yl5)\n",
    "    enc.fit(yl5)\n",
    "\n",
    "    yP5= enc.inverse_transform(yP5)\n",
    "    yP5= yP5.reshape(-1,nSamples)[0]\n",
    "    ########\n",
    "\n",
    "\n",
    "\n",
    "    print(yP5)\n",
    "    print(yP5.shape)\n",
    "\n",
    "    print(yl5)\n",
    "    print(yl5.shape)\n",
    "\n",
    "if 1:#采用决策树\n",
    "    yl5= y[:,2]#01234\n",
    "    dt = tree.DecisionTreeClassifier(max_depth=10,min_samples_leaf=100)\n",
    "    dt = dt.fit(x, yl5)\n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(yl5,yPredict)\n",
    "    print(tmp1)\n",
    "    mat1num = confusion_matrix(yl5,yPredict)\n",
    "    mat2acc = confusion_matrix(yl5,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    yP5 = yPredict\n",
    "\n",
    "###################################开始合并\n",
    "hierachFloor = dict()\n",
    "hierachFloor ['input'] = x\n",
    "hierachFloor ['output'] = y\n",
    "\n",
    "                               \n",
    "                                    \n",
    "                                    \n",
    "#0层为原始输入层\n",
    "mat1num = confusion_matrix(yl5 ,yP5)\n",
    "mat2acc = confusion_matrix(yl5,yP5,normalize='pred')\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "hierachFloor ['floor0'] = {'label':['0','1','2','3','4'],'num_mat': mat1num,'prob_mat': mat2acc}\n",
    "                                    \n",
    "\n",
    "def computeAccuracyDiff(mat1,accy):\n",
    "    h,w = mat1.shape\n",
    "    tmp = np.zeros((h-1,w-1))\n",
    "    matTmp={}\n",
    "    ##从0到最后，行列合并\n",
    "    for index in range(h-1):\n",
    "\n",
    "        tmp = np.zeros((h-1,w))\n",
    "        num = 0\n",
    "        ####行合并\n",
    "        for i in range(h):#行合并\n",
    "            if i == index:\n",
    "                tmp[num]=mat1[i]+mat1[i+1]\n",
    "                num=num+1\n",
    "                continue\n",
    "            if i== index+1:\n",
    "                continue\n",
    "\n",
    "            tmp[num]=mat1[i]\n",
    "            num=num+1\n",
    "\n",
    "        ####列合并   \n",
    "        mat2=tmp\n",
    "        tmp = np.zeros((h-1,w-1))\n",
    "        num = 0\n",
    "        for j in range(w):#列合并\n",
    "            if j == index:\n",
    "                tmp[:,num] = mat2[:,j]+mat2[:,j+1]\n",
    "                num=num+1\n",
    "                continue\n",
    "            if j== index+1:\n",
    "                continue\n",
    "\n",
    "            tmp[:,num] = mat2[:,j]\n",
    "            num=num+1\n",
    "        matTmp[index] = tmp\n",
    "        \n",
    "        #print(\"合并后的所有矩阵\")\n",
    "        #print(index,matTmp[index])#合并后的所有矩阵\n",
    "    matTmp1 = copy.deepcopy(matTmp)\n",
    "   \n",
    "    ##归一化   \n",
    "    maxDiffMat = np.zeros((len(matTmp),1))\n",
    "    for i in range(len(matTmp)):\n",
    "        tmp = matTmp[i]\n",
    "        sumTmp =  sum(tmp)\n",
    "        for j in range(tmp.shape[1]):\n",
    "            tmp[:,j] = tmp[:,j]/(sumTmp[j])\n",
    "\n",
    "        accyNow = tmp[i,i]\n",
    "        maxDiffMat[i]= max(accyNow-accy[i],accyNow-accy[i+1])\n",
    "    maxIndex = np.argmax(maxDiffMat)\n",
    "    maxDiff = max(maxDiffMat)\n",
    "    #print(matTmp1)\n",
    "    return maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1\n",
    "#print(\"最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "#print(“所有合并后的所有矩阵，数目和概率\",matTmp,matTmp1)\n",
    "#print(“各个点合并后的正确率提升矩阵\",maxDiffMat)\n",
    "\n",
    "print(\"\\n\\n\\n###################################################\")\n",
    "print(\"\\n\\n\\n 用数据进行分析\")\n",
    "\n",
    "#1层为5到4层\n",
    "accy = [mat2acc[0,0],mat2acc[1,1],mat2acc[2,2],mat2acc[3,3],mat2acc[4,4]]\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1num,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"\\n\\n\\n5->4,最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(maxDiffMat)\n",
    "print(\"数目矩阵\\n\",matTmp1[maxIndex])\n",
    "print(\"概率矩阵\\n\",np.around(matTmp[maxIndex], decimals=3))\n",
    "hierachFloor ['floor1'] = {'num_mat': matTmp1,'prob_mat': matTmp,'mergeIndex':maxIndex,'mergediffMat':maxDiffMat}\n",
    "\n",
    "###2层4->3\n",
    "mat1num=matTmp1[maxIndex]\n",
    "mat2acc= [matTmp[maxIndex][0,0],matTmp[maxIndex][1,1],matTmp[maxIndex][2,2],matTmp[maxIndex][3,3]]\n",
    "\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1num,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"\\n\\n\\n4->3,最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(maxDiffMat)\n",
    "print(\"数目矩阵\\n\",matTmp1[maxIndex])\n",
    "print(\"概率矩阵\\n\",np.around(matTmp[maxIndex], decimals=3))\n",
    "      \n",
    "hierachFloor ['floor2'] = {'num_mat': matTmp1,'prob_mat': matTmp,'mergeIndex':maxIndex,'mergediffMat':maxDiffMat}\n",
    " \n",
    "###3层3->2\n",
    "mat1num=matTmp1[maxIndex]\n",
    "mat2acc= [matTmp[maxIndex][0,0],matTmp[maxIndex][1,1],matTmp[maxIndex][2,2]]\n",
    "\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1num,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"\\n\\n\\n3->2,最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(maxDiffMat)\n",
    "print(\"数目矩阵\\n\",matTmp1[maxIndex])\n",
    "print(\"概率矩阵\\n\",np.around(matTmp[maxIndex], decimals=3))\n",
    "      \n",
    "hierachFloor['floor3'] = {'num_mat': matTmp1,'prob_mat': matTmp,'mergeIndex':maxIndex,'mergediffMat':maxDiffMat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################融合决策树和多层神经网络###########################################################\n",
    "#######################################第四步根据聚类和识别结果，开始微调\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "\n",
    "\n",
    "##################\n",
    "file1 = \"./trainData/dataAllSim1000.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "ylabel = y\n",
    "print(\"x.shape:\",x.shape,\"y.shape:\",y.shape)\n",
    "\n",
    "yl5= y[:,2]#01234\n",
    "print(\"x.shape:\",x.shape,\"yl5.shape:\",yl5.shape)\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "\n",
    "print(\"x.shape:\",x.shape,\"y.shape:\",yl5.shape,\"y.type:\", type(yl5) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "\n",
    "\n",
    "def getKerasModeFloors(x,y,saveName):\n",
    "    model_name = saveName \n",
    "    model = keras.models.load_model(model_name)\n",
    "    yP5= model.predict([x], batch_size=2560)\n",
    "    nSamples = yP5.shape[0]\n",
    "     ###需要将预测出的值，转换01整数,并转为数字式\n",
    "    for i in range(yP5.shape[0]):\n",
    "        tmp = yP5[i]\n",
    "        index=  np.argmax(tmp)\n",
    "        yP5[i] = [0,0,0,0,0]\n",
    "        yP5[i,index]=1\n",
    "   \n",
    "\n",
    "    ###\n",
    "    enc = OneHotEncoder()\n",
    "    yl5= y[:,2]#01234\n",
    "    yl5 = np.array(yl5)\n",
    "    yl5= yl5.reshape(nSamples,-1)\n",
    "    print(yl5)\n",
    "    enc.fit(yl5)\n",
    "\n",
    "    yP5= enc.inverse_transform(yP5)\n",
    "    yP5= yP5.reshape(-1,nSamples)[0]\n",
    "    \n",
    "    yP4 = np.zeros((yP5.shape[0],1))\n",
    "    yP3 = np.zeros((yP5.shape[0],1))\n",
    "    yP2 = np.zeros((yP5.shape[0],1))\n",
    "\n",
    "    for i in range(yP5.shape[0]):\n",
    "        if(yP5[i]== 2) or (yP5[i]== 1):\n",
    "             yP4[i] = 21\n",
    "        else:\n",
    "             yP4[i] = yP5[i]\n",
    "                \n",
    "        if(yP5[i]== 2) or (yP5[i]== 1) or (yP5[i]== 0):\n",
    "             yP3[i] = 210\n",
    "        else:\n",
    "             yP3[i] = yP5[i]\n",
    "                \n",
    "        if(yP5[i]== 2) or (yP5[i]== 1) or (yP5[i]== 0) or (yP5[i]== 3):\n",
    "             yP2[i] = 3210\n",
    "        else:\n",
    "             yP2[i] = yP5[i]\n",
    "    \n",
    "    return model,yP5,yP4,yP3,yP2\n",
    "\n",
    "#分层决策树\n",
    "def dtFitAndSave(x,y,saveName):\n",
    "    dt = tree.DecisionTreeClassifier(max_depth=10,min_samples_leaf=1000)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    data=tree.export_graphviz(dt, out_file=None,class_names=None,filled=True) \n",
    "    graph = graphviz.Source(data)\n",
    "    graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(tmp1)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    #text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    return dt\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "def getDTSamplesInfo(x,dt):\n",
    "    yPredict = dt.predict_proba(x)\n",
    "    #print(\"\\n\\n getDTSamplesInfo yPredict\",yPredict)\n",
    "    d_path = dt.decision_path(x).todense()\n",
    "    #print(\"\\n\\n d_path\",d_path)\n",
    "    #print(\"impurity\",dt.tree_.impurity)\n",
    "    #print(\"feature\",dt.tree_.feature)\n",
    "    #print(\"threshold\",dt.tree_.threshold)\n",
    "    \n",
    "    #左节点编号  :  clf.tree_.children_left\n",
    "    #右节点编号  :  clf.tree_.children_right\n",
    "    #分割的变量  :  clf.tree_.feature\n",
    "    #分割的阈值  :  clf.tree_.threshold\n",
    "    #不纯度(gini) :  clf.tree_.impurity\n",
    "    #样本个数      :  clf.tree_.n_node_samples\n",
    "    #样本分布      :  clf.tree_.value\n",
    "    #https://blog.csdn.net/ywj_1991/article/details/122985778\n",
    "    #https://www.javaroad.cn/questions/54003\n",
    "    \n",
    "    h,w = d_path.shape\n",
    "    gini =np.zeros((h,1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(h):\n",
    "       path = d_path[i]\n",
    "       v,ind = np.where(path>0)\n",
    "       xtmp = x[i]\n",
    "       #print(\"path\",path,ind,np.array(ind)[-1])\n",
    "    \n",
    "       #print(\"\\n index\",index)\n",
    "       #print(\"impurity\",dt.tree_.impurity[ind])\n",
    "       #print(\"feature\",dt.tree_.feature[ind])\n",
    "       #print(\"threshold\",dt.tree_.threshold[ind])\n",
    "       #print(\"x[index]\",xtmp[ind])\n",
    "       \n",
    "      \n",
    "       #print(\"the leaf node:\",np.array(ind)[-1],\"the simplest rule is\")\n",
    "       #for jj in ind:\n",
    "       #    if dt.tree_.feature[jj] == -2:\n",
    "       #         print(\"label,proba is\",yPredict[i,0],yPredict[i,1])\n",
    "       #         break\n",
    "                \n",
    "       #    if xtmp[jj]<=dt.tree_.threshold[jj]:\n",
    "       #       print(\" x[%d]<=%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "       #    else:\n",
    "       #       print(\" x[%d]>%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "                    \n",
    "       finalPos = np.array(ind)[-1]\n",
    "       gini[i] = dt.tree_.impurity[finalPos]\n",
    "       \n",
    "       #print(\"d_path\",i,path,dt.tree_.impurity[finalPos])\n",
    "       #print(dt.tree_.feature[finalPos])\n",
    "       #print(dt.tree_.threshold[finalPos])\n",
    "       #print(dt.tree_.n_node_samples[finalPos])\n",
    "\n",
    "    \n",
    "    return gini,yPredict\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "\n",
    "index = np.where((yl5 == 2) | (yl5 == 1))\n",
    "yl4 = yl5.copy()\n",
    "yl4[index]=21\n",
    "print(yl4)\n",
    "\n",
    "\n",
    "index = np.where((yl5 == 2) | (yl5 == 1) | (yl5 == 0))\n",
    "yl3 = yl5.copy()\n",
    "yl3[index]=210\n",
    "print(yl3)\n",
    "\n",
    "\n",
    "\n",
    "index = np.where( (yl5 == 3)|(yl5 == 2) | (yl5 == 1) | (yl5 == 0))\n",
    "yl2 = yl5.copy()\n",
    "yl2[index]=3210\n",
    "print(yl2)\n",
    "\n",
    "hierachFloor['floor3'][\"dt\"] = dtFitAndSave(x,yl2,\"Floo3_2\")\n",
    "hierachFloor['floor2'][\"dt\"] = dtFitAndSave(x,yl3,\"Floo2_3\")\n",
    "hierachFloor['floor1'][\"dt\"] = dtFitAndSave(x,yl4,\"Floor1_4\")\n",
    "hierachFloor['floor0'][\"dt\"] = dtFitAndSave(x,yl5,\"Floor0_5\")\n",
    "\n",
    "#giniFloor0,yPredictProFloor0 = getDTSamplesInfo(x,hierachFloor['floor0'][\"dt\"])\n",
    "#giniFloor1,yPredictProFloor1 = getDTSamplesInfo(x,hierachFloor['floor1'][\"dt\"])\n",
    "#giniFloor2,yPredictProFloor2 = getDTSamplesInfo(x,hierachFloor['floor2'][\"dt\"])\n",
    "#giniFloor3,yPredictProFloor3 = getDTSamplesInfo(x,hierachFloor['floor3'][\"dt\"])\n",
    "\n",
    "kerasFloors,yKerasP5,yKerasP4,yKerasP3,yKerasP2=getKerasModeFloors(x,y,'kerasSimple2.h5')\n",
    "##############开始混合检测\n",
    "\n",
    "###0层，5标签\n",
    "\n",
    "'''\n",
    "nSamples,feturesNume  = x.shape\n",
    "yHyLabelFloor0 = np.zeros((nSamples,1))\n",
    "hyCounter = 0\n",
    "for i in range(nSamples):\n",
    "    print(i)\n",
    "    xtmp = x[i]\n",
    "    dt = hierachFloor['floor0'][\"dt\"]\n",
    "    giniFloor0,yPredictProFloor0 = getDTSamplesInfo([xtmp],dt)\n",
    "    giniTmp = giniFloor0[0]\n",
    "    yPredictProFloor0Tmp = yPredictProFloor0[0]\n",
    "    #print('gini',giniTmp )\n",
    "    #print('probPredict',yPredictProFloor0Tmp  )\n",
    "    if giniTmp >0.05 or max(yPredictProFloor0Tmp)<0.98:\n",
    "        yHyLabelFloor0[i] = yKerasP5[i]\n",
    "    else:\n",
    "        yHyLabelFloor0[i] = np.argmax(yPredictProFloor0)\n",
    "        hyCounter = hyCounter+1\n",
    "  \n",
    "print('O层5标签hyCounter',hyCounter)    \n",
    "\n",
    "\n",
    "tmp1 = classification_report(yl5,yHyLabelFloor0)\n",
    "print('hybrid\\n',tmp1)\n",
    "tmp1 = classification_report( yKerasP5,yHyLabelFloor0)\n",
    "print('keras\\n',tmp1)\n",
    "mat1num = confusion_matrix(yl5,yHyLabelFloor0)\n",
    "mat2acc = confusion_matrix(yl5,yHyLabelFloor0,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "'''\n",
    "\n",
    "'''\n",
    "#############1层，4标签\n",
    "nSamples,feturesNume  = x.shape\n",
    "yHyLabelFloor1 = np.zeros((nSamples,1))\n",
    "hyCounter = 0\n",
    "for i in range(nSamples):\n",
    "    print(i)\n",
    "    xtmp = x[i]\n",
    "    dt = hierachFloor['floor1'][\"dt\"]\n",
    "    giniFloor1,yPredictProFloor1 = getDTSamplesInfo([xtmp],dt)\n",
    "    giniTmp = giniFloor1[0]\n",
    "    yPredictProFloor1Tmp = yPredictProFloor1[0]\n",
    "    #print('gini',giniTmp )\n",
    "    #print('probPredict',yPredictProFloor0Tmp  )\n",
    "    if giniTmp >0.05 or max(yPredictProFloor1Tmp)<0.98:\n",
    "        yHyLabelFloor1[i] = yKerasP4[i]\n",
    "    else:\n",
    "        tmp0= [0,3,4,21]\n",
    "        index = np.argmax(yPredictProFloor1)\n",
    "        yHyLabelFloor1[i] = tmp0[index]\n",
    "        hyCounter = hyCounter+1\n",
    "print('O层4标签hyCounter',hyCounter)    \n",
    "\n",
    "\n",
    "tmp1 = classification_report(yl4,yHyLabelFloor1)\n",
    "print('hybrid\\n',tmp1)\n",
    "tmp1 = classification_report( yKerasP4,yHyLabelFloor1)\n",
    "print('keras\\n',tmp1)\n",
    "mat1num = confusion_matrix(yl4,yHyLabelFloor1)\n",
    "mat2acc = confusion_matrix(yl4,yHyLabelFloor1,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "'''\n",
    "\n",
    "\n",
    "def computeAndCompareHybridMode(x,y,dt,kerasPLabel,floorLabel):\n",
    "    nSamples,feturesNume  = x.shape\n",
    "    yHyLabel  = np.zeros((nSamples,1))\n",
    "    hyCounter = 0\n",
    "    for i in range(nSamples):\n",
    "        print(i)\n",
    "        xtmp = x[i]\n",
    "        giniFloor,yPredictProFloor = getDTSamplesInfo([xtmp],dt)\n",
    "        giniTmp = giniFloor[0]\n",
    "        yPredictProFloorTmp = yPredictProFloor[0]\n",
    "        #print('gini',giniTmp )\n",
    "        #print('probPredict',yPredictProFloor0Tmp  )\n",
    "        if giniTmp >0.05 or max(yPredictProFloorTmp)<0.98:\n",
    "            yHyLabel[i] = kerasPLabel[i]\n",
    "        else:\n",
    "            #floorLabel= [3,4,210]\n",
    "            index = np.argmax(yPredictProFloorTmp)\n",
    "            yHyLabel[i] = floorLabel[index]\n",
    "            hyCounter = hyCounter+1\n",
    "    print('floorLabel\\n',floorLabel) \n",
    "    print('hyCounter\\n',hyCounter)    \n",
    "\n",
    "\n",
    "    tmp1 = classification_report(y,yHyLabel)\n",
    "    print('hybrid\\n',tmp1)\n",
    "    tmp1 = classification_report(kerasPLabel,yHyLabel)\n",
    "    print('keras\\n',tmp1)\n",
    "    mat1num = confusion_matrix(y,yHyLabel)\n",
    "    mat2acc = confusion_matrix(y,yHyLabel,normalize='pred')\n",
    "    print('mat1num\\n',mat1num)\n",
    "    print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "    return\n",
    "\n",
    "#floor=2 ,label=3\n",
    "#dt = hierachFloor['floor2'][\"dt\"]\n",
    "#floorLabel= [3,4,210] \n",
    "#computeAndCompareHybridMode(x,yl3,dt,yKerasP3,floorLabel)\n",
    "\n",
    "#floor=3 ,label=2\n",
    "dt = hierachFloor['floor3'][\"dt\"]\n",
    "floorLabel= [4,3210] \n",
    "computeAndCompareHybridMode(x,yl2,dt,yKerasP2,floorLabel)\n",
    "\n",
    "return\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "'''\n",
    "for i in range(nSamples):\n",
    "    for j in range(4)\n",
    "         input1 = x[i,:]\n",
    "         label =  ylabel[i,j]\n",
    "         output1 = Floor[j][\"dt\"].predict_proba(input)\n",
    "         output_gini,output_num =getDT_Info(input)\n",
    "    \n",
    "        if max(output1)<0.95 or output_gini>0.2\n",
    "            output1 =  Floor[j][\"keras\"].predict(input)\n",
    "        \n",
    "        loss=loss+(output1-label)\n",
    "                           \n",
    "                           \n",
    "def getDT_SamplesInfo(x)\n",
    "    yPredict = dt.predict_proba(x[0:3,:])\n",
    "    print(yPredict[:,1])\n",
    "    d_path = dt.decision_path(x[0:3,:]).todense()\n",
    "    print(d_path)\n",
    "    print(\"impurity\",dt.tree_.impurity)\n",
    "    print(\"feature\",dt.tree_.feature)\n",
    "    print(\"threshold\",dt.tree_.threshold)\n",
    "    \n",
    "    #左节点编号  :  clf.tree_.children_left\n",
    "    #右节点编号  :  clf.tree_.children_right\n",
    "    #分割的变量  :  clf.tree_.feature\n",
    "    #分割的阈值  :  clf.tree_.threshold\n",
    "    #不纯度(gini) :  clf.tree_.impurity\n",
    "    #样本个数      :  clf.tree_.n_node_samples\n",
    "    #样本分布      :  clf.tree_.value\n",
    "    #https://blog.csdn.net/ywj_1991/article/details/122985778\n",
    "    #https://www.javaroad.cn/questions/54003\n",
    "\n",
    "    w,h = d_path.shape\n",
    "    for i in range(h):\n",
    "       path = d_path[i]\n",
    "       v,ind = np.where(path>0)\n",
    "       xtmp = x[i]\n",
    "       #print(\"path\",path,ind,np.array(ind)[-1])\n",
    "    \n",
    "       print(\"\\n index\",index)\n",
    "       print(\"impurity\",dt.tree_.impurity[ind])\n",
    "       print(\"feature\",dt.tree_.feature[ind])\n",
    "       print(\"threshold\",dt.tree_.threshold[ind])\n",
    "       print(\"x[index]\",xtmp[ind])\n",
    "       \n",
    "      \n",
    "       print(\"the leaf node:\",np.array(ind)[-1],\"the simplest rule is\")\n",
    "       for jj in ind:\n",
    "           if dt.tree_.feature[jj] == -2:\n",
    "                print(\"label,proba is\",yPredict[i,0],yPredict[i,1])\n",
    "                break\n",
    "                \n",
    "           if xtmp[jj]<=dt.tree_.threshold[jj]:\n",
    "              print(\" x[%d]<=%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "           else:\n",
    "              print(\" x[%d]>%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "                    \n",
    "       finalPos = np.array(ind)[-1]\n",
    "       print(dt.tree_.impurity[finalPos])\n",
    "       print(dt.tree_.feature[finalPos])\n",
    "       print(dt.tree_.threshold[finalPos])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "538fc7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data france\n",
      "x.shape: (132438, 46) y.shape: (132438,) y.type: <class 'numpy.ndarray'>\n",
      "纯决策树的识别\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98    124628\n",
      "           1       0.48      0.19      0.27      1359\n",
      "           2       0.27      0.03      0.06       837\n",
      "           3       0.45      0.06      0.11      1899\n",
      "           4       0.50      0.56      0.53      3715\n",
      "\n",
      "    accuracy                           0.95    132438\n",
      "   macro avg       0.53      0.37      0.39    132438\n",
      "weighted avg       0.94      0.95      0.94    132438\n",
      "\n",
      "[[123399    166     15     97    951]\n",
      " [   846    254     25      4    230]\n",
      " [   498     79     28     13    219]\n",
      " [  1066     12     23    121    677]\n",
      " [  1578     15     12     35   2075]]\n",
      "[[0.969 0.316 0.146 0.359 0.229]\n",
      " [0.007 0.483 0.243 0.015 0.055]\n",
      " [0.004 0.15  0.272 0.048 0.053]\n",
      " [0.008 0.023 0.223 0.448 0.163]\n",
      " [0.012 0.029 0.117 0.13  0.5  ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132438/132438 [00:00<00:00, 510881.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "混合识别的结果\n",
      "\n",
      "floorLabel\n",
      " [0, 1, 2, 3, 4]\n",
      "hyCounter\n",
      " 114006\n",
      "hybrid\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99    124628\n",
      "           1       0.68      0.48      0.56      1359\n",
      "           2       0.62      0.37      0.46       837\n",
      "           3       0.52      0.59      0.55      1899\n",
      "           4       0.71      0.68      0.69      3715\n",
      "\n",
      "    accuracy                           0.96    132438\n",
      "   macro avg       0.70      0.62      0.65    132438\n",
      "weighted avg       0.96      0.96      0.96    132438\n",
      "\n",
      "keras\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99    124628\n",
      "           1       0.67      0.56      0.61      1359\n",
      "           2       0.62      0.41      0.49       837\n",
      "           3       0.52      0.59      0.55      1899\n",
      "           4       0.71      0.68      0.69      3715\n",
      "\n",
      "    accuracy                           0.97    132438\n",
      "   macro avg       0.70      0.64      0.67    132438\n",
      "weighted avg       0.96      0.97      0.96    132438\n",
      "\n",
      "mat1num\n",
      " [[123184    170     94    532    648]\n",
      " [   515    650     23     96     75]\n",
      " [   269     52    307    123     86]\n",
      " [   507     14     26   1117    235]\n",
      " [   789     71     48    292   2515]]\n",
      "mat2acc\n",
      " [[0.983 0.178 0.189 0.246 0.182]\n",
      " [0.004 0.679 0.046 0.044 0.021]\n",
      " [0.002 0.054 0.616 0.057 0.024]\n",
      " [0.004 0.015 0.052 0.517 0.066]\n",
      " [0.006 0.074 0.096 0.135 0.707]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "\n",
    "\n",
    "def getKerasModeFloors2(x,enc,saveName):\n",
    "    model_name = saveName \n",
    "    model = keras.models.load_model(model_name)\n",
    "    yP5= model.predict([x], batch_size=2560)\n",
    "    nSamples = yP5.shape[0]\n",
    "     ###需要将预测出的值，转换01整数,并转为数字式\n",
    "    for i in range(yP5.shape[0]):\n",
    "        tmp = yP5[i]\n",
    "        index=  np.argmax(tmp)\n",
    "        yP5[i] = [0,0,0,0,0]\n",
    "        yP5[i,index]=1\n",
    "   \n",
    "\n",
    "    ###  \n",
    "    yP5= enc.inverse_transform(yP5)\n",
    "    yP5= yP5.reshape(-1,nSamples)[0]\n",
    "    \n",
    "    yP4 = np.zeros((yP5.shape[0],1))\n",
    "    yP3 = np.zeros((yP5.shape[0],1))\n",
    "    yP2 = np.zeros((yP5.shape[0],1))\n",
    "\n",
    "    for i in range(yP5.shape[0]):\n",
    "        if(yP5[i]== 2) or (yP5[i]== 1):\n",
    "             yP4[i] = 21\n",
    "        else:\n",
    "             yP4[i] = yP5[i]\n",
    "                \n",
    "        if(yP5[i]== 2) or (yP5[i]== 1) or (yP5[i]== 0):\n",
    "             yP3[i] = 210\n",
    "        else:\n",
    "             yP3[i] = yP5[i]\n",
    "                \n",
    "        if(yP5[i]== 2) or (yP5[i]== 1) or (yP5[i]== 0) or (yP5[i]== 3):\n",
    "             yP2[i] = 3210\n",
    "        else:\n",
    "             yP2[i] = yP5[i]\n",
    "    \n",
    "    return model,yP5,yP4,yP3,yP2\n",
    "\n",
    "#分层决策树\n",
    "def dtFitAndSave(x,y,saveName):\n",
    "    dt = tree.DecisionTreeClassifier(max_depth=7,min_samples_leaf=100)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    data=tree.export_graphviz(dt, out_file=None,class_names=None,filled=True) \n",
    "    graph = graphviz.Source(data)\n",
    "    graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(\"纯决策树的识别\\n\",tmp1)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    #text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    return dt\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "def getDTSamplesInfo(x,dt):\n",
    "    yPredict = dt.predict_proba(x)\n",
    "    #print(\"\\n\\n getDTSamplesInfo yPredict\",yPredict)\n",
    "    d_path = dt.decision_path(x).todense()\n",
    "    #print(\"\\n\\n d_path\",d_path)\n",
    "    #print(\"impurity\",dt.tree_.impurity)\n",
    "    #print(\"feature\",dt.tree_.feature)\n",
    "    #print(\"threshold\",dt.tree_.threshold)\n",
    "    \n",
    "    #左节点编号  :  clf.tree_.children_left\n",
    "    #右节点编号  :  clf.tree_.children_right\n",
    "    #分割的变量  :  clf.tree_.feature\n",
    "    #分割的阈值  :  clf.tree_.threshold\n",
    "    #不纯度(gini) :  clf.tree_.impurity\n",
    "    #样本个数      :  clf.tree_.n_node_samples\n",
    "    #样本分布      :  clf.tree_.value\n",
    "    #https://blog.csdn.net/ywj_1991/article/details/122985778\n",
    "    #https://www.javaroad.cn/questions/54003\n",
    "    \n",
    "    h,w = d_path.shape\n",
    "    gini =np.zeros((h,1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(h):\n",
    "       path = d_path[i]\n",
    "       v,ind = np.where(path>0)\n",
    "       xtmp = x[i]\n",
    "       #print(\"path\",path,ind,np.array(ind)[-1])\n",
    "    \n",
    "       #print(\"\\n index\",index)\n",
    "       #print(\"impurity\",dt.tree_.impurity[ind])\n",
    "       #print(\"feature\",dt.tree_.feature[ind])\n",
    "       #print(\"threshold\",dt.tree_.threshold[ind])\n",
    "       #print(\"x[index]\",xtmp[ind])\n",
    "       \n",
    "      \n",
    "       #print(\"the leaf node:\",np.array(ind)[-1],\"the simplest rule is\")\n",
    "       #for jj in ind:\n",
    "       #    if dt.tree_.feature[jj] == -2:\n",
    "       #         print(\"label,proba is\",yPredict[i,0],yPredict[i,1])\n",
    "       #         break\n",
    "                \n",
    "       #    if xtmp[jj]<=dt.tree_.threshold[jj]:\n",
    "       #       print(\" x[%d]<=%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "       #    else:\n",
    "       #       print(\" x[%d]>%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "                    \n",
    "       finalPos = np.array(ind)[-1]\n",
    "       gini[i] = dt.tree_.impurity[finalPos]\n",
    "       \n",
    "       #print(\"d_path\",i,path,dt.tree_.impurity[finalPos])\n",
    "       #print(dt.tree_.feature[finalPos])\n",
    "       #print(dt.tree_.threshold[finalPos])\n",
    "       #print(dt.tree_.n_node_samples[finalPos])\n",
    "\n",
    "    \n",
    "       return gini,yPredict\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "from tqdm import tqdm\n",
    "def computeAndCompareHybridMode(x,y,dt,kerasPLabel,floorLabel):\n",
    "    nSamples,feturesNume  = x.shape\n",
    "    yHyLabel  = np.zeros((nSamples,1))\n",
    "    giniFloor,yPredictProFloor = getDTSamplesInfo(x,dt)\n",
    "    prdictMax = np.max(yPredictProFloor,axis=1)\n",
    "    \n",
    "    \n",
    "    index1 = np.argmax(yPredictProFloor, axis = 1)\n",
    "    index1 = index1.astype('int64')\n",
    "    hyCounter = nSamples\n",
    "    for i in tqdm(range(nSamples)):\n",
    "        yHyLabel[i] = floorLabel[index1[i]]\n",
    "        giniTmp = giniFloor[i]\n",
    "        probaTmp = prdictMax[i]\n",
    "        if giniTmp>0.1 or probaTmp<0.95:\n",
    "            yHyLabel[i] = kerasPLabel[i]\n",
    "            hyCounter = hyCounter-1\n",
    "        \n",
    "\n",
    "    print(\"混合识别的结果\\n\")\n",
    "    print('floorLabel\\n',floorLabel) \n",
    "    print('hyCounter\\n',hyCounter)    \n",
    "\n",
    "\n",
    "    tmp1 = classification_report(y,yHyLabel)\n",
    "    print('hybrid\\n',tmp1)\n",
    "    tmp1 = classification_report(y,kerasPLabel)\n",
    "    print('keras\\n',tmp1)\n",
    "    mat1num = confusion_matrix(y,yHyLabel)\n",
    "    mat2acc = confusion_matrix(y,yHyLabel,normalize='pred')\n",
    "    print('mat1num\\n',mat1num)\n",
    "    print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "    return\n",
    "'''\n",
    "from tqdm import tqdm\n",
    "\n",
    "def computeAndCompareHybridMode(x,y,dt,kerasPLabel,floorLabel):\n",
    "    nSamples,feturesNume  = x.shape\n",
    "    yHyLabel  = np.zeros((nSamples,1))\n",
    "    hyCounter = 0\n",
    "    for i in tqdm(range(nSamples)):\n",
    "        #print(i)\n",
    "        xtmp = x[i]\n",
    "        giniFloor,yPredictProFloor = getDTSamplesInfo([xtmp],dt)\n",
    "        giniTmp = giniFloor[0]\n",
    "        yPredictProFloorTmp = yPredictProFloor[0]\n",
    "        #print('gini',giniTmp )\n",
    "        #print('probPredict',yPredictProFloor0Tmp  )\n",
    "        if giniTmp >0.05 or max(yPredictProFloorTmp)<0.98:\n",
    "            yHyLabel[i] = kerasPLabel[i]\n",
    "        else:\n",
    "            #floorLabel= [3,4,210]\n",
    "            index = np.argmax(yPredictProFloorTmp)\n",
    "            yHyLabel[i] = floorLabel[index]\n",
    "            hyCounter = hyCounter+1\n",
    "    print('floorLabel\\n',floorLabel) \n",
    "    print('hyCounter\\n',hyCounter)    \n",
    "\n",
    "\n",
    "    tmp1 = classification_report(y,yHyLabel)\n",
    "    print('hybrid\\n',tmp1)\n",
    "    tmp1 = classification_report(y,kerasPLabel)\n",
    "    print('keras\\n',tmp1)\n",
    "    mat1num = confusion_matrix(y,yHyLabel)\n",
    "    mat2acc = confusion_matrix(y,yHyLabel,normalize='pred')\n",
    "    print('mat1num\\n',mat1num)\n",
    "    print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "    return\n",
    "'''\n",
    "##########################################################################\n",
    "###简单模型2，有隐藏层\n",
    "def kerasFitAndSaveSimple2(x,yOneHot,num_labels,filename):\n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 512\n",
    "    dropout_rate = 0.05\n",
    "    build_model = tf.keras.Sequential()\n",
    "    build_model.add(layers.Dense(relu_size, activation='relu',name=\"layer1\",input_shape=(features_size,)))\n",
    "    build_model.add(layers.Dropout(dropout_rate,name=\"Dropout1-2\"))\n",
    "    build_model.add(layers.Dense(relu_size/2, activation='relu',name=\"layer2\"))\n",
    "    build_model.add(layers.Dropout(dropout_rate,name=\"Dropout2-3\"))\n",
    "    build_model.add(layers.Dense(num_labels, activation='sigmoid',name=\"layer3\"))\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    build_model = keras.models.load_model(filename)\n",
    "    \n",
    "    build_model.fit([x],[yOneHot],epochs=3000, batch_size=10000*1)\n",
    "    #build_model.fit(x,yOneHot,epochs=1000, batch_size=80000*1)\n",
    "    #build_model.save(\"kerasSimple2.h5\")\n",
    "    build_model.save(filename)\n",
    "    plot_model(build_model, to_file='KerasSimple2_HiddenLayer.png', show_shapes=True)\n",
    "    return build_model\n",
    "\n",
    "###简单模型3，resnet_like\n",
    "\n",
    "def global_model(dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    return model\n",
    "\n",
    "def sigmoid_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='sigmoid',name=\"global\"))\n",
    "    return model\n",
    "\n",
    "def kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels,saveName):\n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 512\n",
    "    dropout_rate = 0.05\n",
    "    hierarchy = [1,1,1]\n",
    "    global_models = []\n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(len(hierarchy)):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "\n",
    "    p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    build_model = keras.models.load_model(saveName)\n",
    "    build_model.fit([x],[yOneHot],epochs=10000, batch_size=10000*1)\n",
    "    #saveName = \"KerasSimple3_likeResnet.h5\"\n",
    "    build_model.save(saveName)\n",
    "    #plot_model(build_model, to_file='KerasSimple3_likeResnet.png', show_shapes=True)\n",
    "    return build_model\n",
    "\n",
    "def getKerasResnetRVL(x,enc,saveName):\n",
    "    model_name = saveName \n",
    "    model = keras.models.load_model(model_name)\n",
    "    y= model.predict([x], batch_size=2560)\n",
    "    nSamples = y.shape[0]\n",
    "    ###需要将预测出的值，转换01整数,并转为数字式\n",
    "    for i in range(y.shape[0]):\n",
    "        tmp = y[i]\n",
    "        index=  np.argmax(tmp)\n",
    "        y[i] = [0]*y.shape[1]\n",
    "        y[i,index]=1\n",
    "   \n",
    "\n",
    "    ###  \n",
    "    y= enc.inverse_transform(y)\n",
    "    y= y.reshape(-1,nSamples)[0]\n",
    "    return y\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "####用法国数据进行验证\n",
    "file1 = \"./trainData/france_0_allSamples.csv\"\n",
    "print(\"reading data france\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "h,w = xyData.shape\n",
    "x = xyData[:,1:23]#简单处理与SUMO数据库一致\n",
    "x = xyData[:,1:w-1]#用所有的数据\n",
    "y = xyData[:,w-1]\n",
    "y= y.astype('int64')\n",
    "yl5 = y\n",
    "print(\"x.shape:\",x.shape,\"y.shape:\",y.shape,\"y.type:\", type(y) )\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "\n",
    "\n",
    "####\n",
    "'''\n",
    "file1 = \"./trainData/dataAllSim1000.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "\n",
    "xSumo = xyData[:,0:22]\n",
    "ySumo = xyData[:,22:26]\n",
    "ySumo= ySumo[:,2]#01234\n",
    "ySumo= ySumo.astype('int64')\n",
    "\n",
    "print(\"x.shape:\",x.shape,\"yl5.shape:\",yl5.shape)\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "\n",
    "\n",
    "x = np.concatenate((xSumo,x))\n",
    "y = np.concatenate((ySumo,y))\n",
    "yl5 = y\n",
    "print(\"x.shape:\",x.shape,\"y.shape:\",yl5.shape,\"y.type:\", type(yl5) )\n",
    "'''\n",
    "\n",
    "##########################################################################\n",
    "###keras拟合,oneHot\n",
    "nSamples,nFeatures =  x.shape\n",
    "enc = OneHotEncoder()\n",
    "yl5= yl5.reshape(nSamples,-1)\n",
    "enc.fit(yl5)  \n",
    "\n",
    "\n",
    "\n",
    "##keraskeras拟合\n",
    "filename = \"kerasSimple2FranceDataSetAll1.h5\"\n",
    "if 0:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, yl5, test_size = 0.5)\n",
    "    yOneHot=enc.transform(y_train).toarray()\n",
    "    num_labels = 5 \n",
    "    simpleMode2 = kerasFitAndSaveSimple2(x_train,yOneHot,num_labels,filename)\n",
    "\n",
    "filename = \"KerasSimple3_likeResnet_floor4_5label.h5\"\n",
    "if 1:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, yl5, test_size = 0.5)\n",
    "    \n",
    "    yOneHot=enc.transform(yl5).toarray()\n",
    "    num_labels = 5 \n",
    "    kerasModel3_floor4_5label = kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels,filename) \n",
    "\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "\n",
    "index = np.where((yl5 == 2) | (yl5 == 1))\n",
    "yl4 = yl5.copy()\n",
    "yl4[index]=21\n",
    "#print(yl4)\n",
    "\n",
    "\n",
    "index = np.where((yl5 == 2) | (yl5 == 1) | (yl5 == 0))\n",
    "yl3 = yl5.copy()\n",
    "yl3[index]=210\n",
    "#print(yl3)\n",
    "\n",
    "\n",
    "\n",
    "index = np.where( (yl5 == 3)|(yl5 == 2) | (yl5 == 1) | (yl5 == 0))\n",
    "yl2 = yl5.copy()\n",
    "yl2[index]=3210\n",
    "#print(yl2)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "#hierachFloor['floor3'][\"dt\"] = dtFitAndSave(x,yl2,\"Floo3_2\")\n",
    "#hierachFloor['floor2'][\"dt\"] = dtFitAndSave(x,yl3,\"Floo2_3\")\n",
    "#hierachFloor['floor1'][\"dt\"] = dtFitAndSave(x,yl4,\"Floor1_4\")\n",
    "\n",
    "#dt_floor1_2label = dtFitAndSave(x,yl2,\"Floor1_2label\")\n",
    "#dt_floor2_3label = dtFitAndSave(x,yl3,\"Floor2_3label\")\n",
    "#dt_floor3_4label = dtFitAndSave(x,yl4,\"Floor3_4label\")\n",
    "dt_floor4_5label = dtFitAndSave(x,yl5,\"Floor4_5label\")\n",
    "kerasFloors,yKerasP5,yKerasP4,yKerasP3,yKerasP2=getKerasModeFloors2(x,enc,filename)\n",
    "\n",
    "\n",
    "#floor=3 ,label=2\n",
    "#dt = hierachFloor['floor3'][\"dt\"]\n",
    "\n",
    "#floorLabel= [4,3210] \n",
    "#computeAndCompareHybridMode(x,yl2,dt,yKerasP2,floorLabel)\n",
    "\n",
    "\n",
    "\n",
    "dt =dt_floor4_5label\n",
    "floorLabel= [0,1,2,3,4] \n",
    "computeAndCompareHybridMode(x,yl5,dt,yKerasP5,floorLabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9683aefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "建立多层嵌套决策树模型\n",
      "0.主程序开始，建立多层嵌套决策树模型\n",
      "reading data france\n",
      "x.shape: (132438, 46) y.shape: (132438,) y.type: <class 'numpy.ndarray'>\n",
      "原始样本分为3210和4两类\n",
      "3210样本分为210和3两类\n",
      "x.shape: (128723, 46) y.shape: (128723,) y.type: <class 'numpy.ndarray'>\n",
      "[210 210 210 ...   3   3   3]\n",
      "210样本分为10和2两类\n",
      "x.shape: (126824, 46) y.shape: (126824,) y.type: <class 'numpy.ndarray'>\n",
      "[10 10 10 ...  2  2  2]\n",
      "10样本分为0和1两类\n",
      "x.shape: (125987, 46) y.shape: (125987,) y.type: <class 'numpy.ndarray'>\n",
      "[0 0 0 ... 1 1 1]\n",
      "x.shape: (132438, 46) y.shape: (132438,) y.type: <class 'numpy.ndarray'>\n",
      "y.shape: (132438, 1) y.type: <class 'numpy.ndarray'>\n",
      "Epoch 1/10000\n",
      "132438/132438 [==============================] - 25s 185us/step - loss: 0.3145 - acc: 0.9534\n",
      "Epoch 2/10000\n",
      "132438/132438 [==============================] - 21s 162us/step - loss: 0.1144 - acc: 0.9743\n",
      "Epoch 3/10000\n",
      "132438/132438 [==============================] - 21s 161us/step - loss: 0.1015 - acc: 0.9753\n",
      "Epoch 4/10000\n",
      "132438/132438 [==============================] - 21s 162us/step - loss: 0.0852 - acc: 0.9759\n",
      "Epoch 5/10000\n",
      "100000/132438 [=====================>........] - ETA: 5s - loss: 0.0740 - acc: 0.9771"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-c2c272e7c994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0myOneHot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0msaveName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"hybrid2_KerasSimple3_likeResnet_5label.h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m     \u001b[0mkerasModel3_5label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkerasFitAndSaveSimple3LikeResnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myOneHot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaveName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m     \u001b[0myKeras_5label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetKerasResnetRVL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menc_5label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaveName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0mdt_5label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtFitAndSave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myl5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"5label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-c2c272e7c994>\u001b[0m in \u001b[0;36mkerasFitAndSaveSimple3LikeResnet\u001b[0;34m(x, yOneHot, num_labels, saveName)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m#build_model = keras.models.load_model(saveName)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mbuild_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myOneHot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;31m#saveName = \"KerasSimple3_likeResnet.h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mbuild_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaveName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras220CpuJupyter/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1361\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/envs/keras220CpuJupyter/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    248\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras220CpuJupyter/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    503\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras220CpuJupyter/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    503\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "print(\"建立多层嵌套决策树模型\")\n",
    "#############################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "\n",
    "\n",
    "###############################################################\n",
    "\n",
    "def dtFitAndSave(x,y,saveName):\n",
    "    dt = tree.DecisionTreeClassifier(max_depth=7,min_samples_leaf=100)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    data=tree.export_graphviz(dt, out_file=None,class_names=None,filled=True) \n",
    "    graph = graphviz.Source(data)\n",
    "    graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(\"纯决策树的识别\\n\",tmp1)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    #text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    return dt\n",
    "\n",
    "###############################################################\n",
    "###简单模型3，resnet_like\n",
    "\n",
    "def global_model(dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    return model\n",
    "\n",
    "def sigmoid_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='sigmoid',name=\"global\"))\n",
    "    return model\n",
    "\n",
    "def kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels,saveName):\n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 512\n",
    "    dropout_rate = 0.05\n",
    "    hierarchy = [1,1,1]\n",
    "    global_models = []\n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(len(hierarchy)):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "\n",
    "    p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    #build_model = keras.models.load_model(saveName)\n",
    "    build_model.fit([x],[yOneHot],epochs=10000, batch_size=10000*1)\n",
    "    #saveName = \"KerasSimple3_likeResnet.h5\"\n",
    "    build_model.save(saveName)\n",
    "    #plot_model(build_model, to_file='KerasSimple3_likeResnet.png', show_shapes=True)\n",
    "    return build_model\n",
    "\n",
    "def getKerasResnetRVL(x,enc,saveName):\n",
    "    model_name = saveName \n",
    "    model = keras.models.load_model(model_name)\n",
    "    y= model.predict([x], batch_size=2560)\n",
    "    nSamples = y.shape[0]\n",
    "    ###需要将预测出的值，转换01整数,并转为数字式\n",
    "    for i in range(y.shape[0]):\n",
    "        tmp = y[i]\n",
    "        index=  np.argmax(tmp)\n",
    "        y[i] = [0]*y.shape[1]\n",
    "        y[i,index]=1\n",
    "   \n",
    "\n",
    "    ###  \n",
    "    y= enc.inverse_transform(y)\n",
    "    y= y.reshape(-1,nSamples)[0]\n",
    "    return y\n",
    "##########################################################################\n",
    "def hybridTest(x,y,dt,floorLabel,kerasLabel):\n",
    "    nSamples,feturesNume  = x.shape\n",
    "    yHyLabel  = np.zeros((nSamples,1))#混合模型预测标签\n",
    "    dtLabel = np.zeros((nSamples,1))#决策树模型预测标签\n",
    "    gini,yPredictProb= getDTSamplesInfo(x,dt)\n",
    "    prdictMax = np.max(yPredictProb,axis=1)\n",
    "    \n",
    "\n",
    "    \n",
    "    index1 = np.argmax(yPredictProb, axis = 1)\n",
    "    index1 = index1.astype('int64')\n",
    "    hyCounter = nSamples\n",
    "    for i in tqdm(range(nSamples)):\n",
    "        yHyLabel[i] = floorLabel[index1[i]]\n",
    "        dtLabel[i] = floorLabel[index1[i]]\n",
    "        giniTmp = giniFloor[i]\n",
    "        probaTmp = prdictMax[i]\n",
    "        if giniTmp>0.1 or probaTmp<0.95:\n",
    "            yHyLabel[i] = kerasPLabel[i]\n",
    "            hyCounter = hyCounter-1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"混合识别的结果\\n\")\n",
    "    print('floorLabel\\n',floorLabel) \n",
    "    print('hyCounter\\n',hyCounter)    \n",
    "\n",
    "\n",
    "    tmp1 = classification_report(y,yHyLabel)\n",
    "    print('hybrid\\n',tmp1)\n",
    "    tmp1 = classification_report(y,kerasLabel)\n",
    "    print('keras\\n',tmp1)\n",
    "    mat1num = confusion_matrix(y,yHyLabel)\n",
    "    mat2acc = confusion_matrix(y,yHyLabel,normalize='pred')\n",
    "    print('mat1num\\n',mat1num)\n",
    "    print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################################################################\n",
    "#############################################################\n",
    "print(\"0.主程序开始，建立多层嵌套决策树模型\")\n",
    "#############################################################\n",
    "file1 = \"./trainData/france_0_allSamples.csv\"\n",
    "print(\"reading data france\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "h,w = xyData.shape\n",
    "x = xyData[:,1:23]#简单处理与SUMO数据库一致\n",
    "x = xyData[:,1:w-1]#用所有的数据\n",
    "y = xyData[:,w-1]\n",
    "y= y.astype('int64')\n",
    "yl5 = y\n",
    "xOrigin =x\n",
    "print(\"x.shape:\",x.shape,\"y.shape:\",y.shape,\"y.type:\", type(y) )\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "print(\"原始样本分为3210和4两类\")\n",
    "index0 = np.where( (yl5 == 3)|(yl5 == 2) | (yl5 == 1) | (yl5 == 0))\n",
    "index1 = np.where( (yl5 ==4))\n",
    "x3210_4 = xOrigin.copy()\n",
    "y3210_4 = yl5.copy()\n",
    "y3210_4[index0] = 3210\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "print(\"3210样本分为210和3两类\")\n",
    "index0 = np.where( (yl5 == 2) | (yl5 == 1) | (yl5 == 0))\n",
    "index1 = np.where( (yl5 ==3))\n",
    "xTmp = xOrigin[index0]\n",
    "yTmp = yl5[index0]\n",
    "yTmp[:] = 210\n",
    "\n",
    "\n",
    "\n",
    "x = np.concatenate((xTmp,xOrigin[index1]),axis=0)\n",
    "y = np.concatenate((yTmp,yl5[index1]),axis=0)\n",
    "x210_3 = x\n",
    "y210_3 = y\n",
    "print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "print(y210_3)\n",
    "\n",
    "##################################\n",
    "print(\"210样本分为10和2两类\")\n",
    "index0 = np.where((yl5 == 1) | (yl5 == 0))\n",
    "index1 = np.where( (yl5 ==2))\n",
    "xTmp = xOrigin[index0]\n",
    "yTmp = yl5[index0]\n",
    "yTmp[:] = 10\n",
    "\n",
    "x = np.concatenate((xTmp,xOrigin[index1]),axis=0)\n",
    "y = np.concatenate((yTmp,yl5[index1]),axis=0)\n",
    "x10_2 = x\n",
    "y10_2 = y\n",
    "print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "print(y10_2)\n",
    "\n",
    "\n",
    "##################################\n",
    "print(\"10样本分为0和1两类\")\n",
    "index0 = np.where( (yl5 == 0))\n",
    "index1 = np.where( (yl5 ==1))\n",
    "xTmp = xOrigin[index0]\n",
    "yTmp = yl5[index0]\n",
    "\n",
    "x = np.concatenate((xTmp,xOrigin[index1]),axis=0)\n",
    "y = np.concatenate((yTmp,yl5[index1]),axis=0)\n",
    "\n",
    "x0_1 = x\n",
    "y0_1 = y\n",
    "print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "print(y0_1)\n",
    "\n",
    "if 1:\n",
    "    x=xOrigin\n",
    "    y=yl5\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    \n",
    "    num_labels = 5 \n",
    "    nSamples,nFeatures =  x.shape\n",
    "    enc = OneHotEncoder()\n",
    "    y= y.reshape(nSamples,-1)\n",
    "    \n",
    "    print(\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    enc.fit(y)\n",
    "    yOneHot=enc.transform(y).toarray()\n",
    "    saveName = \"hybrid2_KerasSimple3_likeResnet_5label.h5\"\n",
    "    kerasModel3_5label = kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels,saveName)     \n",
    "    yKeras_5label=getKerasResnetRVL(x,enc_5label,saveName)\n",
    "    dt_5label = dtFitAndSave(x,yl5,\"5label\")\n",
    "    enc_5label = enc\n",
    "    \n",
    "if 1:\n",
    "    print(\"Floor4 训练\")\n",
    "    x= x0_1\n",
    "    y =y0_1\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    \n",
    "    num_labels = 2 \n",
    "    nSamples,nFeatures =  x.shape\n",
    "    enc = OneHotEncoder()\n",
    "    y= y.reshape(nSamples,-1)\n",
    "    \n",
    "    print(\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    enc.fit(y)\n",
    "    yOneHot=enc.transform(y).toarray()\n",
    "    saveName = \"hybrid2_KerasSimple3_likeResnet_floor4.h5\"\n",
    "    kerasModel3_Floor4 = kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels,saveName)     \n",
    "    yKeras_Floor4=getKerasResnetRVL(x,enc,saveName)\n",
    "    dt_Floor4 = dtFitAndSave(x,yl5,\"Floor4\")\n",
    "    enc_floor4 = enc\n",
    "    \n",
    "if 1:\n",
    "    print(\"Floor3 训练\")\n",
    "    x= x10_2\n",
    "    y =y10_2\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    \n",
    "    num_labels = 2 \n",
    "    nSamples,nFeatures =  x.shape\n",
    "    enc = OneHotEncoder()\n",
    "    y= y.reshape(nSamples,-1)\n",
    "    \n",
    "    print(\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    enc.fit(y)\n",
    "    yOneHot=enc.transform(y).toarray()\n",
    "    saveName = \"hybrid2_KerasSimple3_likeResnet_floor3.h5\"\n",
    "    kerasModel3_Floor3 = kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels,saveName)     \n",
    "    yKeras_Floor3=getKerasResnetRVL(x,enc,saveName)\n",
    "    dt_Floor3 = dtFitAndSave(x,yl5,\"Floor3\")\n",
    "    enc_floor3 = enc\n",
    "    \n",
    "if 1:\n",
    "    print(\"Floor2 训练\")\n",
    "    x= x210_3\n",
    "    y =y210_3\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    \n",
    "    num_labels = 2 \n",
    "    nSamples,nFeatures =  x.shape\n",
    "    enc = OneHotEncoder()\n",
    "    y= y.reshape(nSamples,-1)\n",
    "    \n",
    "    print(\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    enc.fit(y)\n",
    "    yOneHot=enc.transform(y).toarray()\n",
    "    saveName = \"hybrid2_KerasSimple3_likeResnet_floor2.h5\"\n",
    "    kerasModel3_Floor2 = kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels,saveName)     \n",
    "    yKeras_Floor2=getKerasResnetRVL(x,enc,saveName)\n",
    "    dt_Floor2 = dtFitAndSave(x,yl5,\"Floor2\")\n",
    "    enc_floor2 = enc\n",
    "    \n",
    "if 1:\n",
    "    print(\"Floor1 训练\")\n",
    "    x= x3210_4\n",
    "    y =y3210_4\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    \n",
    "    num_labels = 2 \n",
    "    nSamples,nFeatures =  x.shape\n",
    "    enc = OneHotEncoder()\n",
    "    y= y.reshape(nSamples,-1)\n",
    "    \n",
    "    print(\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    enc.fit(y)\n",
    "    yOneHot=enc.transform(y).toarray()\n",
    "    saveName = \"hybrid2_KerasSimple3_likeResnet_floor1.h5\"\n",
    "    kerasModel3_Floor1 = kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels,saveName)     \n",
    "    yKeras_Floor1=getKerasResnetRVL(x,enc,saveName)\n",
    "    dt_Floor1 = dtFitAndSave(x,yl5,\"Floor1\")\n",
    "    enc_floor1 = enc\n",
    "\n",
    "\n",
    "##\n",
    "#kerasFloors,yKerasP5,yKerasP4,yKerasP3,yKerasP2=getKerasModeFloors2(x,enc,\"kerasSimple2FranceDataSetAll1.h5\")\n",
    "#dt_floor1_2label = dtFitAndSave(x,yl2,\"Floor1_2label\")\n",
    "#dt_floor2_3label = dtFitAndSave(x,yl3,\"Floor2_3label\")\n",
    "#dt_floor3_4label = dtFitAndSave(x,yl4,\"Floor3_4label\")\n",
    "#dt_floor4_5label = dtFitAndSave(x,yl5,\"Floor4_5label\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ebe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6773ecd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mainTestCSVMLP3(hmcnf_keras).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:keras220CpuJupyter]",
   "language": "python",
   "name": "conda-env-keras220CpuJupyter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
