{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9be4b4",
   "metadata": {
    "id": "48f98b91"
   },
   "outputs": [],
   "source": [
    "#mkdir /content/tmp\n",
    "#%cp -r -f -v /content/drive/MyDrive/SUMONBDT /content/tmp\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#%cd /content/drive/MyDrive/SUMONBDT\n",
    "#%cd /home/liuli/github/SUMONBDT\n",
    "#!nvidia-smi\n",
    "#用于测试oneHot\n",
    "#############################################################也是第一步，读取数据\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "#[2,3,5,9]\n",
    "x1 = [0,0,0,0]\n",
    "x2 = [0,0,0,1]\n",
    "\n",
    "x3 = [1,1,1,2]\n",
    "x4 = [1,1,1,3]\n",
    "x5 = [1,1,2,4]\n",
    "x6 = [1,1,2,5]\n",
    "x7 = [1,2,3,6]\n",
    "x8 = [1,2,3,7]\n",
    "x9 = [1,2,4,8]\n",
    "X = [x1, x2, x3,x4,x5,x6,x7,x8,x9]\n",
    "enc.fit(X)\n",
    "#print(enc.transform(X).toarray())\n",
    "\n",
    "\n",
    "########################读写CSV,并转为oneHot\n",
    "file1 = \"./trainData/dataAllSim1000.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "ylabel = y\n",
    "y = enc.transform(y).toarray()\n",
    "\n",
    "print(\"x.shape:\",x.shape,\"yOneHot.shape:\",y.shape)\n",
    "\n",
    "\n",
    "\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe63701",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "03c551ad",
    "outputId": "e1da57c0-4dd7-440a-bf3f-6194c50c0c1c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "################################################################第二步，训练\n",
    "#1. 核心为keras220不是pytorch\n",
    "#2. 基于hmcnf\n",
    "import model_hmcnf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "#hierarchy = [18, 80, 178, 142, 77, 4]\n",
    "hierarchy = [2,3,5,9]\n",
    "features_size = x.shape[1]\n",
    "label_size = y.shape[1]\n",
    "beta = 0.2\n",
    "dropout_rate=0.1\n",
    "relu_size=384\n",
    "\n",
    "\n",
    "\n",
    "def local_model(num_labels, dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(num_labels, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_model(dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    return model\n",
    "\n",
    "\n",
    "def sigmoid_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='sigmoid',name=\"global\"))\n",
    "    return model\n",
    "\n",
    "features = layers.Input(shape=(features_size,))\n",
    "global_models = []\n",
    "local_models = []\n",
    "\n",
    "\n",
    "for i in range(len(hierarchy)):\n",
    "    if i == 0:\n",
    "        global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "    else:\n",
    "        global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "\n",
    "p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "\n",
    "\n",
    "#显示只有全局模型的情况\n",
    "#modelTmp1 = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "#modelTmp1.summary()#\n",
    "#plot_model(modelTmp1, to_file='Flatten1.png', show_shapes=True)\n",
    "\n",
    "\n",
    "for i in range(len(hierarchy)):\n",
    "    local_models.append(local_model(hierarchy[i], dropout_rate, relu_size)(global_models[i]))\n",
    "    \n",
    "#显示只有局部局模型的情况(部分全局)\n",
    "p_loc = layers.concatenate(local_models)\n",
    "#modelTmp2 = tf.keras.Model(inputs=[features], outputs=[p_loc])\n",
    "#modelTmp2.summary()#\n",
    "#plot_model(modelTmp2, to_file='Flatten2.png', show_shapes=True)\n",
    "p_glob1 = layers.Lambda(lambda x: x*beta,name=\"global\")(p_glob)\n",
    "p_loc1 = layers.Lambda(lambda x: x*(1-beta),name=\"local\")(p_loc)\n",
    "\n",
    "labels = layers.add([p_glob1, p_loc1])\n",
    "\n",
    "model = tf.keras.Model(inputs=[features], outputs=[labels])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_model(model, to_file='FlattenAll.png', show_shapes=True)\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['mae'])\n",
    "model.fit([x],[y],epochs=1000, batch_size=25600*1)\n",
    "model.save(\"hmcnf10000.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87f441",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d7beeed",
    "outputId": "d6908600-8597-41c2-800f-afc59a088154",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##################################################################第三步，验证\n",
    "#%cd /content/drive/MyDrive/SUMONBDT\n",
    "import model_hmcnf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#######################0.准备onehot\n",
    "enc = OneHotEncoder()\n",
    "#[2,3,5,9]\n",
    "x1 = [0,0,0,0]\n",
    "x2 = [0,0,0,1]\n",
    "\n",
    "x3 = [1,1,1,2]\n",
    "x4 = [1,1,1,3]\n",
    "x5 = [1,1,2,4]\n",
    "x6 = [1,1,2,5]\n",
    "x7 = [1,2,3,6]\n",
    "x8 = [1,2,3,7]\n",
    "x9 = [1,2,4,8]\n",
    "X = [x1, x2, x3,x4,x5,x6,x7,x8,x9]\n",
    "enc.fit(X)\n",
    "\n",
    "#######################2.准备数据\n",
    "        \n",
    "file1 = \"./trainData/dataAllSim10000.csv\"\n",
    "file1 = \"./trainData/dataAllSim.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "ylabel = y\n",
    "y = enc.transform(y).toarray()\n",
    "\n",
    "\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "#######################3.预测模型\n",
    "print(\"3.HMCNF预测模型\")\n",
    "hierarchy = [2,3,5,9]\n",
    "features_size = x.shape[1]\n",
    "label_size = y.shape[1]\n",
    "beta = 0.2\n",
    "\n",
    "model_name =\"hmcnf.h5\" \n",
    "\n",
    "model = keras.models.load_model(model_name)\n",
    "y_out = model.predict([x], batch_size=2560)\n",
    "y_predict = np.where(y_out > 0.5, 1, 0)\n",
    "\n",
    "predict_ok = np.where(np.sum(y_predict - y, axis=1) == 0, 1, 0)\n",
    "\n",
    "\n",
    "print(\"validated {} , {} good out of {} samples\".format(model_name, np.sum(predict_ok), predict_ok.shape[0]))\n",
    "del y_predict #节省内存\n",
    "del predict_ok #节省内存\n",
    "#######################3.层次预测预测模型\n",
    "print(\"3.层次预测预测模型\")\n",
    "y1 = np.where(y_out[:,0:2] > 0.5, 1, 0)\n",
    "y2 = np.where(y_out[:,2:5] > 0.5, 1, 0)\n",
    "y3 = np.where(y_out[:,5:10] > 0.5, 1, 0)\n",
    "y4 = np.where(y_out[:,10:19] > 0.5, 1, 0)\n",
    "for i in range(y4.shape[0]):\n",
    "    tmp1 = y1[i]\n",
    "    tmp2 = y2[i]\n",
    "    tmp3 = y3[i]\n",
    "    tmp4 = y4[i]\n",
    "    if sum(tmp1) == 0:\n",
    "        index=  np.argmax(tmp1)\n",
    "        y1[i,index]=1\n",
    "        \n",
    "    if sum(tmp2) == 0:\n",
    "        index=  np.argmax(tmp2)\n",
    "        y2[i,index]=1\n",
    "        \n",
    "    if sum(tmp3) == 0:\n",
    "        index=  np.argmax(tmp3)\n",
    "        y3[i,index]=1\n",
    "    \n",
    "    if sum(tmp4) == 0:\n",
    "        index=  np.argmax(tmp4)\n",
    "        y4[i,index]=1\n",
    "        #print(i,y4[i],index)\n",
    "y_predict = np.concatenate([y1,y2,y3,y4],axis=1)\n",
    "predict_ok = np.where(np.sum(y_predict - y, axis=1) == 0, 1, 0)\n",
    "print(\"validated {} , {} good out of {} samples\".format(model_name, np.sum(predict_ok), predict_ok.shape[0]))\n",
    "\n",
    "#onehot 2 label\n",
    "ypredict = enc.inverse_transform(y_predict)\n",
    "del y_predict #节省内存\n",
    "del predict_ok #节省内存\n",
    "del y1,y2,y3,y4\n",
    "#######################4.评估层次模型\n",
    "#hierarchy = [2,3,5,9]\n",
    "\n",
    "##第一层，2\n",
    "print(\"###################################第一层，2\")\n",
    "h1_yp = ypredict[:,0]\n",
    "h1_yl = ylabel[:,0]\n",
    "tmp1 = classification_report(h1_yl,h1_yp)\n",
    "tmp2 = confusion_matrix(h1_yl,h1_yp,normalize='true')\n",
    "tmp3 = confusion_matrix(h1_yl,h1_yp,normalize='pred')\n",
    "print(tmp1)\n",
    "print(np.around(tmp2, decimals=3))\n",
    "print(np.around(tmp3, decimals=3))\n",
    "\n",
    "\n",
    "##第二层，3\n",
    "print(\"################################第二层，3\")\n",
    "h2_yp = ypredict[:,1]\n",
    "h2_yl = ylabel[:,1]\n",
    "tmp1 = classification_report(h2_yl,h2_yp)\n",
    "tmp2 = confusion_matrix(h2_yl,h2_yp,normalize='true')\n",
    "tmp3 = confusion_matrix(h2_yl,h2_yp,normalize='pred')\n",
    "print(tmp1)\n",
    "print(np.around(tmp2, decimals=3))\n",
    "print(np.around(tmp3, decimals=3))\n",
    "\n",
    "\n",
    "\n",
    "##第三层，5\n",
    "print(\"#############################第三层，5\")\n",
    "h3_yp = ypredict[:,2]\n",
    "h3_yl = ylabel[:,2]\n",
    "tmp1 = classification_report(h3_yl,h3_yp)\n",
    "tmp2 = confusion_matrix(h3_yl,h3_yp,normalize='true')\n",
    "tmp3 = confusion_matrix(h3_yl,h3_yp,normalize='pred')\n",
    "print(tmp1)\n",
    "print(np.around(tmp2, decimals=3))\n",
    "print(np.around(tmp3, decimals=3))\n",
    "\n",
    "\n",
    "##第四层，9\n",
    "print(\"#############################第四层，9\")\n",
    "h4_yp = ypredict[:,3]\n",
    "h4_yl = ylabel[:,3]\n",
    "tmp1 = classification_report(h4_yl,h4_yp)\n",
    "tmp2 = confusion_matrix(h4_yl,h4_yp,normalize='true')\n",
    "tmp3 = confusion_matrix(h4_yl,h4_yp,normalize='pred')\n",
    "print(tmp1)\n",
    "print(np.around(tmp2, decimals=3))\n",
    "print(np.around(tmp3, decimals=3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#第四步，根据混淆矩阵进行聚类。第一列代表识别为类别1的样本真实的类别分布\n",
    "import numpy as np\n",
    "import copy\n",
    "#########################################手动准备模拟数据\n",
    "mat1 = np.array([[0.952,0.004,0.015,0.008],\n",
    " [0.018,0.923,0.016,0.032],\n",
    " [0.016,0.036,0.934,0.047],\n",
    " [0.014,0.037,0.035,0.913]])\n",
    "accy = [mat1[0,0],mat1[1,1],mat1[2,2],mat1[3,3]]\n",
    "print(\"accuracy\",accy)\n",
    "matT1 = mat1\n",
    "mat1[:,0] = matT1[:,0]*1000\n",
    "mat1[:,1] = matT1[:,1]*1000\n",
    "mat1[:,2] = matT1[:,2]*1000\n",
    "mat1[:,3] = matT1[:,3]*1000\n",
    "sumTmp =  sum(mat1)\n",
    "print(sumTmp)\n",
    "print(mat1)\n",
    "\n",
    "##########################################计算最佳合并位置，根据最大的正确率提高\n",
    "def computeAccuracyDiff(mat1,accy):\n",
    "    h,w = mat1.shape\n",
    "    tmp = np.zeros((h-1,w-1))\n",
    "    matTmp={}\n",
    "    ##从0到最后，行列合并\n",
    "    for index in range(h-1):\n",
    "\n",
    "        tmp = np.zeros((h-1,w))\n",
    "        num = 0\n",
    "        ####行合并\n",
    "        for i in range(h):#行合并\n",
    "            if i == index:\n",
    "                tmp[num]=mat1[i]+mat1[i+1]\n",
    "                num=num+1\n",
    "                continue\n",
    "            if i== index+1:\n",
    "                continue\n",
    "\n",
    "            tmp[num]=mat1[i]\n",
    "            num=num+1\n",
    "\n",
    "        ####列合并   \n",
    "        mat2=tmp\n",
    "        tmp = np.zeros((h-1,w-1))\n",
    "        num = 0\n",
    "        for j in range(w):#列合并\n",
    "            if j == index:\n",
    "                tmp[:,num] = mat2[:,j]+mat2[:,j+1]\n",
    "                num=num+1\n",
    "                continue\n",
    "            if j== index+1:\n",
    "                continue\n",
    "\n",
    "            tmp[:,num] = mat2[:,j]\n",
    "            num=num+1\n",
    "        matTmp[index] = tmp\n",
    "        \n",
    "        #print(\"合并后的所有矩阵\")\n",
    "        #print(index,matTmp[index])#合并后的所有矩阵\n",
    "    matTmp1 = copy.deepcopy(matTmp)\n",
    "   \n",
    "    ##归一化   \n",
    "    maxDiffMat = np.zeros((len(matTmp),1))\n",
    "    for i in range(len(matTmp)):\n",
    "        tmp = matTmp[i]\n",
    "        sumTmp =  sum(tmp)\n",
    "        for j in range(tmp.shape[1]):\n",
    "            tmp[:,j] = tmp[:,j]/(sumTmp[j])\n",
    "\n",
    "        accyNow = tmp[i,i]\n",
    "        maxDiffMat[i]= max(accyNow-accy[i],accyNow-accy[i+1])\n",
    "    maxIndex = np.argmax(maxDiffMat)\n",
    "    maxDiff = max(maxDiffMat)\n",
    "    #print(matTmp1)\n",
    "    return maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1\n",
    "\n",
    "#为了思考，不用for循环，直接一步一步做\n",
    "#4到3\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(matTmp1[maxIndex])\n",
    "\n",
    "#3到2\n",
    "mat3to2=matTmp1[maxIndex]\n",
    "accy3to2 = [matTmp[maxIndex][0,0],matTmp[maxIndex][1,1],matTmp[maxIndex][2,2]]\n",
    "maxIndex3to2,maxDiff3to2,maxDiffMat3to2,matTmp3to2,matTmp3to2Origin =computeAccuracyDiff(mat3to2,accy3to2)\n",
    "chosedMat = matTmp3to2Origin[maxIndex3to2]\n",
    "print(\"最佳合并点和矩阵\",maxIndex3to2,maxDiff3to2)\n",
    "print(matTmp3to2Origin[maxIndex3to2])\n",
    "print(matTmp3to2[maxIndex3to2])\n",
    "\n",
    "\n",
    "#########################################采用数据进行分析\n",
    "print(\"\\n\\n\\n###################################################\")\n",
    "print(\"\\n\\n\\n 用第5层数据进行分析\")\n",
    "h3_yp = ypredict[:,2]\n",
    "h3_yl = ylabel[:,2]\n",
    "mat1 = confusion_matrix(h3_yl,h3_yp)\n",
    "p1 = confusion_matrix(h3_yl,h3_yp,normalize='pred')\n",
    "sumTmp = sum(mat1)\n",
    "print(mat1)\n",
    "print(sumTmp)\n",
    "print(np.around(p1, decimals=3))\n",
    "\n",
    "########5->4\n",
    "accy = [p1[0,0],p1[1,1],p1[2,2],p1[3,3],p1[4,4]]\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"\\n\\n\\n5->4,最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(matTmp1[maxIndex])\n",
    "print(np.around(matTmp[maxIndex], decimals=3))\n",
    "\n",
    "########4->3\n",
    "mat1=matTmp1[maxIndex]\n",
    "accy= [matTmp[maxIndex][0,0],matTmp[maxIndex][1,1],matTmp[maxIndex][2,2],matTmp[maxIndex][3,3]]\n",
    "\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"\\n\\n\\n4->3,最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(matTmp1[maxIndex])\n",
    "print(np.around(matTmp[maxIndex], decimals=3))\n",
    "\n",
    "\n",
    "########3->2\n",
    "mat1=matTmp1[maxIndex]\n",
    "accy= [matTmp[maxIndex][0,0],matTmp[maxIndex][1,1],matTmp[maxIndex][2,2]]\n",
    "\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"\\n\\n\\n3->2,最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(matTmp1[maxIndex])\n",
    "print(np.around(matTmp[maxIndex], decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482893a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##测试最简注意力机制，Attention Channel ,SEAttention\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils.vis_utils import plot_model\n",
    "np.random.seed(1337)  # for reproducibility\n",
    " \n",
    "from keras.models import *\n",
    "from keras.layers import Input, Dense,Multiply,Activation\n",
    " \n",
    "input_dim = 4\n",
    "\n",
    "\n",
    "def get_data(n, input_dim, attention_column=1):\n",
    "\n",
    "    x = np.random.standard_normal(size=(n, input_dim))\n",
    "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
    "    x[:, attention_column] = y[:, 0]\n",
    "    return x, y\n",
    "\n",
    " \n",
    " \n",
    "def Att(att_dim,inputs,name):\n",
    "    V = inputs\n",
    "    QK = Dense(att_dim,bias=None)(inputs)\n",
    "    QK = Activation(\"softmax\",name=name)(QK)\n",
    "    MV = Multiply()([V, QK])\n",
    "    return(MV)\n",
    " \n",
    " \n",
    "def build_model():\n",
    "    inputs = Input(shape=(input_dim,))\n",
    " \n",
    "    atts1 = Att(input_dim,inputs,\"attention_vec\")\n",
    " \n",
    "    x = Dense(16)(atts1)\n",
    "    atts2 = Att(16,x,\"attention_vec1\")\n",
    " \n",
    " \n",
    "    output = Dense(1, activation='sigmoid')(atts2)\n",
    "    model = Model(input=inputs, output=output)\n",
    "    return model\n",
    "\n",
    "N = 10000\n",
    "inputs_1, outputs = get_data(N, input_dim) \n",
    "print(inputs_1)\n",
    " \n",
    "m = build_model()\n",
    "plot_model(m, to_file='attMap.png', show_shapes=True)\n",
    "#m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#print(m.summary())\n",
    "#m.fit(inputs_1, outputs, epochs=20, batch_size=128, validation_split=0.2)testing_inputs_1, testing_outputs = get_data(1, input_dim)\n",
    "\n",
    "\n",
    "#原文链接：https://blog.csdn.net/xiaosongshine/article/details/90579679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a54a7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.csdn.net/SKIp121whats112/article/details/122265766\n",
    "#https://scikit-learn.org/stable/modules/tree.html\n",
    "##############测试决策树\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "Input= x\n",
    "Output = ylabel[:,2]\n",
    "print(x)\n",
    "print(Output)\n",
    "dt = tree.DecisionTreeClassifier(max_depth=5,min_samples_split=100,min_samples_leaf=100,min_impurity_decrease=0.001)\n",
    "dt = dt.fit(Input, Output)\n",
    "tree.plot_tree(dt)\n",
    "data=tree.export_graphviz(dt, out_file=None,class_names=['0','1','2','3','4'],filled=True) \n",
    "graph = graphviz.Source(data)\n",
    "graph.render(\"now\")\n",
    "\n",
    "data=tree.export_graphviz(dt, out_file=None,class_names=['0','1','2','3','4'],filled=True,proportion=True) \n",
    "graph = graphviz.Source(data)\n",
    "graph.render(\"nowPercent\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "OutPredict = dt.predict(Input)\n",
    "\n",
    "tmp1 = classification_report(Output,OutPredict )\n",
    "tmp2 = confusion_matrix(Output,OutPredict ,normalize='true')\n",
    "tmp3 = confusion_matrix(Output,OutPredict ,normalize='pred')\n",
    "print(tmp1)\n",
    "print(np.around(tmp2, decimals=3))\n",
    "print(np.around(tmp3, decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d45cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试决策树的特征\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "###测试权重\n",
    "nSamples =5000\n",
    "input_dim = 10\n",
    "#x = np.random.standard_normal(size=(nSamples, input_dim))\n",
    "x = np.random.randint(low=0, high=10, size=(nSamples, input_dim))\n",
    "y1 = np.zeros((nSamples, 1))#>50\n",
    "y1A = np.zeros((nSamples, 1))#>50 and <60\n",
    "y1B = np.zeros((nSamples, 1))#>=60\n",
    "sumX = np.sum(x,axis=1)\n",
    "index=np.where(sumX>40)\n",
    "y1[index]=1\n",
    "index=np.where((sumX>50)& (sumX<70))\n",
    "y1A[index]=1\n",
    "index=np.where(sumX>=70)\n",
    "y1B[index]=1\n",
    "\n",
    "##数据来源2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "file1 = \"./trainData/dataAllSim1000.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "xyData = np.array(xyDataTmp)\n",
    "nSamples, nDims= xyData.shape\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "ylabel = y\n",
    "y1= y[:,0]\n",
    "\n",
    "\n",
    "##################################################################\n",
    "#测试决策树\n",
    "def dtFitAndSave(x,y,class_names1,saveName):\n",
    "    dt = tree.DecisionTreeClassifier(max_depth=7,min_samples_split=100,min_samples_leaf=100,min_impurity_split=0.06,ccp_alpha=0.001)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    data=tree.export_graphviz(dt, out_file=None,class_names=class_names1,filled=True) \n",
    "    graph = graphviz.Source(data)\n",
    "    graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(tmp1)\n",
    "    text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    \n",
    "    yPredict = dt.predict_proba(x[0:3,:])\n",
    "    print(yPredict[:,1])\n",
    "    d_path = dt.decision_path(x[0:3,:]).todense()\n",
    "    print(d_path)\n",
    "    print(\"impurity\",dt.tree_.impurity)\n",
    "    print(\"feature\",dt.tree_.feature)\n",
    "    print(\"threshold\",dt.tree_.threshold)\n",
    "    \n",
    "    #左节点编号  :  clf.tree_.children_left\n",
    "    #右节点编号  :  clf.tree_.children_right\n",
    "    #分割的变量  :  clf.tree_.feature\n",
    "    #分割的阈值  :  clf.tree_.threshold\n",
    "    #不纯度(gini) :  clf.tree_.impurity\n",
    "    #样本个数      :  clf.tree_.n_node_samples\n",
    "    #样本分布      :  clf.tree_.value\n",
    "    #https://blog.csdn.net/ywj_1991/article/details/122985778\n",
    "    #https://www.javaroad.cn/questions/54003\n",
    "\n",
    "    w,h = d_path.shape\n",
    "    for i in range(h):\n",
    "       path = d_path[i]\n",
    "       v,ind = np.where(path>0)\n",
    "       xtmp = x[i]\n",
    "       #print(\"path\",path,ind,np.array(ind)[-1])\n",
    "    \n",
    "       print(\"\\n index\",index)\n",
    "       print(\"impurity\",dt.tree_.impurity[ind])\n",
    "       print(\"feature\",dt.tree_.feature[ind])\n",
    "       print(\"threshold\",dt.tree_.threshold[ind])\n",
    "       print(\"x[index]\",xtmp[ind])\n",
    "       \n",
    "      \n",
    "       print(\"the leaf node:\",np.array(ind)[-1],\"the simplest rule is\")\n",
    "       for jj in ind:\n",
    "           if dt.tree_.feature[jj] == -2:\n",
    "                print(\"label,proba is\",yPredict[i,0],yPredict[i,1])\n",
    "                break\n",
    "                \n",
    "           if xtmp[jj]<=dt.tree_.threshold[jj]:\n",
    "              print(\" x[%d]<=%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "           else:\n",
    "              print(\" x[%d]>%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "                    \n",
    "       finalPos = np.array(ind)[-1]\n",
    "       print(dt.tree_.impurity[finalPos])\n",
    "       print(dt.tree_.feature[finalPos])\n",
    "       print(dt.tree_.threshold[finalPos])\n",
    "\n",
    "dtFitAndSave(x,y1,[\"0\",\"1\"],\"bigger\")\n",
    "\n",
    "###################################################################################\n",
    "#测试神经网络\n",
    "def kerasFitAndSave(x,y,num_labels):\n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 384\n",
    "    dropout_rate =0.1\n",
    "    models=[]\n",
    "    \n",
    "    build_model = tf.keras.Sequential()\n",
    "   \n",
    "    build_model.add(layers.Dense(relu_size, activation='relu',name=\"layer1\",input_shape=(features_size,)))\n",
    "    build_model.add(layers.Dropout(dropout_rate,name=\"Dropout1-2\"))\n",
    "    build_model.add(layers.Dense(num_labels, activation='sigmoid',name=\"layer2\"))\n",
    "    \n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    plot_model(build_model, to_file='AKeras.png', show_shapes=True)\n",
    "    \n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(y)  \n",
    "    yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    build_model.fit([x],[yOnehot],epochs=100, batch_size=80000*1)\n",
    "    build_model.save(\"Akeras.h5\")\n",
    "    plot_model(build_model, to_file='AKeras.png', show_shapes=True)\n",
    "    \n",
    "    return build_model,models\n",
    "\n",
    "def kerasFitAndSaveSimple(x,y,num_labels):\n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 382\n",
    "    models=[]\n",
    "    \n",
    "    build_model = tf.keras.Sequential()\n",
    "    build_model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    build_model.add(layers.Dropout(dropout_rate))\n",
    "    build_model.add(layers.Dense(num_labels, activation='sigmoid'))\n",
    "    build_model.add(layers.Dense(num_labels, activation='sigmoid',name=\"layer1\",input_shape=(features_size,)))\n",
    "    \n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    plot_model(build_model, to_file='AKerasSimple.png', show_shapes=True)\n",
    "    \n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(y)  \n",
    "    yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    build_model.fit([x],[yOnehot],epochs=10000, batch_size=80000*1)\n",
    "    build_model.save(\"Akeras.h5\")\n",
    "    plot_model(build_model, to_file='AKeras.png', show_shapes=True)\n",
    "    \n",
    "    return build_model,models\n",
    "\n",
    "y1 = np.array(y1)\n",
    "y1= y1.reshape(nSamples,-1)\n",
    "print(y1)\n",
    "#kerasFitAndSave(x,y1,2)\n",
    "#kerasFitAndSaveSimple(x,y1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef15fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################融合决策树和多层神经网络###########################################################\n",
    "\n",
    "#######################################第一步读取数据\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "file1 = \"./trainData/dataAllSim1000.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "xyData = np.array(xyDataTmp)\n",
    "nSamples, nDims= xyData.shape\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "ylabel = y\n",
    "y1Level= y[:,0]#01\n",
    "y2Level= y[:,1]#012\n",
    "y3Level= y[:,2]#01234\n",
    "\n",
    "print(\"x.shape:\",x.shape,\"y.shape:\",y.shape)\n",
    "\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70323ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################融合决策树和多层神经网络###########################################################\n",
    "\n",
    "#######################################第二步基于神经网络训练，这里采用简单神经网络，RESNET类似和HNCF三种方法进行训练\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "###简单模型1，没有隐藏层\n",
    "def kerasFitAndSaveSimple1(x,yOneHot,num_labels):\n",
    "    nSamples,features_size = x.shape\n",
    "    build_model = tf.keras.Sequential()\n",
    "    build_model.add(layers.Dense(num_labels, activation='sigmoid',name=\"layer1\",input_shape=(features_size,)))\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    \n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    build_model.fit([x],[yOneHot],epochs=10000, batch_size=80000*1)\n",
    "    build_model.save(\"kerasSimple1.h5\")\n",
    "    plot_model(build_model, to_file='KerasSimple1_noHiddenLayer.png', show_shapes=True)\n",
    "    return build_model\n",
    "\n",
    "###简单模型2，有隐藏层\n",
    "def kerasFitAndSaveSimple2(x,yOneHot,num_labels):\n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 512\n",
    "    dropout_rate = 0.05\n",
    "    build_model = tf.keras.Sequential()\n",
    "    build_model.add(layers.Dense(relu_size, activation='relu',name=\"layer1\",input_shape=(features_size,)))\n",
    "    build_model.add(layers.Dropout(dropout_rate,name=\"Dropout1-2\"))\n",
    "    build_model.add(layers.Dense(relu_size/2, activation='relu',name=\"layer2\"))\n",
    "    build_model.add(layers.Dropout(dropout_rate,name=\"Dropout2-3\"))\n",
    "    build_model.add(layers.Dense(num_labels, activation='sigmoid',name=\"layer3\"))\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    build_model.fit([x],[yOneHot],epochs=10000, batch_size=80000*1)\n",
    "    build_model.save(\"kerasSimple2.h5\")\n",
    "    plot_model(build_model, to_file='KerasSimple2_HiddenLayer.png', show_shapes=True)\n",
    "    return build_model\n",
    "\n",
    "###简单模型3，resnet_like\n",
    "def global_model(dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    return model\n",
    "\n",
    "def sigmoid_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='sigmoid',name=\"global\"))\n",
    "    return model\n",
    "\n",
    "def kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels):\n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 512\n",
    "    dropout_rate = 0.05\n",
    "    hierarchy = [1,1,1]\n",
    "    global_models = []\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(len(hierarchy)):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "\n",
    "    p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    build_model.fit([x],[yOneHot],epochs=10000, batch_size=80000*1)\n",
    "    build_model.save(\"KerasSimple3_likeResnet.h5\")\n",
    "    plot_model(build_model, to_file='KerasSimple3_likeResnet.png', show_shapes=True)\n",
    "    return build_model\n",
    "    print(\"HMCNF is not implemented\")\n",
    "    return False\n",
    "\n",
    "nSamples,features_size = x.shape\n",
    "num_labels = 5\n",
    "enc = OneHotEncoder()\n",
    "y3Level = np.array(y3Level)\n",
    "y3Level= y3Level.reshape(nSamples,-1)\n",
    "print(y3Level)\n",
    "enc.fit(y3Level)  \n",
    "\n",
    "###开始训练\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y3Level, test_size = 0.5)\n",
    "\n",
    "\n",
    "x = x_train\n",
    "yOneHot=enc.transform(y_train).toarray()\n",
    "print(yOneHot)\n",
    "#simpleMode1 = kerasFitAndSaveSimple1(x,yOneHot,num_labels)\n",
    "simpleMode2 = kerasFitAndSaveSimple2(x,yOneHot,num_labels)\n",
    "#simpleMode3 = kerasFitAndSaveSimple3(x,yOneHot,num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################融合决策树和多层神经网络###########################################################\n",
    "#######################################第三步根据识别结果，进行聚类聚类\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "\n",
    "####################################################################\n",
    "file1 = \"./trainData/dataAllSim1000.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "ylabel = y\n",
    "print(\"x.shape:\",x.shape,\"y.shape:\",y.shape)\n",
    "\n",
    "\n",
    "\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "#################################################################\n",
    "\n",
    "if 0:#采用keras\n",
    "    model_name =\"kerasSimple2.h5\" \n",
    "    model = keras.models.load_model(model_name)\n",
    "    yP5= model.predict([x], batch_size=2560)\n",
    "\n",
    "    ###需要将预测出的值，转换01整数,并转为数字式\n",
    "    for i in range(yP5.shape[0]):\n",
    "        tmp = yP5[i]\n",
    "        index=  np.argmax(tmp)\n",
    "        yP5[i] = [0,0,0,0,0]\n",
    "        yP5[i,index]=1\n",
    "    print(yP5)\n",
    "\n",
    "    ###\n",
    "    enc = OneHotEncoder()\n",
    "    yl5= y[:,2]#01234\n",
    "    yl5 = np.array(yl5)\n",
    "    yl5= yl5.reshape(nSamples,-1)\n",
    "    print(yl5)\n",
    "    enc.fit(yl5)\n",
    "\n",
    "    yP5= enc.inverse_transform(yP5)\n",
    "    yP5= yP5.reshape(-1,nSamples)[0]\n",
    "    ########\n",
    "\n",
    "\n",
    "\n",
    "    print(yP5)\n",
    "    print(yP5.shape)\n",
    "\n",
    "    print(yl5)\n",
    "    print(yl5.shape)\n",
    "\n",
    "if 1:#采用决策树\n",
    "    yl5= y[:,2]#01234\n",
    "    dt = tree.DecisionTreeClassifier(max_depth=10,min_samples_leaf=100)\n",
    "    dt = dt.fit(x, yl5)\n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(yl5,yPredict)\n",
    "    print(tmp1)\n",
    "    mat1num = confusion_matrix(yl5,yPredict)\n",
    "    mat2acc = confusion_matrix(yl5,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    yP5 = yPredict\n",
    "\n",
    "###################################开始合并\n",
    "hierachFloor = dict()\n",
    "hierachFloor ['input'] = x\n",
    "hierachFloor ['output'] = y\n",
    "\n",
    "                               \n",
    "                                    \n",
    "                                    \n",
    "#0层为原始输入层\n",
    "mat1num = confusion_matrix(yl5 ,yP5)\n",
    "mat2acc = confusion_matrix(yl5,yP5,normalize='pred')\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "hierachFloor ['floor0'] = {'label':['0','1','2','3','4'],'num_mat': mat1num,'prob_mat': mat2acc}\n",
    "                                    \n",
    "\n",
    "def computeAccuracyDiff(mat1,accy):\n",
    "    h,w = mat1.shape\n",
    "    tmp = np.zeros((h-1,w-1))\n",
    "    matTmp={}\n",
    "    ##从0到最后，行列合并\n",
    "    for index in range(h-1):\n",
    "\n",
    "        tmp = np.zeros((h-1,w))\n",
    "        num = 0\n",
    "        ####行合并\n",
    "        for i in range(h):#行合并\n",
    "            if i == index:\n",
    "                tmp[num]=mat1[i]+mat1[i+1]\n",
    "                num=num+1\n",
    "                continue\n",
    "            if i== index+1:\n",
    "                continue\n",
    "\n",
    "            tmp[num]=mat1[i]\n",
    "            num=num+1\n",
    "\n",
    "        ####列合并   \n",
    "        mat2=tmp\n",
    "        tmp = np.zeros((h-1,w-1))\n",
    "        num = 0\n",
    "        for j in range(w):#列合并\n",
    "            if j == index:\n",
    "                tmp[:,num] = mat2[:,j]+mat2[:,j+1]\n",
    "                num=num+1\n",
    "                continue\n",
    "            if j== index+1:\n",
    "                continue\n",
    "\n",
    "            tmp[:,num] = mat2[:,j]\n",
    "            num=num+1\n",
    "        matTmp[index] = tmp\n",
    "        \n",
    "        #print(\"合并后的所有矩阵\")\n",
    "        #print(index,matTmp[index])#合并后的所有矩阵\n",
    "    matTmp1 = copy.deepcopy(matTmp)\n",
    "   \n",
    "    ##归一化   \n",
    "    maxDiffMat = np.zeros((len(matTmp),1))\n",
    "    for i in range(len(matTmp)):\n",
    "        tmp = matTmp[i]\n",
    "        sumTmp =  sum(tmp)\n",
    "        for j in range(tmp.shape[1]):\n",
    "            tmp[:,j] = tmp[:,j]/(sumTmp[j])\n",
    "\n",
    "        accyNow = tmp[i,i]\n",
    "        maxDiffMat[i]= max(accyNow-accy[i],accyNow-accy[i+1])\n",
    "    maxIndex = np.argmax(maxDiffMat)\n",
    "    maxDiff = max(maxDiffMat)\n",
    "    #print(matTmp1)\n",
    "    return maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1\n",
    "#print(\"最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "#print(“所有合并后的所有矩阵，数目和概率\",matTmp,matTmp1)\n",
    "#print(“各个点合并后的正确率提升矩阵\",maxDiffMat)\n",
    "\n",
    "print(\"\\n\\n\\n###################################################\")\n",
    "print(\"\\n\\n\\n 用数据进行分析\")\n",
    "\n",
    "#1层为5到4层\n",
    "accy = [mat2acc[0,0],mat2acc[1,1],mat2acc[2,2],mat2acc[3,3],mat2acc[4,4]]\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1num,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"\\n\\n\\n5->4,最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(maxDiffMat)\n",
    "print(\"数目矩阵\\n\",matTmp1[maxIndex])\n",
    "print(\"概率矩阵\\n\",np.around(matTmp[maxIndex], decimals=3))\n",
    "hierachFloor ['floor1'] = {'num_mat': matTmp1,'prob_mat': matTmp,'mergeIndex':maxIndex,'mergediffMat':maxDiffMat}\n",
    "\n",
    "###2层4->3\n",
    "mat1num=matTmp1[maxIndex]\n",
    "mat2acc= [matTmp[maxIndex][0,0],matTmp[maxIndex][1,1],matTmp[maxIndex][2,2],matTmp[maxIndex][3,3]]\n",
    "\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1num,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"\\n\\n\\n4->3,最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(maxDiffMat)\n",
    "print(\"数目矩阵\\n\",matTmp1[maxIndex])\n",
    "print(\"概率矩阵\\n\",np.around(matTmp[maxIndex], decimals=3))\n",
    "      \n",
    "hierachFloor ['floor2'] = {'num_mat': matTmp1,'prob_mat': matTmp,'mergeIndex':maxIndex,'mergediffMat':maxDiffMat}\n",
    " \n",
    "###3层3->2\n",
    "mat1num=matTmp1[maxIndex]\n",
    "mat2acc= [matTmp[maxIndex][0,0],matTmp[maxIndex][1,1],matTmp[maxIndex][2,2]]\n",
    "\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1num,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"\\n\\n\\n3->2,最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(maxDiffMat)\n",
    "print(\"数目矩阵\\n\",matTmp1[maxIndex])\n",
    "print(\"概率矩阵\\n\",np.around(matTmp[maxIndex], decimals=3))\n",
    "      \n",
    "hierachFloor['floor3'] = {'num_mat': matTmp1,'prob_mat': matTmp,'mergeIndex':maxIndex,'mergediffMat':maxDiffMat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d3516f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data\n",
      "x.shape: (844538, 22) y.shape: (844538, 4)\n",
      "x.shape: (844538, 22) yl5.shape: (844538,)\n",
      "x.shape: (844538, 22) y.shape: (844538,) y.type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-57-880209cf23e6>, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-57-880209cf23e6>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    return\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "########################################融合决策树和多层神经网络###########################################################\n",
    "#######################################第四步根据聚类和识别结果，开始微调\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "\n",
    "\n",
    "##################\n",
    "file1 = \"./trainData/dataAllSim1000.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "ylabel = y\n",
    "print(\"x.shape:\",x.shape,\"y.shape:\",y.shape)\n",
    "\n",
    "yl5= y[:,2]#01234\n",
    "print(\"x.shape:\",x.shape,\"yl5.shape:\",yl5.shape)\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "\n",
    "print(\"x.shape:\",x.shape,\"y.shape:\",yl5.shape,\"y.type:\", type(yl5) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "\n",
    "\n",
    "def getKerasModeFloors(x,y,saveName):\n",
    "    model_name = saveName \n",
    "    model = keras.models.load_model(model_name)\n",
    "    yP5= model.predict([x], batch_size=2560)\n",
    "    nSamples = yP5.shape[0]\n",
    "     ###需要将预测出的值，转换01整数,并转为数字式\n",
    "    for i in range(yP5.shape[0]):\n",
    "        tmp = yP5[i]\n",
    "        index=  np.argmax(tmp)\n",
    "        yP5[i] = [0,0,0,0,0]\n",
    "        yP5[i,index]=1\n",
    "   \n",
    "\n",
    "    ###\n",
    "    enc = OneHotEncoder()\n",
    "    yl5= y[:,2]#01234\n",
    "    yl5 = np.array(yl5)\n",
    "    yl5= yl5.reshape(nSamples,-1)\n",
    "    print(yl5)\n",
    "    enc.fit(yl5)\n",
    "\n",
    "    yP5= enc.inverse_transform(yP5)\n",
    "    yP5= yP5.reshape(-1,nSamples)[0]\n",
    "    \n",
    "    yP4 = np.zeros((yP5.shape[0],1))\n",
    "    yP3 = np.zeros((yP5.shape[0],1))\n",
    "    yP2 = np.zeros((yP5.shape[0],1))\n",
    "\n",
    "    for i in range(yP5.shape[0]):\n",
    "        if(yP5[i]== 2) or (yP5[i]== 1):\n",
    "             yP4[i] = 21\n",
    "        else:\n",
    "             yP4[i] = yP5[i]\n",
    "                \n",
    "        if(yP5[i]== 2) or (yP5[i]== 1) or (yP5[i]== 0):\n",
    "             yP3[i] = 210\n",
    "        else:\n",
    "             yP3[i] = yP5[i]\n",
    "                \n",
    "        if(yP5[i]== 2) or (yP5[i]== 1) or (yP5[i]== 0) or (yP5[i]== 3):\n",
    "             yP2[i] = 3210\n",
    "        else:\n",
    "             yP2[i] = yP5[i]\n",
    "    \n",
    "    return model,yP5,yP4,yP3,yP2\n",
    "\n",
    "#分层决策树\n",
    "def dtFitAndSave(x,y,saveName):\n",
    "    dt = tree.DecisionTreeClassifier(max_depth=10,min_samples_leaf=1000)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    data=tree.export_graphviz(dt, out_file=None,class_names=None,filled=True) \n",
    "    graph = graphviz.Source(data)\n",
    "    graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(tmp1)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    #text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    return dt\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "def getDTSamplesInfo(x,dt):\n",
    "    yPredict = dt.predict_proba(x)\n",
    "    #print(\"\\n\\n getDTSamplesInfo yPredict\",yPredict)\n",
    "    d_path = dt.decision_path(x).todense()\n",
    "    #print(\"\\n\\n d_path\",d_path)\n",
    "    #print(\"impurity\",dt.tree_.impurity)\n",
    "    #print(\"feature\",dt.tree_.feature)\n",
    "    #print(\"threshold\",dt.tree_.threshold)\n",
    "    \n",
    "    #左节点编号  :  clf.tree_.children_left\n",
    "    #右节点编号  :  clf.tree_.children_right\n",
    "    #分割的变量  :  clf.tree_.feature\n",
    "    #分割的阈值  :  clf.tree_.threshold\n",
    "    #不纯度(gini) :  clf.tree_.impurity\n",
    "    #样本个数      :  clf.tree_.n_node_samples\n",
    "    #样本分布      :  clf.tree_.value\n",
    "    #https://blog.csdn.net/ywj_1991/article/details/122985778\n",
    "    #https://www.javaroad.cn/questions/54003\n",
    "    \n",
    "    h,w = d_path.shape\n",
    "    gini =np.zeros((h,1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(h):\n",
    "       path = d_path[i]\n",
    "       v,ind = np.where(path>0)\n",
    "       xtmp = x[i]\n",
    "       #print(\"path\",path,ind,np.array(ind)[-1])\n",
    "    \n",
    "       #print(\"\\n index\",index)\n",
    "       #print(\"impurity\",dt.tree_.impurity[ind])\n",
    "       #print(\"feature\",dt.tree_.feature[ind])\n",
    "       #print(\"threshold\",dt.tree_.threshold[ind])\n",
    "       #print(\"x[index]\",xtmp[ind])\n",
    "       \n",
    "      \n",
    "       #print(\"the leaf node:\",np.array(ind)[-1],\"the simplest rule is\")\n",
    "       #for jj in ind:\n",
    "       #    if dt.tree_.feature[jj] == -2:\n",
    "       #         print(\"label,proba is\",yPredict[i,0],yPredict[i,1])\n",
    "       #         break\n",
    "                \n",
    "       #    if xtmp[jj]<=dt.tree_.threshold[jj]:\n",
    "       #       print(\" x[%d]<=%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "       #    else:\n",
    "       #       print(\" x[%d]>%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "                    \n",
    "       finalPos = np.array(ind)[-1]\n",
    "       gini[i] = dt.tree_.impurity[finalPos]\n",
    "       \n",
    "       #print(\"d_path\",i,path,dt.tree_.impurity[finalPos])\n",
    "       #print(dt.tree_.feature[finalPos])\n",
    "       #print(dt.tree_.threshold[finalPos])\n",
    "       #print(dt.tree_.n_node_samples[finalPos])\n",
    "\n",
    "    \n",
    "    return gini,yPredict\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "\n",
    "index = np.where((yl5 == 2) | (yl5 == 1))\n",
    "yl4 = yl5.copy()\n",
    "yl4[index]=21\n",
    "print(yl4)\n",
    "\n",
    "\n",
    "index = np.where((yl5 == 2) | (yl5 == 1) | (yl5 == 0))\n",
    "yl3 = yl5.copy()\n",
    "yl3[index]=210\n",
    "print(yl3)\n",
    "\n",
    "\n",
    "\n",
    "index = np.where( (yl5 == 3)|(yl5 == 2) | (yl5 == 1) | (yl5 == 0))\n",
    "yl2 = yl5.copy()\n",
    "yl2[index]=3210\n",
    "print(yl2)\n",
    "\n",
    "hierachFloor['floor3'][\"dt\"] = dtFitAndSave(x,yl2,\"Floo3_2\")\n",
    "hierachFloor['floor2'][\"dt\"] = dtFitAndSave(x,yl3,\"Floo2_3\")\n",
    "hierachFloor['floor1'][\"dt\"] = dtFitAndSave(x,yl4,\"Floor1_4\")\n",
    "hierachFloor['floor0'][\"dt\"] = dtFitAndSave(x,yl5,\"Floor0_5\")\n",
    "\n",
    "#giniFloor0,yPredictProFloor0 = getDTSamplesInfo(x,hierachFloor['floor0'][\"dt\"])\n",
    "#giniFloor1,yPredictProFloor1 = getDTSamplesInfo(x,hierachFloor['floor1'][\"dt\"])\n",
    "#giniFloor2,yPredictProFloor2 = getDTSamplesInfo(x,hierachFloor['floor2'][\"dt\"])\n",
    "#giniFloor3,yPredictProFloor3 = getDTSamplesInfo(x,hierachFloor['floor3'][\"dt\"])\n",
    "\n",
    "kerasFloors,yKerasP5,yKerasP4,yKerasP3,yKerasP2=getKerasModeFloors(x,y,'kerasSimple2.h5')\n",
    "##############开始混合检测\n",
    "\n",
    "###0层，5标签\n",
    "\n",
    "'''\n",
    "nSamples,feturesNume  = x.shape\n",
    "yHyLabelFloor0 = np.zeros((nSamples,1))\n",
    "hyCounter = 0\n",
    "for i in range(nSamples):\n",
    "    print(i)\n",
    "    xtmp = x[i]\n",
    "    dt = hierachFloor['floor0'][\"dt\"]\n",
    "    giniFloor0,yPredictProFloor0 = getDTSamplesInfo([xtmp],dt)\n",
    "    giniTmp = giniFloor0[0]\n",
    "    yPredictProFloor0Tmp = yPredictProFloor0[0]\n",
    "    #print('gini',giniTmp )\n",
    "    #print('probPredict',yPredictProFloor0Tmp  )\n",
    "    if giniTmp >0.05 or max(yPredictProFloor0Tmp)<0.98:\n",
    "        yHyLabelFloor0[i] = yKerasP5[i]\n",
    "    else:\n",
    "        yHyLabelFloor0[i] = np.argmax(yPredictProFloor0)\n",
    "        hyCounter = hyCounter+1\n",
    "  \n",
    "print('O层5标签hyCounter',hyCounter)    \n",
    "\n",
    "\n",
    "tmp1 = classification_report(yl5,yHyLabelFloor0)\n",
    "print('hybrid\\n',tmp1)\n",
    "tmp1 = classification_report( yKerasP5,yHyLabelFloor0)\n",
    "print('keras\\n',tmp1)\n",
    "mat1num = confusion_matrix(yl5,yHyLabelFloor0)\n",
    "mat2acc = confusion_matrix(yl5,yHyLabelFloor0,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "'''\n",
    "\n",
    "'''\n",
    "#############1层，4标签\n",
    "nSamples,feturesNume  = x.shape\n",
    "yHyLabelFloor1 = np.zeros((nSamples,1))\n",
    "hyCounter = 0\n",
    "for i in range(nSamples):\n",
    "    print(i)\n",
    "    xtmp = x[i]\n",
    "    dt = hierachFloor['floor1'][\"dt\"]\n",
    "    giniFloor1,yPredictProFloor1 = getDTSamplesInfo([xtmp],dt)\n",
    "    giniTmp = giniFloor1[0]\n",
    "    yPredictProFloor1Tmp = yPredictProFloor1[0]\n",
    "    #print('gini',giniTmp )\n",
    "    #print('probPredict',yPredictProFloor0Tmp  )\n",
    "    if giniTmp >0.05 or max(yPredictProFloor1Tmp)<0.98:\n",
    "        yHyLabelFloor1[i] = yKerasP4[i]\n",
    "    else:\n",
    "        tmp0= [0,3,4,21]\n",
    "        index = np.argmax(yPredictProFloor1)\n",
    "        yHyLabelFloor1[i] = tmp0[index]\n",
    "        hyCounter = hyCounter+1\n",
    "print('O层4标签hyCounter',hyCounter)    \n",
    "\n",
    "\n",
    "tmp1 = classification_report(yl4,yHyLabelFloor1)\n",
    "print('hybrid\\n',tmp1)\n",
    "tmp1 = classification_report( yKerasP4,yHyLabelFloor1)\n",
    "print('keras\\n',tmp1)\n",
    "mat1num = confusion_matrix(yl4,yHyLabelFloor1)\n",
    "mat2acc = confusion_matrix(yl4,yHyLabelFloor1,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "'''\n",
    "\n",
    "\n",
    "def computeAndCompareHybridMode(x,y,dt,kerasPLabel,floorLabel):\n",
    "    nSamples,feturesNume  = x.shape\n",
    "    yHyLabel  = np.zeros((nSamples,1))\n",
    "    hyCounter = 0\n",
    "    for i in range(nSamples):\n",
    "        print(i)\n",
    "        xtmp = x[i]\n",
    "        giniFloor,yPredictProFloor = getDTSamplesInfo([xtmp],dt)\n",
    "        giniTmp = giniFloor[0]\n",
    "        yPredictProFloorTmp = yPredictProFloor[0]\n",
    "        #print('gini',giniTmp )\n",
    "        #print('probPredict',yPredictProFloor0Tmp  )\n",
    "        if giniTmp >0.05 or max(yPredictProFloorTmp)<0.98:\n",
    "            yHyLabel[i] = kerasPLabel[i]\n",
    "        else:\n",
    "            #floorLabel= [3,4,210]\n",
    "            index = np.argmax(yPredictProFloorTmp)\n",
    "            yHyLabel[i] = floorLabel[index]\n",
    "            hyCounter = hyCounter+1\n",
    "    print('floorLabel\\n',floorLabel) \n",
    "    print('hyCounter\\n',hyCounter)    \n",
    "\n",
    "\n",
    "    tmp1 = classification_report(y,yHyLabel)\n",
    "    print('hybrid\\n',tmp1)\n",
    "    tmp1 = classification_report(kerasPLabel,yHyLabel)\n",
    "    print('keras\\n',tmp1)\n",
    "    mat1num = confusion_matrix(y,yHyLabel)\n",
    "    mat2acc = confusion_matrix(y,yHyLabel,normalize='pred')\n",
    "    print('mat1num\\n',mat1num)\n",
    "    print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "    return\n",
    "\n",
    "#floor=2 ,label=3\n",
    "#dt = hierachFloor['floor2'][\"dt\"]\n",
    "#floorLabel= [3,4,210] \n",
    "#computeAndCompareHybridMode(x,yl3,dt,yKerasP3,floorLabel)\n",
    "\n",
    "#floor=3 ,label=2\n",
    "dt = hierachFloor['floor3'][\"dt\"]\n",
    "floorLabel= [4,3210] \n",
    "computeAndCompareHybridMode(x,yl2,dt,yKerasP2,floorLabel)\n",
    "\n",
    "return\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "'''\n",
    "for i in range(nSamples):\n",
    "    for j in range(4)\n",
    "         input1 = x[i,:]\n",
    "         label =  ylabel[i,j]\n",
    "         output1 = Floor[j][\"dt\"].predict_proba(input)\n",
    "         output_gini,output_num =getDT_Info(input)\n",
    "    \n",
    "        if max(output1)<0.95 or output_gini>0.2\n",
    "            output1 =  Floor[j][\"keras\"].predict(input)\n",
    "        \n",
    "        loss=loss+(output1-label)\n",
    "                           \n",
    "                           \n",
    "def getDT_SamplesInfo(x)\n",
    "    yPredict = dt.predict_proba(x[0:3,:])\n",
    "    print(yPredict[:,1])\n",
    "    d_path = dt.decision_path(x[0:3,:]).todense()\n",
    "    print(d_path)\n",
    "    print(\"impurity\",dt.tree_.impurity)\n",
    "    print(\"feature\",dt.tree_.feature)\n",
    "    print(\"threshold\",dt.tree_.threshold)\n",
    "    \n",
    "    #左节点编号  :  clf.tree_.children_left\n",
    "    #右节点编号  :  clf.tree_.children_right\n",
    "    #分割的变量  :  clf.tree_.feature\n",
    "    #分割的阈值  :  clf.tree_.threshold\n",
    "    #不纯度(gini) :  clf.tree_.impurity\n",
    "    #样本个数      :  clf.tree_.n_node_samples\n",
    "    #样本分布      :  clf.tree_.value\n",
    "    #https://blog.csdn.net/ywj_1991/article/details/122985778\n",
    "    #https://www.javaroad.cn/questions/54003\n",
    "\n",
    "    w,h = d_path.shape\n",
    "    for i in range(h):\n",
    "       path = d_path[i]\n",
    "       v,ind = np.where(path>0)\n",
    "       xtmp = x[i]\n",
    "       #print(\"path\",path,ind,np.array(ind)[-1])\n",
    "    \n",
    "       print(\"\\n index\",index)\n",
    "       print(\"impurity\",dt.tree_.impurity[ind])\n",
    "       print(\"feature\",dt.tree_.feature[ind])\n",
    "       print(\"threshold\",dt.tree_.threshold[ind])\n",
    "       print(\"x[index]\",xtmp[ind])\n",
    "       \n",
    "      \n",
    "       print(\"the leaf node:\",np.array(ind)[-1],\"the simplest rule is\")\n",
    "       for jj in ind:\n",
    "           if dt.tree_.feature[jj] == -2:\n",
    "                print(\"label,proba is\",yPredict[i,0],yPredict[i,1])\n",
    "                break\n",
    "                \n",
    "           if xtmp[jj]<=dt.tree_.threshold[jj]:\n",
    "              print(\" x[%d]<=%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "           else:\n",
    "              print(\" x[%d]>%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "                    \n",
    "       finalPos = np.array(ind)[-1]\n",
    "       print(dt.tree_.impurity[finalPos])\n",
    "       print(dt.tree_.feature[finalPos])\n",
    "       print(dt.tree_.threshold[finalPos])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd18e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data france\n",
      "x.shape: (71998, 22) y.shape: (71998,) y.type: <class 'numpy.ndarray'>\n",
      "reading data\n",
      "x.shape: (71998, 22) yl5.shape: (916536, 1)\n",
      "x.shape: (916536, 22) y.shape: (916536,) y.type: <class 'numpy.ndarray'>\n",
      "Epoch 1/1000\n",
      "183307/183307 [==============================] - 7s 39us/step - loss: 0.6756 - acc: 0.8140\n",
      "Epoch 2/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.5280 - acc: 0.8727\n",
      "Epoch 3/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.4974 - acc: 0.8880\n",
      "Epoch 4/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.4456 - acc: 0.8931\n",
      "Epoch 5/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.3996 - acc: 0.9015\n",
      "Epoch 6/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.3653 - acc: 0.9014\n",
      "Epoch 7/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.3349 - acc: 0.9003\n",
      "Epoch 8/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.2955 - acc: 0.9026\n",
      "Epoch 9/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.2540 - acc: 0.9127\n",
      "Epoch 10/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.2352 - acc: 0.9156\n",
      "Epoch 11/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.2316 - acc: 0.9170\n",
      "Epoch 12/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.2221 - acc: 0.9192\n",
      "Epoch 13/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.2099 - acc: 0.9212\n",
      "Epoch 14/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.2018 - acc: 0.9227\n",
      "Epoch 15/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1976 - acc: 0.9239\n",
      "Epoch 16/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1936 - acc: 0.9252\n",
      "Epoch 17/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1886 - acc: 0.9263\n",
      "Epoch 18/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1850 - acc: 0.9266\n",
      "Epoch 19/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1815 - acc: 0.9281\n",
      "Epoch 20/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1774 - acc: 0.9289\n",
      "Epoch 21/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1743 - acc: 0.9302\n",
      "Epoch 22/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1715 - acc: 0.9310\n",
      "Epoch 23/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1684 - acc: 0.9316\n",
      "Epoch 24/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1655 - acc: 0.9321\n",
      "Epoch 25/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1625 - acc: 0.9334\n",
      "Epoch 26/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1600 - acc: 0.9343\n",
      "Epoch 27/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1573 - acc: 0.9355\n",
      "Epoch 28/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1545 - acc: 0.9364\n",
      "Epoch 29/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1524 - acc: 0.9367\n",
      "Epoch 30/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1499 - acc: 0.9377\n",
      "Epoch 31/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1478 - acc: 0.9387\n",
      "Epoch 32/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1457 - acc: 0.9394\n",
      "Epoch 33/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1434 - acc: 0.9397\n",
      "Epoch 34/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1415 - acc: 0.9408\n",
      "Epoch 35/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1394 - acc: 0.9414\n",
      "Epoch 36/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1376 - acc: 0.9423\n",
      "Epoch 37/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1359 - acc: 0.9426\n",
      "Epoch 38/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1342 - acc: 0.9432\n",
      "Epoch 39/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1325 - acc: 0.9441\n",
      "Epoch 40/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1309 - acc: 0.9445\n",
      "Epoch 41/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1295 - acc: 0.9449\n",
      "Epoch 42/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1279 - acc: 0.9453\n",
      "Epoch 43/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1268 - acc: 0.9459\n",
      "Epoch 44/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1250 - acc: 0.9464\n",
      "Epoch 45/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1239 - acc: 0.9469\n",
      "Epoch 46/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1224 - acc: 0.9475\n",
      "Epoch 47/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1215 - acc: 0.9475\n",
      "Epoch 48/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1202 - acc: 0.9478\n",
      "Epoch 49/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1189 - acc: 0.9487\n",
      "Epoch 50/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1176 - acc: 0.9491\n",
      "Epoch 51/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1167 - acc: 0.9494\n",
      "Epoch 52/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1158 - acc: 0.9496\n",
      "Epoch 53/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1147 - acc: 0.9502\n",
      "Epoch 54/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1139 - acc: 0.9505\n",
      "Epoch 55/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1129 - acc: 0.9506\n",
      "Epoch 56/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1119 - acc: 0.9510\n",
      "Epoch 57/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1112 - acc: 0.9512\n",
      "Epoch 58/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1101 - acc: 0.9516\n",
      "Epoch 59/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1091 - acc: 0.9520\n",
      "Epoch 60/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1084 - acc: 0.9525\n",
      "Epoch 61/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1071 - acc: 0.9528\n",
      "Epoch 62/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1066 - acc: 0.9531\n",
      "Epoch 63/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1061 - acc: 0.9533\n",
      "Epoch 64/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1051 - acc: 0.9534\n",
      "Epoch 65/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1043 - acc: 0.9540\n",
      "Epoch 66/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.1036 - acc: 0.9542\n",
      "Epoch 67/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1029 - acc: 0.9545\n",
      "Epoch 68/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1022 - acc: 0.9549\n",
      "Epoch 69/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1015 - acc: 0.9551\n",
      "Epoch 70/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1008 - acc: 0.9551\n",
      "Epoch 71/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.1002 - acc: 0.9558\n",
      "Epoch 72/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0998 - acc: 0.9556\n",
      "Epoch 73/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0990 - acc: 0.9560\n",
      "Epoch 74/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.0985 - acc: 0.9562\n",
      "Epoch 75/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.0980 - acc: 0.9561\n",
      "Epoch 76/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.0978 - acc: 0.9567\n",
      "Epoch 77/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.0972 - acc: 0.9566\n",
      "Epoch 78/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0968 - acc: 0.9567\n",
      "Epoch 79/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0964 - acc: 0.9571\n",
      "Epoch 80/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.0959 - acc: 0.9572\n",
      "Epoch 81/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0954 - acc: 0.9571\n",
      "Epoch 82/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0947 - acc: 0.9577\n",
      "Epoch 83/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.0944 - acc: 0.9579\n",
      "Epoch 84/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0936 - acc: 0.9578\n",
      "Epoch 85/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0931 - acc: 0.9582\n",
      "Epoch 86/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.0931 - acc: 0.9581\n",
      "Epoch 87/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0929 - acc: 0.9583\n",
      "Epoch 88/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.0922 - acc: 0.9590\n",
      "Epoch 89/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0919 - acc: 0.9586\n",
      "Epoch 90/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.0914 - acc: 0.9589\n",
      "Epoch 91/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0910 - acc: 0.9590\n",
      "Epoch 92/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0907 - acc: 0.9591\n",
      "Epoch 93/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0904 - acc: 0.9594\n",
      "Epoch 94/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.0900 - acc: 0.9595\n",
      "Epoch 95/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0895 - acc: 0.9597\n",
      "Epoch 96/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0895 - acc: 0.9596\n",
      "Epoch 97/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.0891 - acc: 0.9598\n",
      "Epoch 98/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.0887 - acc: 0.9600\n",
      "Epoch 99/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0882 - acc: 0.9600\n",
      "Epoch 100/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.0882 - acc: 0.9600\n",
      "Epoch 101/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.0879 - acc: 0.9605\n",
      "Epoch 102/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.0871 - acc: 0.9606\n",
      "Epoch 103/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0870 - acc: 0.9605\n",
      "Epoch 104/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.0869 - acc: 0.9606\n",
      "Epoch 105/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.0864 - acc: 0.9610\n",
      "Epoch 106/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0863 - acc: 0.9608\n",
      "Epoch 107/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0863 - acc: 0.9610\n",
      "Epoch 108/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.0859 - acc: 0.9611\n",
      "Epoch 109/1000\n",
      "183307/183307 [==============================] - 5s 30us/step - loss: 0.0858 - acc: 0.9610\n",
      "Epoch 110/1000\n",
      "183307/183307 [==============================] - 5s 29us/step - loss: 0.0852 - acc: 0.9615\n",
      "Epoch 111/1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "\n",
    "\n",
    "def getKerasModeFloors2(x,enc,saveName):\n",
    "    model_name = saveName \n",
    "    model = keras.models.load_model(model_name)\n",
    "    yP5= model.predict([x], batch_size=2560)\n",
    "    nSamples = yP5.shape[0]\n",
    "     ###需要将预测出的值，转换01整数,并转为数字式\n",
    "    for i in range(yP5.shape[0]):\n",
    "        tmp = yP5[i]\n",
    "        index=  np.argmax(tmp)\n",
    "        yP5[i] = [0,0,0,0,0]\n",
    "        yP5[i,index]=1\n",
    "   \n",
    "\n",
    "    ###  \n",
    "    yP5= enc.inverse_transform(yP5)\n",
    "    yP5= yP5.reshape(-1,nSamples)[0]\n",
    "    \n",
    "    yP4 = np.zeros((yP5.shape[0],1))\n",
    "    yP3 = np.zeros((yP5.shape[0],1))\n",
    "    yP2 = np.zeros((yP5.shape[0],1))\n",
    "\n",
    "    for i in range(yP5.shape[0]):\n",
    "        if(yP5[i]== 2) or (yP5[i]== 1):\n",
    "             yP4[i] = 21\n",
    "        else:\n",
    "             yP4[i] = yP5[i]\n",
    "                \n",
    "        if(yP5[i]== 2) or (yP5[i]== 1) or (yP5[i]== 0):\n",
    "             yP3[i] = 210\n",
    "        else:\n",
    "             yP3[i] = yP5[i]\n",
    "                \n",
    "        if(yP5[i]== 2) or (yP5[i]== 1) or (yP5[i]== 0) or (yP5[i]== 3):\n",
    "             yP2[i] = 3210\n",
    "        else:\n",
    "             yP2[i] = yP5[i]\n",
    "    \n",
    "    return model,yP5,yP4,yP3,yP2\n",
    "\n",
    "#分层决策树\n",
    "def dtFitAndSave(x,y,saveName):\n",
    "    dt = tree.DecisionTreeClassifier(max_depth=10,min_samples_leaf=1000)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    data=tree.export_graphviz(dt, out_file=None,class_names=None,filled=True) \n",
    "    graph = graphviz.Source(data)\n",
    "    graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(tmp1)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    #text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    return dt\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "def getDTSamplesInfo(x,dt):\n",
    "    yPredict = dt.predict_proba(x)\n",
    "    #print(\"\\n\\n getDTSamplesInfo yPredict\",yPredict)\n",
    "    d_path = dt.decision_path(x).todense()\n",
    "    #print(\"\\n\\n d_path\",d_path)\n",
    "    #print(\"impurity\",dt.tree_.impurity)\n",
    "    #print(\"feature\",dt.tree_.feature)\n",
    "    #print(\"threshold\",dt.tree_.threshold)\n",
    "    \n",
    "    #左节点编号  :  clf.tree_.children_left\n",
    "    #右节点编号  :  clf.tree_.children_right\n",
    "    #分割的变量  :  clf.tree_.feature\n",
    "    #分割的阈值  :  clf.tree_.threshold\n",
    "    #不纯度(gini) :  clf.tree_.impurity\n",
    "    #样本个数      :  clf.tree_.n_node_samples\n",
    "    #样本分布      :  clf.tree_.value\n",
    "    #https://blog.csdn.net/ywj_1991/article/details/122985778\n",
    "    #https://www.javaroad.cn/questions/54003\n",
    "    \n",
    "    h,w = d_path.shape\n",
    "    gini =np.zeros((h,1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(h):\n",
    "       path = d_path[i]\n",
    "       v,ind = np.where(path>0)\n",
    "       xtmp = x[i]\n",
    "       #print(\"path\",path,ind,np.array(ind)[-1])\n",
    "    \n",
    "       #print(\"\\n index\",index)\n",
    "       #print(\"impurity\",dt.tree_.impurity[ind])\n",
    "       #print(\"feature\",dt.tree_.feature[ind])\n",
    "       #print(\"threshold\",dt.tree_.threshold[ind])\n",
    "       #print(\"x[index]\",xtmp[ind])\n",
    "       \n",
    "      \n",
    "       #print(\"the leaf node:\",np.array(ind)[-1],\"the simplest rule is\")\n",
    "       #for jj in ind:\n",
    "       #    if dt.tree_.feature[jj] == -2:\n",
    "       #         print(\"label,proba is\",yPredict[i,0],yPredict[i,1])\n",
    "       #         break\n",
    "                \n",
    "       #    if xtmp[jj]<=dt.tree_.threshold[jj]:\n",
    "       #       print(\" x[%d]<=%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "       #    else:\n",
    "       #       print(\" x[%d]>%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "                    \n",
    "       finalPos = np.array(ind)[-1]\n",
    "       gini[i] = dt.tree_.impurity[finalPos]\n",
    "       \n",
    "       #print(\"d_path\",i,path,dt.tree_.impurity[finalPos])\n",
    "       #print(dt.tree_.feature[finalPos])\n",
    "       #print(dt.tree_.threshold[finalPos])\n",
    "       #print(dt.tree_.n_node_samples[finalPos])\n",
    "\n",
    "    \n",
    "       return gini,yPredict\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "def computeAndCompareHybridMode(x,y,dt,kerasPLabel,floorLabel):\n",
    "    nSamples,feturesNume  = x.shape\n",
    "    yHyLabel  = np.zeros((nSamples,1))\n",
    "    hyCounter = 0\n",
    "    for i in range(nSamples):\n",
    "        print(i)\n",
    "        xtmp = x[i]\n",
    "        giniFloor,yPredictProFloor = getDTSamplesInfo([xtmp],dt)\n",
    "        giniTmp = giniFloor[0]\n",
    "        yPredictProFloorTmp = yPredictProFloor[0]\n",
    "        #print('gini',giniTmp )\n",
    "        #print('probPredict',yPredictProFloor0Tmp  )\n",
    "        if giniTmp >0.05 or max(yPredictProFloorTmp)<0.98:\n",
    "            yHyLabel[i] = kerasPLabel[i]\n",
    "        else:\n",
    "            #floorLabel= [3,4,210]\n",
    "            index = np.argmax(yPredictProFloorTmp)\n",
    "            yHyLabel[i] = floorLabel[index]\n",
    "            hyCounter = hyCounter+1\n",
    "    print('floorLabel\\n',floorLabel) \n",
    "    print('hyCounter\\n',hyCounter)    \n",
    "\n",
    "\n",
    "    tmp1 = classification_report(y,yHyLabel)\n",
    "    print('hybrid\\n',tmp1)\n",
    "    tmp1 = classification_report(y,kerasPLabel)\n",
    "    print('keras\\n',tmp1)\n",
    "    mat1num = confusion_matrix(y,yHyLabel)\n",
    "    mat2acc = confusion_matrix(y,yHyLabel,normalize='pred')\n",
    "    print('mat1num\\n',mat1num)\n",
    "    print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "    return\n",
    "\n",
    "##########################################################################\n",
    "###简单模型2，有隐藏层\n",
    "def kerasFitAndSaveSimple2(x,yOneHot,num_labels):\n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 512\n",
    "    dropout_rate = 0.05\n",
    "    build_model = tf.keras.Sequential()\n",
    "    build_model.add(layers.Dense(relu_size, activation='relu',name=\"layer1\",input_shape=(features_size,)))\n",
    "    build_model.add(layers.Dropout(dropout_rate,name=\"Dropout1-2\"))\n",
    "    build_model.add(layers.Dense(relu_size/2, activation='relu',name=\"layer2\"))\n",
    "    build_model.add(layers.Dropout(dropout_rate,name=\"Dropout2-3\"))\n",
    "    build_model.add(layers.Dense(num_labels, activation='sigmoid',name=\"layer3\"))\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    build_model.fit([x],[yOneHot],epochs=1000, batch_size=80000*1)\n",
    "    build_model.save(\"kerasSimple2.h5\")\n",
    "    plot_model(build_model, to_file='KerasSimple2_HiddenLayer.png', show_shapes=True)\n",
    "    return build_model\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####用法国数据进行验证\n",
    "file1 = \"./trainData/france_0_allSamples.csv\"\n",
    "print(\"reading data france\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "h,w = xyData.shape\n",
    "x = xyData[:,1:23]#简单处理与SUMO数据库一致\n",
    "y = xyData[:,w-1]\n",
    "y= y.astype('int64')\n",
    "\n",
    "print(\"x.shape:\",x.shape,\"y.shape:\",y.shape,\"y.type:\", type(y) )\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "\n",
    "####\n",
    "file1 = \"./trainData/dataAllSim1000.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "\n",
    "xSumo = xyData[:,0:22]\n",
    "ySumo = xyData[:,22:26]\n",
    "ySumo= ySumo[:,2]#01234\n",
    "\n",
    "\n",
    "print(\"x.shape:\",x.shape,\"yl5.shape:\",yl5.shape)\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "\n",
    "\n",
    "x = np.concatenate((xSumo,x))\n",
    "y = np.concatenate((ySumo,y))\n",
    "yl5 = y\n",
    "print(\"x.shape:\",x.shape,\"y.shape:\",yl5.shape,\"y.type:\", type(yl5) )\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "###keras拟合,oneHot\n",
    "nSamples,nFeatures =  x.shape\n",
    "enc = OneHotEncoder()\n",
    "yl5= yl5.reshape(nSamples,-1)\n",
    "enc.fit(yl5)  \n",
    "\n",
    "##keras拟合\n",
    "if 0\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, yl5, test_size = 0.8)\n",
    "    yOneHot=enc.transform(y_train).toarray()\n",
    "    num_labels = 5\n",
    "    simpleMode2 = kerasFitAndSaveSimple2(x_train,yOneHot,num_labels)\n",
    "return\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "index = np.where((yl5 == 2) | (yl5 == 1))\n",
    "yl4 = yl5.copy()\n",
    "yl4[index]=21\n",
    "print(yl4)\n",
    "\n",
    "\n",
    "index = np.where((yl5 == 2) | (yl5 == 1) | (yl5 == 0))\n",
    "yl3 = yl5.copy()\n",
    "yl3[index]=210\n",
    "print(yl3)\n",
    "\n",
    "\n",
    "\n",
    "index = np.where( (yl5 == 3)|(yl5 == 2) | (yl5 == 1) | (yl5 == 0))\n",
    "yl2 = yl5.copy()\n",
    "yl2[index]=3210\n",
    "print(yl2)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "#hierachFloor['floor3'][\"dt\"] = dtFitAndSave(x,yl2,\"Floo3_2\")\n",
    "#hierachFloor['floor2'][\"dt\"] = dtFitAndSave(x,yl3,\"Floo2_3\")\n",
    "#hierachFloor['floor1'][\"dt\"] = dtFitAndSave(x,yl4,\"Floor1_4\")\n",
    "hierachFloor['floor0'][\"dt\"] = dtFitAndSave(x,yl5,\"Floor0_5\")\n",
    "\n",
    "\n",
    "kerasFloors,yKerasP5,yKerasP4,yKerasP3,yKerasP2=getKerasModeFloors2(x,enc,'kerasSimple2.h5')\n",
    "\n",
    "\n",
    "#floor=3 ,label=2\n",
    "#dt = hierachFloor['floor3'][\"dt\"]\n",
    "#floorLabel= [4,3210] \n",
    "#computeAndCompareHybridMode(x,yl2,dt,yKerasP2,floorLabel)\n",
    "\n",
    "\n",
    "dt = hierachFloor['floor0'][\"dt\"]\n",
    "floorLabel= [0,1,2,3,4] \n",
    "computeAndCompareHybridMode(x,yl5,dt,yKerasP5,floorLabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b47a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mainTestCSVMLP3(hmcnf_keras).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:keras220CpuJupyter]",
   "language": "python",
   "name": "conda-env-keras220CpuJupyter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
