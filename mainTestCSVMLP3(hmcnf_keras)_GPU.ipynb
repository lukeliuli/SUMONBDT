{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62371e5",
   "metadata": {
    "id": "48f98b91"
   },
   "outputs": [],
   "source": [
    "#mkdir /content/tmp\n",
    "#%cp -r -f -v /content/drive/MyDrive/SUMONBDT /content/tmp\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#%cd /content/drive/MyDrive/SUMONBDT\n",
    "#%cd /home/liuli/github/SUMONBDT\n",
    "#!nvidia-smi\n",
    "#用于测试oneHot\n",
    "#############################################################也是第一步，读取数据\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "#[2,3,5,9]\n",
    "x1 = [0,0,0,0]\n",
    "x2 = [0,0,0,1]\n",
    "\n",
    "x3 = [1,1,1,2]\n",
    "x4 = [1,1,1,3]\n",
    "x5 = [1,1,2,4]\n",
    "x6 = [1,1,2,5]\n",
    "x7 = [1,2,3,6]\n",
    "x8 = [1,2,3,7]\n",
    "x9 = [1,2,4,8]\n",
    "X = [x1, x2, x3,x4,x5,x6,x7,x8,x9]\n",
    "enc.fit(X)\n",
    "#print(enc.transform(X).toarray())\n",
    "\n",
    "\n",
    "########################读写CSV,并转为oneHot\n",
    "file1 = \"./trainData/dataAllSim1000.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "ylabel = y\n",
    "y = enc.transform(y).toarray()\n",
    "\n",
    "print(\"x.shape:\",x.shape,\"yOneHot.shape:\",y.shape)\n",
    "\n",
    "\n",
    "\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad77154",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "03c551ad",
    "outputId": "e1da57c0-4dd7-440a-bf3f-6194c50c0c1c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "################################################################第二步，训练\n",
    "#1. 核心为keras220不是pytorch\n",
    "#2. 基于hmcnf\n",
    "import model_hmcnf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "#hierarchy = [18, 80, 178, 142, 77, 4]\n",
    "hierarchy = [2,3,5,9]\n",
    "features_size = x.shape[1]\n",
    "label_size = y.shape[1]\n",
    "beta = 0.2\n",
    "dropout_rate=0.1\n",
    "relu_size=384\n",
    "\n",
    "\n",
    "\n",
    "def local_model(num_labels, dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(num_labels, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_model(dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    return model\n",
    "\n",
    "\n",
    "def sigmoid_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='sigmoid',name=\"global\"))\n",
    "    return model\n",
    "\n",
    "features = layers.Input(shape=(features_size,))\n",
    "global_models = []\n",
    "local_models = []\n",
    "\n",
    "\n",
    "for i in range(len(hierarchy)):\n",
    "    if i == 0:\n",
    "        global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "    else:\n",
    "        global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "\n",
    "p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "\n",
    "\n",
    "#显示只有全局模型的情况\n",
    "#modelTmp1 = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "#modelTmp1.summary()#\n",
    "#plot_model(modelTmp1, to_file='Flatten1.png', show_shapes=True)\n",
    "\n",
    "\n",
    "for i in range(len(hierarchy)):\n",
    "    local_models.append(local_model(hierarchy[i], dropout_rate, relu_size)(global_models[i]))\n",
    "    \n",
    "#显示只有局部局模型的情况(部分全局)\n",
    "p_loc = layers.concatenate(local_models)\n",
    "#modelTmp2 = tf.keras.Model(inputs=[features], outputs=[p_loc])\n",
    "#modelTmp2.summary()#\n",
    "#plot_model(modelTmp2, to_file='Flatten2.png', show_shapes=True)\n",
    "p_glob1 = layers.Lambda(lambda x: x*beta,name=\"global\")(p_glob)\n",
    "p_loc1 = layers.Lambda(lambda x: x*(1-beta),name=\"local\")(p_loc)\n",
    "\n",
    "labels = layers.add([p_glob1, p_loc1])\n",
    "\n",
    "model = tf.keras.Model(inputs=[features], outputs=[labels])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_model(model, to_file='FlattenAll.png', show_shapes=True)\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['mae'])\n",
    "model.fit([x],[y],epochs=1000, batch_size=25600*1)\n",
    "model.save(\"hmcnf10000.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c77366c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d7beeed",
    "outputId": "d6908600-8597-41c2-800f-afc59a088154"
   },
   "outputs": [],
   "source": [
    "##################################################################第三步，验证\n",
    "#%cd /content/drive/MyDrive/SUMONBDT\n",
    "import model_hmcnf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#######################0.准备onehot\n",
    "enc = OneHotEncoder()\n",
    "#[2,3,5,9]\n",
    "x1 = [0,0,0,0]\n",
    "x2 = [0,0,0,1]\n",
    "\n",
    "x3 = [1,1,1,2]\n",
    "x4 = [1,1,1,3]\n",
    "x5 = [1,1,2,4]\n",
    "x6 = [1,1,2,5]\n",
    "x7 = [1,2,3,6]\n",
    "x8 = [1,2,3,7]\n",
    "x9 = [1,2,4,8]\n",
    "X = [x1, x2, x3,x4,x5,x6,x7,x8,x9]\n",
    "enc.fit(X)\n",
    "\n",
    "#######################2.准备数据\n",
    "        \n",
    "file1 = \"./trainData/dataAllSim10000.csv\"\n",
    "file1 = \"./trainData/dataAllSim.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "ylabel = y\n",
    "y = enc.transform(y).toarray()\n",
    "\n",
    "\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "#######################3.预测模型\n",
    "print(\"3.HMCNF预测模型\")\n",
    "hierarchy = [2,3,5,9]\n",
    "features_size = x.shape[1]\n",
    "label_size = y.shape[1]\n",
    "beta = 0.2\n",
    "\n",
    "model_name =\"hmcnf.h5\" \n",
    "\n",
    "model = keras.models.load_model(model_name)\n",
    "y_out = model.predict([x], batch_size=2560)\n",
    "y_predict = np.where(y_out > 0.5, 1, 0)\n",
    "\n",
    "predict_ok = np.where(np.sum(y_predict - y, axis=1) == 0, 1, 0)\n",
    "\n",
    "\n",
    "print(\"validated {} , {} good out of {} samples\".format(model_name, np.sum(predict_ok), predict_ok.shape[0]))\n",
    "del y_predict #节省内存\n",
    "del predict_ok #节省内存\n",
    "#######################3.层次预测预测模型\n",
    "print(\"3.层次预测预测模型\")\n",
    "y1 = np.where(y_out[:,0:2] > 0.5, 1, 0)\n",
    "y2 = np.where(y_out[:,2:5] > 0.5, 1, 0)\n",
    "y3 = np.where(y_out[:,5:10] > 0.5, 1, 0)\n",
    "y4 = np.where(y_out[:,10:19] > 0.5, 1, 0)\n",
    "for i in range(y4.shape[0]):\n",
    "    tmp1 = y1[i]\n",
    "    tmp2 = y2[i]\n",
    "    tmp3 = y3[i]\n",
    "    tmp4 = y4[i]\n",
    "    if sum(tmp1) == 0:\n",
    "        index=  np.argmax(tmp1)\n",
    "        y1[i,index]=1\n",
    "        \n",
    "    if sum(tmp2) == 0:\n",
    "        index=  np.argmax(tmp2)\n",
    "        y2[i,index]=1\n",
    "        \n",
    "    if sum(tmp3) == 0:\n",
    "        index=  np.argmax(tmp3)\n",
    "        y3[i,index]=1\n",
    "    \n",
    "    if sum(tmp4) == 0:\n",
    "        index=  np.argmax(tmp4)\n",
    "        y4[i,index]=1\n",
    "        #print(i,y4[i],index)\n",
    "y_predict = np.concatenate([y1,y2,y3,y4],axis=1)\n",
    "predict_ok = np.where(np.sum(y_predict - y, axis=1) == 0, 1, 0)\n",
    "print(\"validated {} , {} good out of {} samples\".format(model_name, np.sum(predict_ok), predict_ok.shape[0]))\n",
    "\n",
    "#onehot 2 label\n",
    "ypredict = enc.inverse_transform(y_predict)\n",
    "del y_predict #节省内存\n",
    "del predict_ok #节省内存\n",
    "del y1,y2,y3,y4\n",
    "#######################4.评估层次模型\n",
    "#hierarchy = [2,3,5,9]\n",
    "\n",
    "##第一层，2\n",
    "print(\"###################################第一层，2\")\n",
    "h1_yp = ypredict[:,0]\n",
    "h1_yl = ylabel[:,0]\n",
    "tmp1 = classification_report(h1_yl,h1_yp)\n",
    "tmp2 = confusion_matrix(h1_yl,h1_yp,normalize='true')\n",
    "tmp3 = confusion_matrix(h1_yl,h1_yp,normalize='pred')\n",
    "print(tmp1)\n",
    "print(np.around(tmp2, decimals=3))\n",
    "print(np.around(tmp3, decimals=3))\n",
    "\n",
    "\n",
    "##第二层，3\n",
    "print(\"################################第二层，3\")\n",
    "h2_yp = ypredict[:,1]\n",
    "h2_yl = ylabel[:,1]\n",
    "tmp1 = classification_report(h2_yl,h2_yp)\n",
    "tmp2 = confusion_matrix(h2_yl,h2_yp,normalize='true')\n",
    "tmp3 = confusion_matrix(h2_yl,h2_yp,normalize='pred')\n",
    "print(tmp1)\n",
    "print(np.around(tmp2, decimals=3))\n",
    "print(np.around(tmp3, decimals=3))\n",
    "\n",
    "\n",
    "\n",
    "##第三层，5\n",
    "print(\"#############################第三层，5\")\n",
    "h3_yp = ypredict[:,2]\n",
    "h3_yl = ylabel[:,2]\n",
    "tmp1 = classification_report(h3_yl,h3_yp)\n",
    "tmp2 = confusion_matrix(h3_yl,h3_yp,normalize='true')\n",
    "tmp3 = confusion_matrix(h3_yl,h3_yp,normalize='pred')\n",
    "print(tmp1)\n",
    "print(np.around(tmp2, decimals=3))\n",
    "print(np.around(tmp3, decimals=3))\n",
    "\n",
    "\n",
    "##第四层，9\n",
    "print(\"#############################第四层，9\")\n",
    "h4_yp = ypredict[:,3]\n",
    "h4_yl = ylabel[:,3]\n",
    "tmp1 = classification_report(h4_yl,h4_yp)\n",
    "tmp2 = confusion_matrix(h4_yl,h4_yp,normalize='true')\n",
    "tmp3 = confusion_matrix(h4_yl,h4_yp,normalize='pred')\n",
    "print(tmp1)\n",
    "print(np.around(tmp2, decimals=3))\n",
    "print(np.around(tmp3, decimals=3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04808aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#第四步，根据混淆矩阵进行聚类。第一列代表识别为类别1的样本真实的类别分布\n",
    "import numpy as np\n",
    "import copy\n",
    "#########################################手动准备模拟数据\n",
    "mat1 = np.array([[0.952,0.004,0.015,0.008],\n",
    " [0.018,0.923,0.016,0.032],\n",
    " [0.016,0.036,0.934,0.047],\n",
    " [0.014,0.037,0.035,0.913]])\n",
    "accy = [mat1[0,0],mat1[1,1],mat1[2,2],mat1[3,3]]\n",
    "print(\"accuracy\",accy)\n",
    "matT1 = mat1\n",
    "mat1[:,0] = matT1[:,0]*1000\n",
    "mat1[:,1] = matT1[:,1]*1000\n",
    "mat1[:,2] = matT1[:,2]*1000\n",
    "mat1[:,3] = matT1[:,3]*1000\n",
    "sumTmp =  sum(mat1)\n",
    "print(sumTmp)\n",
    "print(mat1)\n",
    "\n",
    "##########################################计算最佳合并位置，根据最大的正确率提高\n",
    "def computeAccuracyDiff(mat1,accy):\n",
    "    h,w = mat1.shape\n",
    "    tmp = np.zeros((h-1,w-1))\n",
    "    matTmp={}\n",
    "    ##从0到最后，行列合并\n",
    "    for index in range(h-1):\n",
    "\n",
    "        tmp = np.zeros((h-1,w))\n",
    "        num = 0\n",
    "        ####行合并\n",
    "        for i in range(h):#行合并\n",
    "            if i == index:\n",
    "                tmp[num]=mat1[i]+mat1[i+1]\n",
    "                num=num+1\n",
    "                continue\n",
    "            if i== index+1:\n",
    "                continue\n",
    "\n",
    "            tmp[num]=mat1[i]\n",
    "            num=num+1\n",
    "\n",
    "        ####列合并   \n",
    "        mat2=tmp\n",
    "        tmp = np.zeros((h-1,w-1))\n",
    "        num = 0\n",
    "        for j in range(w):#列合并\n",
    "            if j == index:\n",
    "                tmp[:,num] = mat2[:,j]+mat2[:,j+1]\n",
    "                num=num+1\n",
    "                continue\n",
    "            if j== index+1:\n",
    "                continue\n",
    "\n",
    "            tmp[:,num] = mat2[:,j]\n",
    "            num=num+1\n",
    "        matTmp[index] = tmp\n",
    "        \n",
    "        #print(\"合并后的所有矩阵\")\n",
    "        #print(index,matTmp[index])#合并后的所有矩阵\n",
    "    matTmp1 = copy.deepcopy(matTmp)\n",
    "   \n",
    "    ##归一化   \n",
    "    maxDiffMat = np.zeros((len(matTmp),1))\n",
    "    for i in range(len(matTmp)):\n",
    "        tmp = matTmp[i]\n",
    "        sumTmp =  sum(tmp)\n",
    "        for j in range(tmp.shape[1]):\n",
    "            tmp[:,j] = tmp[:,j]/(sumTmp[j])\n",
    "\n",
    "        accyNow = tmp[i,i]\n",
    "        maxDiffMat[i]= max(accyNow-accy[i],accyNow-accy[i+1])\n",
    "    maxIndex = np.argmax(maxDiffMat)\n",
    "    maxDiff = max(maxDiffMat)\n",
    "    #print(matTmp1)\n",
    "    return maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1\n",
    "\n",
    "#为了思考，不用for循环，直接一步一步做\n",
    "#4到3\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(matTmp1[maxIndex])\n",
    "\n",
    "#3到2\n",
    "mat3to2=matTmp1[maxIndex]\n",
    "accy3to2 = [matTmp[maxIndex][0,0],matTmp[maxIndex][1,1],matTmp[maxIndex][2,2]]\n",
    "maxIndex3to2,maxDiff3to2,maxDiffMat3to2,matTmp3to2,matTmp3to2Origin =computeAccuracyDiff(mat3to2,accy3to2)\n",
    "chosedMat = matTmp3to2Origin[maxIndex3to2]\n",
    "print(\"最佳合并点和矩阵\",maxIndex3to2,maxDiff3to2)\n",
    "print(matTmp3to2Origin[maxIndex3to2])\n",
    "print(matTmp3to2[maxIndex3to2])\n",
    "\n",
    "\n",
    "#########################################采用数据进行分析\n",
    "print(\"\\n\\n\\n###################################################\")\n",
    "print(\"\\n\\n\\n 用第5层数据进行分析\")\n",
    "h3_yp = ypredict[:,2]\n",
    "h3_yl = ylabel[:,2]\n",
    "mat1 = confusion_matrix(h3_yl,h3_yp)\n",
    "p1 = confusion_matrix(h3_yl,h3_yp,normalize='pred')\n",
    "sumTmp = sum(mat1)\n",
    "print(mat1)\n",
    "print(sumTmp)\n",
    "print(np.around(p1, decimals=3))\n",
    "\n",
    "########5->4\n",
    "accy = [p1[0,0],p1[1,1],p1[2,2],p1[3,3],p1[4,4]]\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"\\n\\n\\n5->4,最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(matTmp1[maxIndex])\n",
    "print(np.around(matTmp[maxIndex], decimals=3))\n",
    "\n",
    "########4->3\n",
    "mat1=matTmp1[maxIndex]\n",
    "accy= [matTmp[maxIndex][0,0],matTmp[maxIndex][1,1],matTmp[maxIndex][2,2],matTmp[maxIndex][3,3]]\n",
    "\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"\\n\\n\\n4->3,最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(matTmp1[maxIndex])\n",
    "print(np.around(matTmp[maxIndex], decimals=3))\n",
    "\n",
    "\n",
    "########3->2\n",
    "mat1=matTmp1[maxIndex]\n",
    "accy= [matTmp[maxIndex][0,0],matTmp[maxIndex][1,1],matTmp[maxIndex][2,2]]\n",
    "\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"\\n\\n\\n3->2,最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(matTmp1[maxIndex])\n",
    "print(np.around(matTmp[maxIndex], decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##测试最简注意力机制，Attention Channel ,SEAttention\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils.vis_utils import plot_model\n",
    "np.random.seed(1337)  # for reproducibility\n",
    " \n",
    "from keras.models import *\n",
    "from keras.layers import Input, Dense,Multiply,Activation\n",
    " \n",
    "input_dim = 4\n",
    "\n",
    "\n",
    "def get_data(n, input_dim, attention_column=1):\n",
    "\n",
    "    x = np.random.standard_normal(size=(n, input_dim))\n",
    "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
    "    x[:, attention_column] = y[:, 0]\n",
    "    return x, y\n",
    "\n",
    " \n",
    " \n",
    "def Att(att_dim,inputs,name):\n",
    "    V = inputs\n",
    "    QK = Dense(att_dim,bias=None)(inputs)\n",
    "    QK = Activation(\"softmax\",name=name)(QK)\n",
    "    MV = Multiply()([V, QK])\n",
    "    return(MV)\n",
    " \n",
    " \n",
    "def build_model():\n",
    "    inputs = Input(shape=(input_dim,))\n",
    " \n",
    "    atts1 = Att(input_dim,inputs,\"attention_vec\")\n",
    " \n",
    "    x = Dense(16)(atts1)\n",
    "    atts2 = Att(16,x,\"attention_vec1\")\n",
    " \n",
    " \n",
    "    output = Dense(1, activation='sigmoid')(atts2)\n",
    "    model = Model(input=inputs, output=output)\n",
    "    return model\n",
    "\n",
    "N = 10000\n",
    "inputs_1, outputs = get_data(N, input_dim) \n",
    "print(inputs_1)\n",
    " \n",
    "m = build_model()\n",
    "plot_model(m, to_file='attMap.png', show_shapes=True)\n",
    "#m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#print(m.summary())\n",
    "#m.fit(inputs_1, outputs, epochs=20, batch_size=128, validation_split=0.2)testing_inputs_1, testing_outputs = get_data(1, input_dim)\n",
    "\n",
    "\n",
    "#原文链接：https://blog.csdn.net/xiaosongshine/article/details/90579679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b0c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.csdn.net/SKIp121whats112/article/details/122265766\n",
    "#https://scikit-learn.org/stable/modules/tree.html\n",
    "##############测试决策树\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "Input= x\n",
    "Output = ylabel[:,2]\n",
    "print(x)\n",
    "print(Output)\n",
    "dt = tree.DecisionTreeClassifier(max_depth=5,min_samples_split=100,min_samples_leaf=100,min_impurity_decrease=0.001)\n",
    "dt = dt.fit(Input, Output)\n",
    "tree.plot_tree(dt)\n",
    "data=tree.export_graphviz(dt, out_file=None,class_names=['0','1','2','3','4'],filled=True) \n",
    "graph = graphviz.Source(data)\n",
    "graph.render(\"now\")\n",
    "\n",
    "data=tree.export_graphviz(dt, out_file=None,class_names=['0','1','2','3','4'],filled=True,proportion=True) \n",
    "graph = graphviz.Source(data)\n",
    "graph.render(\"nowPercent\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "OutPredict = dt.predict(Input)\n",
    "\n",
    "tmp1 = classification_report(Output,OutPredict )\n",
    "tmp2 = confusion_matrix(Output,OutPredict ,normalize='true')\n",
    "tmp3 = confusion_matrix(Output,OutPredict ,normalize='pred')\n",
    "print(tmp1)\n",
    "print(np.around(tmp2, decimals=3))\n",
    "print(np.around(tmp3, decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bc51c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试决策树的特征\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "###测试权重\n",
    "nSamples =5000\n",
    "input_dim = 10\n",
    "#x = np.random.standard_normal(size=(nSamples, input_dim))\n",
    "x = np.random.randint(low=0, high=10, size=(nSamples, input_dim))\n",
    "y1 = np.zeros((nSamples, 1))#>50\n",
    "y1A = np.zeros((nSamples, 1))#>50 and <60\n",
    "y1B = np.zeros((nSamples, 1))#>=60\n",
    "sumX = np.sum(x,axis=1)\n",
    "index=np.where(sumX>40)\n",
    "y1[index]=1\n",
    "index=np.where((sumX>50)& (sumX<70))\n",
    "y1A[index]=1\n",
    "index=np.where(sumX>=70)\n",
    "y1B[index]=1\n",
    "\n",
    "##数据来源2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "file1 = \"./trainData/dataAllSim1000.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "xyData = np.array(xyDataTmp)\n",
    "nSamples, nDims= xyData.shape\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "ylabel = y\n",
    "y1= y[:,0]\n",
    "\n",
    "\n",
    "##################################################################\n",
    "#测试决策树\n",
    "def dtFitAndSave(x,y,class_names1,saveName):\n",
    "    dt = tree.DecisionTreeClassifier(max_depth=7,min_samples_split=100,min_samples_leaf=100,min_impurity_split=0.06,ccp_alpha=0.001)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    data=tree.export_graphviz(dt, out_file=None,class_names=class_names1,filled=True) \n",
    "    graph = graphviz.Source(data)\n",
    "    graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(tmp1)\n",
    "    text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    \n",
    "    yPredict = dt.predict_proba(x[0:3,:])\n",
    "    print(yPredict[:,1])\n",
    "    d_path = dt.decision_path(x[0:3,:]).todense()\n",
    "    print(d_path)\n",
    "    print(\"impurity\",dt.tree_.impurity)\n",
    "    print(\"feature\",dt.tree_.feature)\n",
    "    print(\"threshold\",dt.tree_.threshold)\n",
    "    \n",
    "    #左节点编号  :  clf.tree_.children_left\n",
    "    #右节点编号  :  clf.tree_.children_right\n",
    "    #分割的变量  :  clf.tree_.feature\n",
    "    #分割的阈值  :  clf.tree_.threshold\n",
    "    #不纯度(gini) :  clf.tree_.impurity\n",
    "    #样本个数      :  clf.tree_.n_node_samples\n",
    "    #样本分布      :  clf.tree_.value\n",
    "    #https://blog.csdn.net/ywj_1991/article/details/122985778\n",
    "    #https://www.javaroad.cn/questions/54003\n",
    "\n",
    "    w,h = d_path.shape\n",
    "    for i in range(h):\n",
    "       path = d_path[i]\n",
    "       v,ind = np.where(path>0)\n",
    "       xtmp = x[i]\n",
    "       #print(\"path\",path,ind,np.array(ind)[-1])\n",
    "    \n",
    "       print(\"\\n index\",index)\n",
    "       print(\"impurity\",dt.tree_.impurity[ind])\n",
    "       print(\"feature\",dt.tree_.feature[ind])\n",
    "       print(\"threshold\",dt.tree_.threshold[ind])\n",
    "       print(\"x[index]\",xtmp[ind])\n",
    "       \n",
    "      \n",
    "       print(\"the leaf node:\",np.array(ind)[-1],\"the simplest rule is\")\n",
    "       for jj in ind:\n",
    "           if dt.tree_.feature[jj] == -2:\n",
    "                print(\"label,proba is\",yPredict[i,0],yPredict[i,1])\n",
    "                break\n",
    "                \n",
    "           if xtmp[jj]<=dt.tree_.threshold[jj]:\n",
    "              print(\" x[%d]<=%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "           else:\n",
    "              print(\" x[%d]>%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "                    \n",
    "       finalPos = np.array(ind)[-1]\n",
    "       print(dt.tree_.impurity[finalPos])\n",
    "       print(dt.tree_.feature[finalPos])\n",
    "       print(dt.tree_.threshold[finalPos])\n",
    "\n",
    "dtFitAndSave(x,y1,[\"0\",\"1\"],\"bigger\")\n",
    "\n",
    "###################################################################################\n",
    "#测试神经网络\n",
    "def kerasFitAndSave(x,y,num_labels):\n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 384\n",
    "    dropout_rate =0.1\n",
    "    models=[]\n",
    "    \n",
    "    build_model = tf.keras.Sequential()\n",
    "   \n",
    "    build_model.add(layers.Dense(relu_size, activation='relu',name=\"layer1\",input_shape=(features_size,)))\n",
    "    build_model.add(layers.Dropout(dropout_rate,name=\"Dropout1-2\"))\n",
    "    build_model.add(layers.Dense(num_labels, activation='sigmoid',name=\"layer2\"))\n",
    "    \n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    plot_model(build_model, to_file='AKeras.png', show_shapes=True)\n",
    "    \n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(y)  \n",
    "    yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    build_model.fit([x],[yOnehot],epochs=100, batch_size=80000*1)\n",
    "    build_model.save(\"Akeras.h5\")\n",
    "    plot_model(build_model, to_file='AKeras.png', show_shapes=True)\n",
    "    \n",
    "    return build_model,models\n",
    "\n",
    "def kerasFitAndSaveSimple(x,y,num_labels):\n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 382\n",
    "    models=[]\n",
    "    \n",
    "    build_model = tf.keras.Sequential()\n",
    "    build_model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    build_model.add(layers.Dropout(dropout_rate))\n",
    "    build_model.add(layers.Dense(num_labels, activation='sigmoid'))\n",
    "    build_model.add(layers.Dense(num_labels, activation='sigmoid',name=\"layer1\",input_shape=(features_size,)))\n",
    "    \n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    plot_model(build_model, to_file='AKerasSimple.png', show_shapes=True)\n",
    "    \n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(y)  \n",
    "    yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    build_model.fit([x],[yOnehot],epochs=10000, batch_size=80000*1)\n",
    "    build_model.save(\"Akeras.h5\")\n",
    "    plot_model(build_model, to_file='AKeras.png', show_shapes=True)\n",
    "    \n",
    "    return build_model,models\n",
    "\n",
    "y1 = np.array(y1)\n",
    "y1= y1.reshape(nSamples,-1)\n",
    "print(y1)\n",
    "#kerasFitAndSave(x,y1,2)\n",
    "#kerasFitAndSaveSimple(x,y1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b94bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################融合决策树和多层神经网络###########################################################\n",
    "\n",
    "#######################################第一步读取数据\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "file1 = \"./trainData/dataAllSim1000.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "xyData = np.array(xyDataTmp)\n",
    "nSamples, nDims= xyData.shape\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "ylabel = y\n",
    "y1Level= y[:,0]#01\n",
    "y2Level= y[:,1]#012\n",
    "y3Level= y[:,2]#01234\n",
    "\n",
    "print(\"x.shape:\",x.shape,\"y.shape:\",y.shape)\n",
    "\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a18ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################融合决策树和多层神经网络###########################################################\n",
    "\n",
    "#######################################第二步基于神经网络训练，这里采用简单神经网络，RESNET类似和HNCF三种方法进行训练\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "###简单模型1，没有隐藏层\n",
    "def kerasFitAndSaveSimple1(x,yOneHot,num_labels):\n",
    "    nSamples,features_size = x.shape\n",
    "    build_model = tf.keras.Sequential()\n",
    "    build_model.add(layers.Dense(num_labels, activation='sigmoid',name=\"layer1\",input_shape=(features_size,)))\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    \n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    build_model.fit([x],[yOneHot],epochs=10000, batch_size=80000*1)\n",
    "    build_model.save(\"kerasSimple1.h5\")\n",
    "    plot_model(build_model, to_file='KerasSimple1_noHiddenLayer.png', show_shapes=True)\n",
    "    return build_model\n",
    "\n",
    "###简单模型2，有隐藏层\n",
    "def kerasFitAndSaveSimple2(x,yOneHot,num_labels):\n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 512\n",
    "    dropout_rate = 0.05\n",
    "    build_model = tf.keras.Sequential()\n",
    "    build_model.add(layers.Dense(relu_size, activation='relu',name=\"layer1\",input_shape=(features_size,)))\n",
    "    build_model.add(layers.Dropout(dropout_rate,name=\"Dropout1-2\"))\n",
    "    build_model.add(layers.Dense(relu_size/2, activation='relu',name=\"layer2\"))\n",
    "    build_model.add(layers.Dropout(dropout_rate,name=\"Dropout2-3\"))\n",
    "    build_model.add(layers.Dense(num_labels, activation='sigmoid',name=\"layer3\"))\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    build_model.fit([x],[yOneHot],epochs=10000, batch_size=80000*1)\n",
    "    build_model.save(\"kerasSimple2.h5\")\n",
    "    plot_model(build_model, to_file='KerasSimple2_HiddenLayer.png', show_shapes=True)\n",
    "    return build_model\n",
    "\n",
    "###简单模型3，resnet_like\n",
    "def global_model(dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    return model\n",
    "\n",
    "def sigmoid_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='sigmoid',name=\"global\"))\n",
    "    return model\n",
    "\n",
    "def kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels):\n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 512\n",
    "    dropout_rate = 0.05\n",
    "    hierarchy = [1,1,1]\n",
    "    global_models = []\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(len(hierarchy)):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "\n",
    "    p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    build_model.fit([x],[yOneHot],epochs=10000, batch_size=80000*1)\n",
    "    build_model.save(\"KerasSimple3_likeResnet.h5\")\n",
    "    plot_model(build_model, to_file='KerasSimple3_likeResnet.png', show_shapes=True)\n",
    "    return build_model\n",
    "    print(\"HMCNF is not implemented\")\n",
    "    return False\n",
    "\n",
    "nSamples,features_size = x.shape\n",
    "num_labels = 5\n",
    "enc = OneHotEncoder()\n",
    "y3Level = np.array(y3Level)\n",
    "y3Level= y3Level.reshape(nSamples,-1)\n",
    "print(y3Level)\n",
    "enc.fit(y3Level)  \n",
    "\n",
    "###开始训练\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y3Level, test_size = 0.5)\n",
    "\n",
    "\n",
    "x = x_train\n",
    "yOneHot=enc.transform(y_train).toarray()\n",
    "print(yOneHot)\n",
    "#simpleMode1 = kerasFitAndSaveSimple1(x,yOneHot,num_labels)\n",
    "simpleMode2 = kerasFitAndSaveSimple2(x,yOneHot,num_labels)\n",
    "#simpleMode3 = kerasFitAndSaveSimple3(x,yOneHot,num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7326578",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################融合决策树和多层神经网络###########################################################\n",
    "#######################################第三步根据识别结果，进行聚类聚类\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "\n",
    "####################################################################\n",
    "file1 = \"./trainData/dataAllSim1000.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "ylabel = y\n",
    "print(\"x.shape:\",x.shape,\"y.shape:\",y.shape)\n",
    "\n",
    "\n",
    "\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "#################################################################\n",
    "\n",
    "if 0:#采用keras\n",
    "    model_name =\"kerasSimple2.h5\" \n",
    "    model = keras.models.load_model(model_name)\n",
    "    yP5= model.predict([x], batch_size=2560)\n",
    "\n",
    "    ###需要将预测出的值，转换01整数,并转为数字式\n",
    "    for i in range(yP5.shape[0]):\n",
    "        tmp = yP5[i]\n",
    "        index=  np.argmax(tmp)\n",
    "        yP5[i] = [0,0,0,0,0]\n",
    "        yP5[i,index]=1\n",
    "    print(yP5)\n",
    "\n",
    "    ###\n",
    "    enc = OneHotEncoder()\n",
    "    yl5= y[:,2]#01234\n",
    "    yl5 = np.array(yl5)\n",
    "    yl5= yl5.reshape(nSamples,-1)\n",
    "    print(yl5)\n",
    "    enc.fit(yl5)\n",
    "\n",
    "    yP5= enc.inverse_transform(yP5)\n",
    "    yP5= yP5.reshape(-1,nSamples)[0]\n",
    "    ########\n",
    "\n",
    "\n",
    "\n",
    "    print(yP5)\n",
    "    print(yP5.shape)\n",
    "\n",
    "    print(yl5)\n",
    "    print(yl5.shape)\n",
    "\n",
    "if 1:#采用决策树\n",
    "    yl5= y[:,2]#01234\n",
    "    dt = tree.DecisionTreeClassifier(max_depth=10,min_samples_leaf=100)\n",
    "    dt = dt.fit(x, yl5)\n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(yl5,yPredict)\n",
    "    print(tmp1)\n",
    "    mat1num = confusion_matrix(yl5,yPredict)\n",
    "    mat2acc = confusion_matrix(yl5,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    yP5 = yPredict\n",
    "\n",
    "###################################开始合并\n",
    "hierachFloor = dict()\n",
    "hierachFloor ['input'] = x\n",
    "hierachFloor ['output'] = y\n",
    "\n",
    "                               \n",
    "                                    \n",
    "                                    \n",
    "#0层为原始输入层\n",
    "mat1num = confusion_matrix(yl5 ,yP5)\n",
    "mat2acc = confusion_matrix(yl5,yP5,normalize='pred')\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "hierachFloor ['floor0'] = {'label':['0','1','2','3','4'],'num_mat': mat1num,'prob_mat': mat2acc}\n",
    "                                    \n",
    "\n",
    "def computeAccuracyDiff(mat1,accy):\n",
    "    h,w = mat1.shape\n",
    "    tmp = np.zeros((h-1,w-1))\n",
    "    matTmp={}\n",
    "    ##从0到最后，行列合并\n",
    "    for index in range(h-1):\n",
    "\n",
    "        tmp = np.zeros((h-1,w))\n",
    "        num = 0\n",
    "        ####行合并\n",
    "        for i in range(h):#行合并\n",
    "            if i == index:\n",
    "                tmp[num]=mat1[i]+mat1[i+1]\n",
    "                num=num+1\n",
    "                continue\n",
    "            if i== index+1:\n",
    "                continue\n",
    "\n",
    "            tmp[num]=mat1[i]\n",
    "            num=num+1\n",
    "\n",
    "        ####列合并   \n",
    "        mat2=tmp\n",
    "        tmp = np.zeros((h-1,w-1))\n",
    "        num = 0\n",
    "        for j in range(w):#列合并\n",
    "            if j == index:\n",
    "                tmp[:,num] = mat2[:,j]+mat2[:,j+1]\n",
    "                num=num+1\n",
    "                continue\n",
    "            if j== index+1:\n",
    "                continue\n",
    "\n",
    "            tmp[:,num] = mat2[:,j]\n",
    "            num=num+1\n",
    "        matTmp[index] = tmp\n",
    "        \n",
    "        #print(\"合并后的所有矩阵\")\n",
    "        #print(index,matTmp[index])#合并后的所有矩阵\n",
    "    matTmp1 = copy.deepcopy(matTmp)\n",
    "   \n",
    "    ##归一化   \n",
    "    maxDiffMat = np.zeros((len(matTmp),1))\n",
    "    for i in range(len(matTmp)):\n",
    "        tmp = matTmp[i]\n",
    "        sumTmp =  sum(tmp)\n",
    "        for j in range(tmp.shape[1]):\n",
    "            tmp[:,j] = tmp[:,j]/(sumTmp[j])\n",
    "\n",
    "        accyNow = tmp[i,i]\n",
    "        maxDiffMat[i]= max(accyNow-accy[i],accyNow-accy[i+1])\n",
    "    maxIndex = np.argmax(maxDiffMat)\n",
    "    maxDiff = max(maxDiffMat)\n",
    "    #print(matTmp1)\n",
    "    return maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1\n",
    "#print(\"最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "#print(“所有合并后的所有矩阵，数目和概率\",matTmp,matTmp1)\n",
    "#print(“各个点合并后的正确率提升矩阵\",maxDiffMat)\n",
    "\n",
    "print(\"\\n\\n\\n###################################################\")\n",
    "print(\"\\n\\n\\n 用数据进行分析\")\n",
    "\n",
    "#1层为5到4层\n",
    "accy = [mat2acc[0,0],mat2acc[1,1],mat2acc[2,2],mat2acc[3,3],mat2acc[4,4]]\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1num,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"\\n\\n\\n5->4,最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(maxDiffMat)\n",
    "print(\"数目矩阵\\n\",matTmp1[maxIndex])\n",
    "print(\"概率矩阵\\n\",np.around(matTmp[maxIndex], decimals=3))\n",
    "hierachFloor ['floor1'] = {'num_mat': matTmp1,'prob_mat': matTmp,'mergeIndex':maxIndex,'mergediffMat':maxDiffMat}\n",
    "\n",
    "###2层4->3\n",
    "mat1num=matTmp1[maxIndex]\n",
    "mat2acc= [matTmp[maxIndex][0,0],matTmp[maxIndex][1,1],matTmp[maxIndex][2,2],matTmp[maxIndex][3,3]]\n",
    "\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1num,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"\\n\\n\\n4->3,最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(maxDiffMat)\n",
    "print(\"数目矩阵\\n\",matTmp1[maxIndex])\n",
    "print(\"概率矩阵\\n\",np.around(matTmp[maxIndex], decimals=3))\n",
    "      \n",
    "hierachFloor ['floor2'] = {'num_mat': matTmp1,'prob_mat': matTmp,'mergeIndex':maxIndex,'mergediffMat':maxDiffMat}\n",
    " \n",
    "###3层3->2\n",
    "mat1num=matTmp1[maxIndex]\n",
    "mat2acc= [matTmp[maxIndex][0,0],matTmp[maxIndex][1,1],matTmp[maxIndex][2,2]]\n",
    "\n",
    "maxIndex,maxDiff,maxDiffMat,matTmp,matTmp1 =computeAccuracyDiff(mat1num,accy)\n",
    "chosedMat =  matTmp1[maxIndex]\n",
    "print(\"\\n\\n\\n3->2,最佳合并点和矩阵\",maxIndex,maxDiff)\n",
    "print(maxDiffMat)\n",
    "print(\"数目矩阵\\n\",matTmp1[maxIndex])\n",
    "print(\"概率矩阵\\n\",np.around(matTmp[maxIndex], decimals=3))\n",
    "      \n",
    "hierachFloor['floor3'] = {'num_mat': matTmp1,'prob_mat': matTmp,'mergeIndex':maxIndex,'mergediffMat':maxDiffMat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b054c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################融合决策树和多层神经网络###########################################################\n",
    "#######################################第四步根据聚类和识别结果，开始微调\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "\n",
    "\n",
    "##################\n",
    "file1 = \"./trainData/dataAllSim1000.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "ylabel = y\n",
    "print(\"x.shape:\",x.shape,\"y.shape:\",y.shape)\n",
    "\n",
    "yl5= y[:,2]#01234\n",
    "print(\"x.shape:\",x.shape,\"yl5.shape:\",yl5.shape)\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "\n",
    "print(\"x.shape:\",x.shape,\"y.shape:\",yl5.shape,\"y.type:\", type(yl5) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "\n",
    "\n",
    "def getKerasModeFloors(x,y,saveName):\n",
    "    model_name = saveName \n",
    "    model = keras.models.load_model(model_name)\n",
    "    yP5= model.predict([x], batch_size=2560)\n",
    "    nSamples = yP5.shape[0]\n",
    "     ###需要将预测出的值，转换01整数,并转为数字式\n",
    "    for i in range(yP5.shape[0]):\n",
    "        tmp = yP5[i]\n",
    "        index=  np.argmax(tmp)\n",
    "        yP5[i] = [0,0,0,0,0]\n",
    "        yP5[i,index]=1\n",
    "   \n",
    "\n",
    "    ###\n",
    "    enc = OneHotEncoder()\n",
    "    yl5= y[:,2]#01234\n",
    "    yl5 = np.array(yl5)\n",
    "    yl5= yl5.reshape(nSamples,-1)\n",
    "    print(yl5)\n",
    "    enc.fit(yl5)\n",
    "\n",
    "    yP5= enc.inverse_transform(yP5)\n",
    "    yP5= yP5.reshape(-1,nSamples)[0]\n",
    "    \n",
    "    yP4 = np.zeros((yP5.shape[0],1))\n",
    "    yP3 = np.zeros((yP5.shape[0],1))\n",
    "    yP2 = np.zeros((yP5.shape[0],1))\n",
    "\n",
    "    for i in range(yP5.shape[0]):\n",
    "        if(yP5[i]== 2) or (yP5[i]== 1):\n",
    "             yP4[i] = 21\n",
    "        else:\n",
    "             yP4[i] = yP5[i]\n",
    "                \n",
    "        if(yP5[i]== 2) or (yP5[i]== 1) or (yP5[i]== 0):\n",
    "             yP3[i] = 210\n",
    "        else:\n",
    "             yP3[i] = yP5[i]\n",
    "                \n",
    "        if(yP5[i]== 2) or (yP5[i]== 1) or (yP5[i]== 0) or (yP5[i]== 3):\n",
    "             yP2[i] = 3210\n",
    "        else:\n",
    "             yP2[i] = yP5[i]\n",
    "    \n",
    "    return model,yP5,yP4,yP3,yP2\n",
    "\n",
    "#分层决策树\n",
    "def dtFitAndSave(x,y,saveName):\n",
    "    dt = tree.DecisionTreeClassifier(max_depth=10,min_samples_leaf=1000)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    data=tree.export_graphviz(dt, out_file=None,class_names=None,filled=True) \n",
    "    graph = graphviz.Source(data)\n",
    "    graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(tmp1)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    #text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    return dt\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "def getDTSamplesInfo(x,dt):\n",
    "    yPredict = dt.predict_proba(x)\n",
    "    #print(\"\\n\\n getDTSamplesInfo yPredict\",yPredict)\n",
    "    d_path = dt.decision_path(x).todense()\n",
    "    #print(\"\\n\\n d_path\",d_path)\n",
    "    #print(\"impurity\",dt.tree_.impurity)\n",
    "    #print(\"feature\",dt.tree_.feature)\n",
    "    #print(\"threshold\",dt.tree_.threshold)\n",
    "    \n",
    "    #左节点编号  :  clf.tree_.children_left\n",
    "    #右节点编号  :  clf.tree_.children_right\n",
    "    #分割的变量  :  clf.tree_.feature\n",
    "    #分割的阈值  :  clf.tree_.threshold\n",
    "    #不纯度(gini) :  clf.tree_.impurity\n",
    "    #样本个数      :  clf.tree_.n_node_samples\n",
    "    #样本分布      :  clf.tree_.value\n",
    "    #https://blog.csdn.net/ywj_1991/article/details/122985778\n",
    "    #https://www.javaroad.cn/questions/54003\n",
    "    \n",
    "    h,w = d_path.shape\n",
    "    gini =np.zeros((h,1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(h):\n",
    "       path = d_path[i]\n",
    "       v,ind = np.where(path>0)\n",
    "       xtmp = x[i]\n",
    "       #print(\"path\",path,ind,np.array(ind)[-1])\n",
    "    \n",
    "       #print(\"\\n index\",index)\n",
    "       #print(\"impurity\",dt.tree_.impurity[ind])\n",
    "       #print(\"feature\",dt.tree_.feature[ind])\n",
    "       #print(\"threshold\",dt.tree_.threshold[ind])\n",
    "       #print(\"x[index]\",xtmp[ind])\n",
    "       \n",
    "      \n",
    "       #print(\"the leaf node:\",np.array(ind)[-1],\"the simplest rule is\")\n",
    "       #for jj in ind:\n",
    "       #    if dt.tree_.feature[jj] == -2:\n",
    "       #         print(\"label,proba is\",yPredict[i,0],yPredict[i,1])\n",
    "       #         break\n",
    "                \n",
    "       #    if xtmp[jj]<=dt.tree_.threshold[jj]:\n",
    "       #       print(\" x[%d]<=%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "       #    else:\n",
    "       #       print(\" x[%d]>%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "                    \n",
    "       finalPos = np.array(ind)[-1]\n",
    "       gini[i] = dt.tree_.impurity[finalPos]\n",
    "       \n",
    "       #print(\"d_path\",i,path,dt.tree_.impurity[finalPos])\n",
    "       #print(dt.tree_.feature[finalPos])\n",
    "       #print(dt.tree_.threshold[finalPos])\n",
    "       #print(dt.tree_.n_node_samples[finalPos])\n",
    "\n",
    "    \n",
    "    return gini,yPredict\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "\n",
    "index = np.where((yl5 == 2) | (yl5 == 1))\n",
    "yl4 = yl5.copy()\n",
    "yl4[index]=21\n",
    "print(yl4)\n",
    "\n",
    "\n",
    "index = np.where((yl5 == 2) | (yl5 == 1) | (yl5 == 0))\n",
    "yl3 = yl5.copy()\n",
    "yl3[index]=210\n",
    "print(yl3)\n",
    "\n",
    "\n",
    "\n",
    "index = np.where( (yl5 == 3)|(yl5 == 2) | (yl5 == 1) | (yl5 == 0))\n",
    "yl2 = yl5.copy()\n",
    "yl2[index]=3210\n",
    "print(yl2)\n",
    "\n",
    "\n",
    "hierachFloor['floor3'][\"dt\"] = dtFitAndSave(x,yl2,\"Floo3_2\")\n",
    "hierachFloor['floor2'][\"dt\"] = dtFitAndSave(x,yl3,\"Floo2_3\")\n",
    "hierachFloor['floor1'][\"dt\"] = dtFitAndSave(x,yl4,\"Floor1_4\")\n",
    "hierachFloor['floor0'][\"dt\"] = dtFitAndSave(x,yl5,\"Floor0_5\")\n",
    "\n",
    "#giniFloor0,yPredictProFloor0 = getDTSamplesInfo(x,hierachFloor['floor0'][\"dt\"])\n",
    "#giniFloor1,yPredictProFloor1 = getDTSamplesInfo(x,hierachFloor['floor1'][\"dt\"])\n",
    "#giniFloor2,yPredictProFloor2 = getDTSamplesInfo(x,hierachFloor['floor2'][\"dt\"])\n",
    "#giniFloor3,yPredictProFloor3 = getDTSamplesInfo(x,hierachFloor['floor3'][\"dt\"])\n",
    "\n",
    "kerasFloors,yKerasP5,yKerasP4,yKerasP3,yKerasP2=getKerasModeFloors(x,y,'kerasSimple2.h5')\n",
    "##############开始混合检测\n",
    "\n",
    "###0层，5标签\n",
    "\n",
    "'''\n",
    "nSamples,feturesNume  = x.shape\n",
    "yHyLabelFloor0 = np.zeros((nSamples,1))\n",
    "hyCounter = 0\n",
    "for i in range(nSamples):\n",
    "    print(i)\n",
    "    xtmp = x[i]\n",
    "    dt = hierachFloor['floor0'][\"dt\"]\n",
    "    giniFloor0,yPredictProFloor0 = getDTSamplesInfo([xtmp],dt)\n",
    "    giniTmp = giniFloor0[0]\n",
    "    yPredictProFloor0Tmp = yPredictProFloor0[0]\n",
    "    #print('gini',giniTmp )\n",
    "    #print('probPredict',yPredictProFloor0Tmp  )\n",
    "    if giniTmp >0.05 or max(yPredictProFloor0Tmp)<0.98:\n",
    "        yHyLabelFloor0[i] = yKerasP5[i]\n",
    "    else:\n",
    "        yHyLabelFloor0[i] = np.argmax(yPredictProFloor0)\n",
    "        hyCounter = hyCounter+1\n",
    "  \n",
    "print('O层5标签hyCounter',hyCounter)    \n",
    "\n",
    "\n",
    "tmp1 = classification_report(yl5,yHyLabelFloor0)\n",
    "print('hybrid\\n',tmp1)\n",
    "tmp1 = classification_report( yKerasP5,yHyLabelFloor0)\n",
    "print('keras\\n',tmp1)\n",
    "mat1num = confusion_matrix(yl5,yHyLabelFloor0)\n",
    "mat2acc = confusion_matrix(yl5,yHyLabelFloor0,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "'''\n",
    "\n",
    "'''\n",
    "#############1层，4标签\n",
    "nSamples,feturesNume  = x.shape\n",
    "yHyLabelFloor1 = np.zeros((nSamples,1))\n",
    "hyCounter = 0\n",
    "for i in range(nSamples):\n",
    "    print(i)\n",
    "    xtmp = x[i]\n",
    "    dt = hierachFloor['floor1'][\"dt\"]\n",
    "    giniFloor1,yPredictProFloor1 = getDTSamplesInfo([xtmp],dt)\n",
    "    giniTmp = giniFloor1[0]\n",
    "    yPredictProFloor1Tmp = yPredictProFloor1[0]\n",
    "    #print('gini',giniTmp )\n",
    "    #print('probPredict',yPredictProFloor0Tmp  )\n",
    "    if giniTmp >0.05 or max(yPredictProFloor1Tmp)<0.98:\n",
    "        yHyLabelFloor1[i] = yKerasP4[i]\n",
    "    else:\n",
    "        tmp0= [0,3,4,21]\n",
    "        index = np.argmax(yPredictProFloor1)\n",
    "        yHyLabelFloor1[i] = tmp0[index]\n",
    "        hyCounter = hyCounter+1\n",
    "print('O层4标签hyCounter',hyCounter)    \n",
    "\n",
    "\n",
    "tmp1 = classification_report(yl4,yHyLabelFloor1)\n",
    "print('hybrid\\n',tmp1)\n",
    "tmp1 = classification_report( yKerasP4,yHyLabelFloor1)\n",
    "print('keras\\n',tmp1)\n",
    "mat1num = confusion_matrix(yl4,yHyLabelFloor1)\n",
    "mat2acc = confusion_matrix(yl4,yHyLabelFloor1,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "'''\n",
    "\n",
    "\n",
    "def computeAndCompareHybridMode(x,y,dt,kerasPLabel,floorLabel):\n",
    "    nSamples,feturesNume  = x.shape\n",
    "    yHyLabel  = np.zeros((nSamples,1))\n",
    "    hyCounter = 0\n",
    "    for i in range(nSamples):\n",
    "        print(i)\n",
    "        xtmp = x[i]\n",
    "        giniFloor,yPredictProFloor = getDTSamplesInfo([xtmp],dt)\n",
    "        giniTmp = giniFloor[0]\n",
    "        yPredictProFloorTmp = yPredictProFloor[0]\n",
    "        #print('gini',giniTmp )\n",
    "        #print('probPredict',yPredictProFloor0Tmp  )\n",
    "        if giniTmp >0.05 or max(yPredictProFloorTmp)<0.98:\n",
    "            yHyLabel[i] = kerasPLabel[i]\n",
    "        else:\n",
    "            #floorLabel= [3,4,210]\n",
    "            index = np.argmax(yPredictProFloorTmp)\n",
    "            yHyLabel[i] = floorLabel[index]\n",
    "            hyCounter = hyCounter+1\n",
    "    print('floorLabel\\n',floorLabel) \n",
    "    print('hyCounter\\n',hyCounter)    \n",
    "\n",
    "\n",
    "    tmp1 = classification_report(y,yHyLabel)\n",
    "    print('hybrid\\n',tmp1)\n",
    "    tmp1 = classification_report(kerasPLabel,yHyLabel)\n",
    "    print('keras\\n',tmp1)\n",
    "    mat1num = confusion_matrix(y,yHyLabel)\n",
    "    mat2acc = confusion_matrix(y,yHyLabel,normalize='pred')\n",
    "    print('mat1num\\n',mat1num)\n",
    "    print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "    return\n",
    "\n",
    "#floor=2 ,label=3\n",
    "#dt = hierachFloor['floor2'][\"dt\"]\n",
    "#floorLabel= [3,4,210] \n",
    "#computeAndCompareHybridMode(x,yl3,dt,yKerasP3,floorLabel)\n",
    "\n",
    "#floor=3 ,label=2\n",
    "dt = hierachFloor['floor3'][\"dt\"]\n",
    "floorLabel= [4,3210] \n",
    "computeAndCompareHybridMode(x,yl2,dt,yKerasP2,floorLabel)\n",
    "\n",
    "return\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "'''\n",
    "for i in range(nSamples):\n",
    "    for j in range(4)\n",
    "         input1 = x[i,:]\n",
    "         label =  ylabel[i,j]\n",
    "         output1 = Floor[j][\"dt\"].predict_proba(input)\n",
    "         output_gini,output_num =getDT_Info(input)\n",
    "    \n",
    "        if max(output1)<0.95 or output_gini>0.2\n",
    "            output1 =  Floor[j][\"keras\"].predict(input)\n",
    "        \n",
    "        loss=loss+(output1-label)\n",
    "                           \n",
    "                           \n",
    "def getDT_SamplesInfo(x)\n",
    "    yPredict = dt.predict_proba(x[0:3,:])\n",
    "    print(yPredict[:,1])\n",
    "    d_path = dt.decision_path(x[0:3,:]).todense()\n",
    "    print(d_path)\n",
    "    print(\"impurity\",dt.tree_.impurity)\n",
    "    print(\"feature\",dt.tree_.feature)\n",
    "    print(\"threshold\",dt.tree_.threshold)\n",
    "    \n",
    "    #左节点编号  :  clf.tree_.children_left\n",
    "    #右节点编号  :  clf.tree_.children_right\n",
    "    #分割的变量  :  clf.tree_.feature\n",
    "    #分割的阈值  :  clf.tree_.threshold\n",
    "    #不纯度(gini) :  clf.tree_.impurity\n",
    "    #样本个数      :  clf.tree_.n_node_samples\n",
    "    #样本分布      :  clf.tree_.value\n",
    "    #https://blog.csdn.net/ywj_1991/article/details/122985778\n",
    "    #https://www.javaroad.cn/questions/54003\n",
    "\n",
    "    w,h = d_path.shape\n",
    "    for i in range(h):\n",
    "       path = d_path[i]\n",
    "       v,ind = np.where(path>0)\n",
    "       xtmp = x[i]\n",
    "       #print(\"path\",path,ind,np.array(ind)[-1])\n",
    "    \n",
    "       print(\"\\n index\",index)\n",
    "       print(\"impurity\",dt.tree_.impurity[ind])\n",
    "       print(\"feature\",dt.tree_.feature[ind])\n",
    "       print(\"threshold\",dt.tree_.threshold[ind])\n",
    "       print(\"x[index]\",xtmp[ind])\n",
    "       \n",
    "      \n",
    "       print(\"the leaf node:\",np.array(ind)[-1],\"the simplest rule is\")\n",
    "       for jj in ind:\n",
    "           if dt.tree_.feature[jj] == -2:\n",
    "                print(\"label,proba is\",yPredict[i,0],yPredict[i,1])\n",
    "                break\n",
    "                \n",
    "           if xtmp[jj]<=dt.tree_.threshold[jj]:\n",
    "              print(\" x[%d]<=%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "           else:\n",
    "              print(\" x[%d]>%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "                    \n",
    "       finalPos = np.array(ind)[-1]\n",
    "       print(dt.tree_.impurity[finalPos])\n",
    "       print(dt.tree_.feature[finalPos])\n",
    "       print(dt.tree_.threshold[finalPos])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99de97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data france\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72010 entries, 0 to 72009\n",
      "Data columns (total 48 columns):\n",
      "vehID             72010 non-null object\n",
      "redLightTime      72010 non-null float64\n",
      "distToRedLight    72010 non-null float64\n",
      "speed             72010 non-null float64\n",
      "laneAvgSpeed      72010 non-null float64\n",
      "arriveTime1       72010 non-null float64\n",
      "arriveTime2       72010 non-null float64\n",
      "vehPos_1          72010 non-null float64\n",
      "vehSpeed_1        72010 non-null float64\n",
      "vehPos_2          72010 non-null float64\n",
      "vehSpeed_2        72010 non-null float64\n",
      "vehPos_3          72010 non-null float64\n",
      "vehSpeed_3        72010 non-null float64\n",
      "vehPos_4          72010 non-null float64\n",
      "vehSpeed_4        72010 non-null float64\n",
      "vehPos_5          72010 non-null float64\n",
      "vehSpeed_5        72010 non-null float64\n",
      "vehPos_6          72010 non-null float64\n",
      "vehSpeed_6        72010 non-null float64\n",
      "vehPos_7          72010 non-null float64\n",
      "vehSpeed_7        72010 non-null float64\n",
      "vehPos_8          72010 non-null float64\n",
      "vehSpeed_8        72010 non-null float64\n",
      "vehPos_9          72010 non-null float64\n",
      "vehSpeed_9        72010 non-null float64\n",
      "vehPos_10         72010 non-null float64\n",
      "vehSpeed_10       72010 non-null float64\n",
      "vehPos_11         72010 non-null float64\n",
      "vehSpeed_11       72010 non-null float64\n",
      "vehPos_12         72010 non-null float64\n",
      "vehSpeed_12       72010 non-null float64\n",
      "vehPos_13         72010 non-null float64\n",
      "vehSpeed_13       72010 non-null float64\n",
      "vehPos_14         72010 non-null float64\n",
      "vehSpeed_14       72010 non-null float64\n",
      "vehPos_15         72010 non-null float64\n",
      "vehSpeed_15       72010 non-null float64\n",
      "vehPos_16         72010 non-null float64\n",
      "vehSpeed_16       72010 non-null float64\n",
      "vehPos_17         72010 non-null float64\n",
      "vehSpeed_17       72010 non-null float64\n",
      "vehPos_18         72010 non-null float64\n",
      "vehSpeed_18       72010 non-null float64\n",
      "vehPos_19         72010 non-null int64\n",
      "vehSpeed_19       72010 non-null int64\n",
      "vehPos_20         72010 non-null int64\n",
      "vehSpeed_20       72010 non-null int64\n",
      "speedFlag         72010 non-null int64\n",
      "dtypes: float64(42), int64(5), object(1)\n",
      "memory usage: 26.4+ MB\n",
      "None\n",
      "x.shape: (72010, 46) y.shape: (72010,) y.type: <class 'numpy.ndarray'> y.unique [0 1 2 3 4]\n",
      "x_train.shape (36005, 46) yOneHot.shape (36005, 5)\n",
      "Epoch 1/5000\n",
      "36005/36005 [==============================] - 2s 58us/step - loss: 6.6992 - acc: 0.2994\n",
      "Epoch 2/5000\n",
      "36005/36005 [==============================] - 1s 29us/step - loss: 2.9508 - acc: 0.6516\n",
      "Epoch 3/5000\n",
      "36005/36005 [==============================] - 1s 28us/step - loss: 0.2564 - acc: 0.9306\n",
      "Epoch 4/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.2425 - acc: 0.9384\n",
      "Epoch 5/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.2445 - acc: 0.9450\n",
      "Epoch 6/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.2479 - acc: 0.9482\n",
      "Epoch 7/5000\n",
      "36005/36005 [==============================] - 1s 30us/step - loss: 0.2454 - acc: 0.9486\n",
      "Epoch 8/5000\n",
      "36005/36005 [==============================] - 1s 31us/step - loss: 0.2361 - acc: 0.9487\n",
      "Epoch 9/5000\n",
      "36005/36005 [==============================] - 1s 30us/step - loss: 0.2223 - acc: 0.9491\n",
      "Epoch 10/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.2102 - acc: 0.9499\n",
      "Epoch 11/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.2110 - acc: 0.9484\n",
      "Epoch 12/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.2151 - acc: 0.9466\n",
      "Epoch 13/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.2167 - acc: 0.9463\n",
      "Epoch 14/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.2118 - acc: 0.9480\n",
      "Epoch 15/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.2031 - acc: 0.9499\n",
      "Epoch 16/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1945 - acc: 0.9515\n",
      "Epoch 17/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.1900 - acc: 0.9535\n",
      "Epoch 18/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1888 - acc: 0.9533\n",
      "Epoch 19/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1875 - acc: 0.9536\n",
      "Epoch 20/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.1835 - acc: 0.9538\n",
      "Epoch 21/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1793 - acc: 0.9545\n",
      "Epoch 22/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1715 - acc: 0.9550\n",
      "Epoch 23/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1651 - acc: 0.9554\n",
      "Epoch 24/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1603 - acc: 0.9557\n",
      "Epoch 25/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1557 - acc: 0.9558\n",
      "Epoch 26/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.1507 - acc: 0.9557\n",
      "Epoch 27/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1454 - acc: 0.9568\n",
      "Epoch 28/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.1401 - acc: 0.9576\n",
      "Epoch 29/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1352 - acc: 0.9579\n",
      "Epoch 30/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1305 - acc: 0.9583\n",
      "Epoch 31/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1290 - acc: 0.9577\n",
      "Epoch 32/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1267 - acc: 0.9565\n",
      "Epoch 33/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1249 - acc: 0.9552\n",
      "Epoch 34/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1222 - acc: 0.9549\n",
      "Epoch 35/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.1187 - acc: 0.9560\n",
      "Epoch 36/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1153 - acc: 0.9570\n",
      "Epoch 37/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1115 - acc: 0.9584\n",
      "Epoch 38/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1099 - acc: 0.9588\n",
      "Epoch 39/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1073 - acc: 0.9598\n",
      "Epoch 40/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1064 - acc: 0.9599\n",
      "Epoch 41/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1044 - acc: 0.9603\n",
      "Epoch 42/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1034 - acc: 0.9606\n",
      "Epoch 43/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1019 - acc: 0.9607\n",
      "Epoch 44/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1010 - acc: 0.9604\n",
      "Epoch 45/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.1002 - acc: 0.9604\n",
      "Epoch 46/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0983 - acc: 0.9612\n",
      "Epoch 47/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0971 - acc: 0.9618\n",
      "Epoch 48/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0959 - acc: 0.9622\n",
      "Epoch 49/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0954 - acc: 0.9625\n",
      "Epoch 50/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0941 - acc: 0.9631\n",
      "Epoch 51/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0937 - acc: 0.9634\n",
      "Epoch 52/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0926 - acc: 0.9637\n",
      "Epoch 53/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0915 - acc: 0.9643\n",
      "Epoch 54/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0910 - acc: 0.9643\n",
      "Epoch 55/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0900 - acc: 0.9650\n",
      "Epoch 56/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0897 - acc: 0.9645\n",
      "Epoch 57/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0891 - acc: 0.9647\n",
      "Epoch 58/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0888 - acc: 0.9648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0882 - acc: 0.9652\n",
      "Epoch 60/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0875 - acc: 0.9653\n",
      "Epoch 61/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0869 - acc: 0.9652\n",
      "Epoch 62/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0862 - acc: 0.9654\n",
      "Epoch 63/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0861 - acc: 0.9653\n",
      "Epoch 64/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0855 - acc: 0.9661\n",
      "Epoch 65/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0857 - acc: 0.9658\n",
      "Epoch 66/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0844 - acc: 0.9662\n",
      "Epoch 67/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0842 - acc: 0.9662\n",
      "Epoch 68/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0845 - acc: 0.9660\n",
      "Epoch 69/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0837 - acc: 0.9665\n",
      "Epoch 70/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0833 - acc: 0.9667\n",
      "Epoch 71/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0830 - acc: 0.9668\n",
      "Epoch 72/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0826 - acc: 0.9667\n",
      "Epoch 73/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0827 - acc: 0.9668\n",
      "Epoch 74/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0822 - acc: 0.9676\n",
      "Epoch 75/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0820 - acc: 0.9667\n",
      "Epoch 76/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0818 - acc: 0.9672\n",
      "Epoch 77/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0811 - acc: 0.9672\n",
      "Epoch 78/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0808 - acc: 0.9674\n",
      "Epoch 79/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0806 - acc: 0.9676\n",
      "Epoch 80/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0807 - acc: 0.9671\n",
      "Epoch 81/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0805 - acc: 0.9675\n",
      "Epoch 82/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0793 - acc: 0.9681\n",
      "Epoch 83/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0797 - acc: 0.9680\n",
      "Epoch 84/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0793 - acc: 0.9679\n",
      "Epoch 85/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0798 - acc: 0.9675\n",
      "Epoch 86/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0790 - acc: 0.9681\n",
      "Epoch 87/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0787 - acc: 0.9683\n",
      "Epoch 88/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0788 - acc: 0.9679\n",
      "Epoch 89/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0786 - acc: 0.9682\n",
      "Epoch 90/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0782 - acc: 0.9682\n",
      "Epoch 91/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0780 - acc: 0.9685\n",
      "Epoch 92/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0778 - acc: 0.9682\n",
      "Epoch 93/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0775 - acc: 0.9687\n",
      "Epoch 94/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0778 - acc: 0.9685\n",
      "Epoch 95/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0776 - acc: 0.9684\n",
      "Epoch 96/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0769 - acc: 0.9686\n",
      "Epoch 97/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0770 - acc: 0.9689\n",
      "Epoch 98/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0767 - acc: 0.9689\n",
      "Epoch 99/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0763 - acc: 0.9691\n",
      "Epoch 100/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0766 - acc: 0.9688\n",
      "Epoch 101/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0762 - acc: 0.9694\n",
      "Epoch 102/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0765 - acc: 0.9688\n",
      "Epoch 103/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0757 - acc: 0.9691\n",
      "Epoch 104/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0754 - acc: 0.9692\n",
      "Epoch 105/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0758 - acc: 0.9692\n",
      "Epoch 106/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0755 - acc: 0.9693\n",
      "Epoch 107/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0752 - acc: 0.9691\n",
      "Epoch 108/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0749 - acc: 0.9697\n",
      "Epoch 109/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0748 - acc: 0.9696\n",
      "Epoch 110/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0749 - acc: 0.9693\n",
      "Epoch 111/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0749 - acc: 0.9694\n",
      "Epoch 112/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0745 - acc: 0.9697\n",
      "Epoch 113/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0743 - acc: 0.9697\n",
      "Epoch 114/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0740 - acc: 0.9699\n",
      "Epoch 115/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0740 - acc: 0.9696\n",
      "Epoch 116/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0741 - acc: 0.9694\n",
      "Epoch 117/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0741 - acc: 0.9694\n",
      "Epoch 118/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0737 - acc: 0.9698\n",
      "Epoch 119/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0732 - acc: 0.9704\n",
      "Epoch 120/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0734 - acc: 0.9699\n",
      "Epoch 121/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0734 - acc: 0.9698\n",
      "Epoch 122/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0730 - acc: 0.9703\n",
      "Epoch 123/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0730 - acc: 0.9701\n",
      "Epoch 124/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0731 - acc: 0.9697\n",
      "Epoch 125/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0725 - acc: 0.9702\n",
      "Epoch 126/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0728 - acc: 0.9701\n",
      "Epoch 127/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0723 - acc: 0.9704\n",
      "Epoch 128/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0724 - acc: 0.9703\n",
      "Epoch 129/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0726 - acc: 0.9703\n",
      "Epoch 130/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0721 - acc: 0.9705\n",
      "Epoch 131/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0721 - acc: 0.9703\n",
      "Epoch 132/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0719 - acc: 0.9703\n",
      "Epoch 133/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0717 - acc: 0.9703\n",
      "Epoch 134/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0718 - acc: 0.9708\n",
      "Epoch 135/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0717 - acc: 0.9705\n",
      "Epoch 136/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0717 - acc: 0.9707\n",
      "Epoch 137/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0715 - acc: 0.9706\n",
      "Epoch 138/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0711 - acc: 0.9708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0710 - acc: 0.9705\n",
      "Epoch 140/5000\n",
      "36005/36005 [==============================] - 1s 28us/step - loss: 0.0711 - acc: 0.9706\n",
      "Epoch 141/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0709 - acc: 0.9710\n",
      "Epoch 142/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0707 - acc: 0.9707\n",
      "Epoch 143/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0708 - acc: 0.9711\n",
      "Epoch 144/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0708 - acc: 0.9708\n",
      "Epoch 145/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0707 - acc: 0.9708\n",
      "Epoch 146/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0705 - acc: 0.9711\n",
      "Epoch 147/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0698 - acc: 0.9711\n",
      "Epoch 148/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0705 - acc: 0.9708\n",
      "Epoch 149/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0702 - acc: 0.9709\n",
      "Epoch 150/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0698 - acc: 0.9712\n",
      "Epoch 151/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0698 - acc: 0.9710\n",
      "Epoch 152/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0699 - acc: 0.9711\n",
      "Epoch 153/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0697 - acc: 0.9711\n",
      "Epoch 154/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0694 - acc: 0.9713\n",
      "Epoch 155/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0695 - acc: 0.9712\n",
      "Epoch 156/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0696 - acc: 0.9710\n",
      "Epoch 157/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0693 - acc: 0.9712\n",
      "Epoch 158/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0688 - acc: 0.9712\n",
      "Epoch 159/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0690 - acc: 0.9714\n",
      "Epoch 160/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0692 - acc: 0.9710\n",
      "Epoch 161/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0690 - acc: 0.9714\n",
      "Epoch 162/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0690 - acc: 0.9714\n",
      "Epoch 163/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0685 - acc: 0.9714\n",
      "Epoch 164/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0690 - acc: 0.9713\n",
      "Epoch 165/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0685 - acc: 0.9714\n",
      "Epoch 166/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0684 - acc: 0.9717\n",
      "Epoch 167/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0680 - acc: 0.9714\n",
      "Epoch 168/5000\n",
      "36005/36005 [==============================] - 1s 28us/step - loss: 0.0686 - acc: 0.9714\n",
      "Epoch 169/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0681 - acc: 0.9715\n",
      "Epoch 170/5000\n",
      "36005/36005 [==============================] - 1s 28us/step - loss: 0.0682 - acc: 0.9716\n",
      "Epoch 171/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0679 - acc: 0.9716\n",
      "Epoch 172/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0680 - acc: 0.9718\n",
      "Epoch 173/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0682 - acc: 0.9714\n",
      "Epoch 174/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0674 - acc: 0.9717\n",
      "Epoch 175/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0677 - acc: 0.9718\n",
      "Epoch 176/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0676 - acc: 0.9717\n",
      "Epoch 177/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0668 - acc: 0.9724\n",
      "Epoch 178/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0677 - acc: 0.9720\n",
      "Epoch 179/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0673 - acc: 0.9721\n",
      "Epoch 180/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0675 - acc: 0.9722\n",
      "Epoch 181/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0674 - acc: 0.9721\n",
      "Epoch 182/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0671 - acc: 0.9722\n",
      "Epoch 183/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0674 - acc: 0.9721\n",
      "Epoch 184/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0671 - acc: 0.9721\n",
      "Epoch 185/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0671 - acc: 0.9720\n",
      "Epoch 186/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0671 - acc: 0.9720\n",
      "Epoch 187/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0663 - acc: 0.9719\n",
      "Epoch 188/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0670 - acc: 0.9718\n",
      "Epoch 189/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0666 - acc: 0.9721\n",
      "Epoch 190/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0669 - acc: 0.9718\n",
      "Epoch 191/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0665 - acc: 0.9723\n",
      "Epoch 192/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0663 - acc: 0.9720\n",
      "Epoch 193/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0662 - acc: 0.9726\n",
      "Epoch 194/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0664 - acc: 0.9721\n",
      "Epoch 195/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0666 - acc: 0.9720\n",
      "Epoch 196/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0661 - acc: 0.9724\n",
      "Epoch 197/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0663 - acc: 0.9718\n",
      "Epoch 198/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0658 - acc: 0.9723\n",
      "Epoch 199/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0663 - acc: 0.9723\n",
      "Epoch 200/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0658 - acc: 0.9726\n",
      "Epoch 201/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0658 - acc: 0.9723\n",
      "Epoch 202/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0655 - acc: 0.9728\n",
      "Epoch 203/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0655 - acc: 0.9725\n",
      "Epoch 204/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0658 - acc: 0.9723\n",
      "Epoch 205/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0655 - acc: 0.9727\n",
      "Epoch 206/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0657 - acc: 0.9724\n",
      "Epoch 207/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0652 - acc: 0.9728\n",
      "Epoch 208/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0655 - acc: 0.9725\n",
      "Epoch 209/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0655 - acc: 0.9725\n",
      "Epoch 210/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0655 - acc: 0.9726\n",
      "Epoch 211/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0652 - acc: 0.9726\n",
      "Epoch 212/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0650 - acc: 0.9725\n",
      "Epoch 213/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0647 - acc: 0.9728\n",
      "Epoch 214/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0648 - acc: 0.9727\n",
      "Epoch 215/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0649 - acc: 0.9729\n",
      "Epoch 216/5000\n",
      "36005/36005 [==============================] - 1s 28us/step - loss: 0.0649 - acc: 0.9729\n",
      "Epoch 217/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0647 - acc: 0.9729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 218/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0646 - acc: 0.9730\n",
      "Epoch 219/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0649 - acc: 0.9726\n",
      "Epoch 220/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0648 - acc: 0.9729\n",
      "Epoch 221/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0644 - acc: 0.9731\n",
      "Epoch 222/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0647 - acc: 0.9730\n",
      "Epoch 223/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0646 - acc: 0.9733\n",
      "Epoch 224/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0644 - acc: 0.9730\n",
      "Epoch 225/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0642 - acc: 0.9733\n",
      "Epoch 226/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0641 - acc: 0.9729\n",
      "Epoch 227/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0642 - acc: 0.9730\n",
      "Epoch 228/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0637 - acc: 0.9732\n",
      "Epoch 229/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0641 - acc: 0.9729\n",
      "Epoch 230/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0642 - acc: 0.9731\n",
      "Epoch 231/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0638 - acc: 0.9734\n",
      "Epoch 232/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0641 - acc: 0.9731\n",
      "Epoch 233/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0638 - acc: 0.9732\n",
      "Epoch 234/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0640 - acc: 0.9732\n",
      "Epoch 235/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0639 - acc: 0.9731\n",
      "Epoch 236/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0636 - acc: 0.9734\n",
      "Epoch 237/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0637 - acc: 0.9731\n",
      "Epoch 238/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0637 - acc: 0.9732\n",
      "Epoch 239/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0636 - acc: 0.9733\n",
      "Epoch 240/5000\n",
      "36005/36005 [==============================] - 1s 28us/step - loss: 0.0634 - acc: 0.9732\n",
      "Epoch 241/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0636 - acc: 0.9730\n",
      "Epoch 242/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0634 - acc: 0.9729\n",
      "Epoch 243/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0635 - acc: 0.9732\n",
      "Epoch 244/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0635 - acc: 0.9735\n",
      "Epoch 245/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0638 - acc: 0.9731\n",
      "Epoch 246/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0634 - acc: 0.9736\n",
      "Epoch 247/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0632 - acc: 0.9734\n",
      "Epoch 248/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0632 - acc: 0.9732\n",
      "Epoch 249/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0629 - acc: 0.9736\n",
      "Epoch 250/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0629 - acc: 0.9734\n",
      "Epoch 251/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0631 - acc: 0.9736\n",
      "Epoch 252/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0631 - acc: 0.9733\n",
      "Epoch 253/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0627 - acc: 0.9735\n",
      "Epoch 254/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0626 - acc: 0.9734\n",
      "Epoch 255/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0627 - acc: 0.9736\n",
      "Epoch 256/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0630 - acc: 0.9734\n",
      "Epoch 257/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0628 - acc: 0.9734\n",
      "Epoch 258/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0624 - acc: 0.9736\n",
      "Epoch 259/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0628 - acc: 0.9733\n",
      "Epoch 260/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0624 - acc: 0.9737\n",
      "Epoch 261/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0624 - acc: 0.9736\n",
      "Epoch 262/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0620 - acc: 0.9737\n",
      "Epoch 263/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0622 - acc: 0.9740\n",
      "Epoch 264/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0625 - acc: 0.9736\n",
      "Epoch 265/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0622 - acc: 0.9739\n",
      "Epoch 266/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0623 - acc: 0.9738\n",
      "Epoch 267/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0623 - acc: 0.9734\n",
      "Epoch 268/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0623 - acc: 0.9736\n",
      "Epoch 269/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0623 - acc: 0.9737\n",
      "Epoch 270/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0616 - acc: 0.9739\n",
      "Epoch 271/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0620 - acc: 0.9739\n",
      "Epoch 272/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0624 - acc: 0.9736\n",
      "Epoch 273/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0621 - acc: 0.9739\n",
      "Epoch 274/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0617 - acc: 0.9740\n",
      "Epoch 275/5000\n",
      "36005/36005 [==============================] - 1s 28us/step - loss: 0.0619 - acc: 0.9738\n",
      "Epoch 276/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0618 - acc: 0.9737\n",
      "Epoch 277/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0617 - acc: 0.9741\n",
      "Epoch 278/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0617 - acc: 0.9738\n",
      "Epoch 279/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0617 - acc: 0.9740\n",
      "Epoch 280/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0615 - acc: 0.9737\n",
      "Epoch 281/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0617 - acc: 0.9740\n",
      "Epoch 282/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0618 - acc: 0.9740\n",
      "Epoch 283/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0616 - acc: 0.9740\n",
      "Epoch 284/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0615 - acc: 0.9741\n",
      "Epoch 285/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0619 - acc: 0.9737\n",
      "Epoch 286/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0616 - acc: 0.9739\n",
      "Epoch 287/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0613 - acc: 0.9743\n",
      "Epoch 288/5000\n",
      "36005/36005 [==============================] - 1s 28us/step - loss: 0.0613 - acc: 0.9740\n",
      "Epoch 289/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0613 - acc: 0.9742\n",
      "Epoch 290/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0610 - acc: 0.9742\n",
      "Epoch 291/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0615 - acc: 0.9742\n",
      "Epoch 292/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0611 - acc: 0.9742\n",
      "Epoch 293/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0610 - acc: 0.9744\n",
      "Epoch 294/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0610 - acc: 0.9743\n",
      "Epoch 295/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0607 - acc: 0.9743\n",
      "Epoch 296/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0610 - acc: 0.9741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0610 - acc: 0.9742\n",
      "Epoch 298/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0608 - acc: 0.9742\n",
      "Epoch 299/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0607 - acc: 0.9743\n",
      "Epoch 300/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0608 - acc: 0.9743\n",
      "Epoch 301/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0611 - acc: 0.9742\n",
      "Epoch 302/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0609 - acc: 0.9743\n",
      "Epoch 303/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0606 - acc: 0.9742\n",
      "Epoch 304/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0607 - acc: 0.9740\n",
      "Epoch 305/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0609 - acc: 0.9744\n",
      "Epoch 306/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0605 - acc: 0.9744\n",
      "Epoch 307/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0606 - acc: 0.9739\n",
      "Epoch 308/5000\n",
      "36005/36005 [==============================] - 1s 28us/step - loss: 0.0606 - acc: 0.9743\n",
      "Epoch 309/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0603 - acc: 0.9745\n",
      "Epoch 310/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0604 - acc: 0.9744\n",
      "Epoch 311/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0606 - acc: 0.9743\n",
      "Epoch 312/5000\n",
      "36005/36005 [==============================] - 1s 27us/step - loss: 0.0600 - acc: 0.9745\n",
      "Epoch 313/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0604 - acc: 0.9743\n",
      "Epoch 314/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0601 - acc: 0.9747\n",
      "Epoch 315/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0604 - acc: 0.9742\n",
      "Epoch 316/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0602 - acc: 0.9745\n",
      "Epoch 317/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0601 - acc: 0.9745\n",
      "Epoch 318/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0602 - acc: 0.9745\n",
      "Epoch 319/5000\n",
      "36005/36005 [==============================] - 1s 26us/step - loss: 0.0598 - acc: 0.9746\n",
      "Epoch 320/5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "\n",
    "\n",
    "def getKerasModeFloors2(x,enc,saveName):\n",
    "    model_name = saveName \n",
    "    model = keras.models.load_model(model_name)\n",
    "    yP5= model.predict([x], batch_size=2560)\n",
    "    nSamples = yP5.shape[0]\n",
    "     ###需要将预测出的值，转换01整数,并转为数字式\n",
    "    for i in range(yP5.shape[0]):\n",
    "        tmp = yP5[i]\n",
    "        index=  np.argmax(tmp)\n",
    "        yP5[i] = [0,0,0,0,0]\n",
    "        yP5[i,index]=1\n",
    "   \n",
    "\n",
    "    ###  \n",
    "    yP5= enc.inverse_transform(yP5)\n",
    "    yP5= yP5.reshape(-1,nSamples)[0]\n",
    "    \n",
    "    yP4 = np.zeros((yP5.shape[0],1))\n",
    "    yP3 = np.zeros((yP5.shape[0],1))\n",
    "    yP2 = np.zeros((yP5.shape[0],1))\n",
    "\n",
    "    for i in range(yP5.shape[0]):\n",
    "        if(yP5[i]== 2) or (yP5[i]== 1):\n",
    "             yP4[i] = 21\n",
    "        else:\n",
    "             yP4[i] = yP5[i]\n",
    "                \n",
    "        if(yP5[i]== 2) or (yP5[i]== 1) or (yP5[i]== 0):\n",
    "             yP3[i] = 210\n",
    "        else:\n",
    "             yP3[i] = yP5[i]\n",
    "                \n",
    "        if(yP5[i]== 2) or (yP5[i]== 1) or (yP5[i]== 0) or (yP5[i]== 3):\n",
    "             yP2[i] = 3210\n",
    "        else:\n",
    "             yP2[i] = yP5[i]\n",
    "    \n",
    "    return model,yP5,yP4,yP3,yP2\n",
    "\n",
    "#分层决策树\n",
    "def dtFitAndSave(x,y,saveName):\n",
    "    dt = tree.DecisionTreeClassifier(max_depth=10,min_samples_leaf=1000)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    data=tree.export_graphviz(dt, out_file=None,class_names=None,filled=True) \n",
    "    graph = graphviz.Source(data)\n",
    "    graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(tmp1)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    #text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    return dt\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "def getDTSamplesInfo(x,dt):\n",
    "    yPredict = dt.predict_proba(x)\n",
    "    #print(\"\\n\\n getDTSamplesInfo yPredict\",yPredict)\n",
    "    d_path = dt.decision_path(x).todense()\n",
    "    #print(\"\\n\\n d_path\",d_path)\n",
    "    #print(\"impurity\",dt.tree_.impurity)\n",
    "    #print(\"feature\",dt.tree_.feature)\n",
    "    #print(\"threshold\",dt.tree_.threshold)\n",
    "    \n",
    "    #左节点编号  :  clf.tree_.children_left\n",
    "    #右节点编号  :  clf.tree_.children_right\n",
    "    #分割的变量  :  clf.tree_.feature\n",
    "    #分割的阈值  :  clf.tree_.threshold\n",
    "    #不纯度(gini) :  clf.tree_.impurity\n",
    "    #样本个数      :  clf.tree_.n_node_samples\n",
    "    #样本分布      :  clf.tree_.value\n",
    "    #https://blog.csdn.net/ywj_1991/article/details/122985778\n",
    "    #https://www.javaroad.cn/questions/54003\n",
    "    \n",
    "    h,w = d_path.shape\n",
    "    gini =np.zeros((h,1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(h):\n",
    "       path = d_path[i]\n",
    "       v,ind = np.where(path>0)\n",
    "       xtmp = x[i]\n",
    "       #print(\"path\",path,ind,np.array(ind)[-1])\n",
    "    \n",
    "       #print(\"\\n index\",index)\n",
    "       #print(\"impurity\",dt.tree_.impurity[ind])\n",
    "       #print(\"feature\",dt.tree_.feature[ind])\n",
    "       #print(\"threshold\",dt.tree_.threshold[ind])\n",
    "       #print(\"x[index]\",xtmp[ind])\n",
    "       \n",
    "      \n",
    "       #print(\"the leaf node:\",np.array(ind)[-1],\"the simplest rule is\")\n",
    "       #for jj in ind:\n",
    "       #    if dt.tree_.feature[jj] == -2:\n",
    "       #         print(\"label,proba is\",yPredict[i,0],yPredict[i,1])\n",
    "       #         break\n",
    "                \n",
    "       #    if xtmp[jj]<=dt.tree_.threshold[jj]:\n",
    "       #       print(\" x[%d]<=%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "       #    else:\n",
    "       #       print(\" x[%d]>%.3f\" %(dt.tree_.feature[jj],dt.tree_.threshold[jj]))\n",
    "                    \n",
    "       finalPos = np.array(ind)[-1]\n",
    "       gini[i] = dt.tree_.impurity[finalPos]\n",
    "       \n",
    "       #print(\"d_path\",i,path,dt.tree_.impurity[finalPos])\n",
    "       #print(dt.tree_.feature[finalPos])\n",
    "       #print(dt.tree_.threshold[finalPos])\n",
    "       #print(dt.tree_.n_node_samples[finalPos])\n",
    "\n",
    "    \n",
    "       return gini,yPredict\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "def computeAndCompareHybridMode(x,y,dt,kerasPLabel,floorLabel):\n",
    "    nSamples,feturesNume  = x.shape\n",
    "    yHyLabel  = np.zeros((nSamples,1))\n",
    "    hyCounter = 0\n",
    "    for i in range(nSamples):\n",
    "        print(i)\n",
    "        xtmp = x[i]\n",
    "        giniFloor,yPredictProFloor = getDTSamplesInfo([xtmp],dt)\n",
    "        giniTmp = giniFloor[0]\n",
    "        yPredictProFloorTmp = yPredictProFloor[0]\n",
    "        #print('gini',giniTmp )\n",
    "        #print('probPredict',yPredictProFloor0Tmp  )\n",
    "        if giniTmp >0.05 or max(yPredictProFloorTmp)<0.98:\n",
    "            yHyLabel[i] = kerasPLabel[i]\n",
    "        else:\n",
    "            #floorLabel= [3,4,210]\n",
    "            index = np.argmax(yPredictProFloorTmp)\n",
    "            yHyLabel[i] = floorLabel[index]\n",
    "            hyCounter = hyCounter+1\n",
    "    print('floorLabel\\n',floorLabel) \n",
    "    print('hyCounter\\n',hyCounter)    \n",
    "\n",
    "\n",
    "    tmp1 = classification_report(y,yHyLabel)\n",
    "    print('hybrid\\n',tmp1)\n",
    "    tmp1 = classification_report(y,kerasPLabel)\n",
    "    print('keras\\n',tmp1)\n",
    "    mat1num = confusion_matrix(y,yHyLabel)\n",
    "    mat2acc = confusion_matrix(y,yHyLabel,normalize='pred')\n",
    "    print('mat1num\\n',mat1num)\n",
    "    print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "    return\n",
    "\n",
    "##########################################################################\n",
    "###简单模型2，有隐藏层\n",
    "def kerasFitAndSaveSimple2(x,yOneHot,num_labels,filename):\n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 512\n",
    "    dropout_rate = 0.05\n",
    "    build_model = tf.keras.Sequential()\n",
    "    build_model.add(layers.Dense(relu_size, activation='relu',name=\"layer1\",input_shape=(features_size,)))\n",
    "    build_model.add(layers.Dropout(dropout_rate,name=\"Dropout1-2\"))\n",
    "    build_model.add(layers.Dense(relu_size/2, activation='relu',name=\"layer2\"))\n",
    "    build_model.add(layers.Dropout(dropout_rate,name=\"Dropout2-3\"))\n",
    "    build_model.add(layers.Dense(num_labels, activation='sigmoid',name=\"layer3\"))\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    build_model.fit(x,yOneHot,epochs=5000, batch_size=80000*1)\n",
    "    #build_model.fit([x],[yOneHot],epochs=100000, batch_size=80000*1)\n",
    "    #build_model.save(\"kerasSimple2.h5\")\n",
    "    build_model.save(filename)\n",
    "    #plot_model(build_model, to_file='KerasSimple2_HiddenLayer.png', show_shapes=True)\n",
    "    return build_model\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "####用法国数据进行验证\n",
    "file1 = \"./trainData/france_0_allSamples.csv\"\n",
    "print(\"reading data france\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "print(xyDataTmp.info())\n",
    "\n",
    "xyData = np.array(xyDataTmp)\n",
    "h,w = xyData.shape\n",
    "x = xyData[:,1:23]#简单处理与SUMO数据库一致,到vehSpeed_8\n",
    "x = xyData[:,1:w-1]#用原始数据\n",
    "y = xyData[:,w-1]\n",
    "\n",
    "x = x.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "yl5 = y\n",
    "print(\"x.shape:\",x.shape,\"y.shape:\",y.shape,\"y.type:\", type(y),\"y.unique\",np.unique(y) )\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    " \n",
    "####\n",
    "'''\n",
    "file1 = \"./trainData/dataAllSim1000.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "\n",
    "xSumo = xyData[:,0:22]\n",
    "ySumo = xyData[:,22:26]\n",
    "ySumo= ySumo[:,2]#01234\n",
    "\n",
    "xSumo = xSumo.astype(np.float32)\n",
    "ySumo= np.floor(ySumo).astype(np.int64)\n",
    "index = np.where(ySumo>=0)\n",
    "xSumo = xSumo[index]\n",
    "ySumo = ySumo[index]\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "\n",
    "\n",
    "x = np.concatenate((xSumo,x))\n",
    "y = np.concatenate((ySumo,y))\n",
    "yl5 = y\n",
    "print(\"x.shape:\",x.shape,\"y.shape:\",yl5.shape,\"y.type:\", type(yl5),\"yl5.unique\",np.unique(yl5) )\n",
    "\n",
    "'''\n",
    "##########################################################################\n",
    "###keras拟合,oneHot\n",
    "nSamples,nFeatures =  x.shape\n",
    "enc = OneHotEncoder()\n",
    "yl5= yl5.reshape(nSamples,-1)\n",
    "enc.fit(yl5)  \n",
    "\n",
    " \n",
    "##keras拟合\n",
    "if 1:\n",
    "    filename = \"kerasSimple2_franceDataset1.h5\"\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, yl5, test_size = 0.5)\n",
    "    yOneHot=enc.transform(y_train).toarray()\n",
    "    num_labels = 5\n",
    "    print('x_train.shape',x_train.shape,'yOneHot.shape',yOneHot.shape)\n",
    "    simpleMode2 = kerasFitAndSaveSimple2(x_train,yOneHot,num_labels,filename)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "return\n",
    "\n",
    "index = np.where((yl5 == 2) | (yl5 == 1))\n",
    "yl4 = yl5.copy()\n",
    "yl4[index]=21\n",
    "print(yl4)\n",
    "\n",
    "\n",
    "index = np.where((yl5 == 2) | (yl5 == 1) | (yl5 == 0))\n",
    "yl3 = yl5.copy()\n",
    "yl3[index]=210\n",
    "print(yl3)\n",
    "\n",
    "\n",
    "\n",
    "index = np.where( (yl5 == 3)|(yl5 == 2) | (yl5 == 1) | (yl5 == 0))\n",
    "yl2 = yl5.copy()\n",
    "yl2[index]=3210\n",
    "print(yl2)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "#hierachFloor['floor3'][\"dt\"] = dtFitAndSave(x,yl2,\"Floo3_2\")\n",
    "#hierachFloor['floor2'][\"dt\"] = dtFitAndSave(x,yl3,\"Floo2_3\")\n",
    "#hierachFloor['floor1'][\"dt\"] = dtFitAndSave(x,yl4,\"Floor1_4\")\n",
    "#hierachFloor['floor0'][\"dt\"] = dtFitAndSave(x,yl5,\"Floor0_5\")\n",
    "dt_floor0_label5 = dtFitAndSave(x,yl5,\"Floor0_5\")\n",
    "\n",
    "kerasFloors,yKerasP5,yKerasP4,yKerasP3,yKerasP2=getKerasModeFloors2(x,enc,'kerasSimple2.h5')\n",
    "\n",
    "\n",
    "#floor=3 ,label=2\n",
    "#dt = hierachFloor['floor3'][\"dt\"]\n",
    "#floorLabel= [4,3210] \n",
    "#computeAndCompareHybridMode(x,yl2,dt,yKerasP2,floorLabel)\n",
    "\n",
    "\n",
    "dt = dt_floor0_label5\n",
    "floorLabel= [0,1,2,3,4] \n",
    "computeAndCompareHybridMode(x,yl5,dt,yKerasP5,floorLabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a47825",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mainTestCSVMLP3(hmcnf_keras).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:keras220CpuJupyter]",
   "language": "python",
   "name": "conda-env-keras220CpuJupyter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
