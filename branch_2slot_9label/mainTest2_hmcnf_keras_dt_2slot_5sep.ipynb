{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f6c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T03:55:17.555121Z",
     "start_time": "2023-01-26T03:55:17.435109Z"
    }
   },
   "outputs": [],
   "source": [
    "#一些常用的命令\n",
    "!nvidia-smi\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#一些常用的命令\n",
    "!git status\n",
    "!git add .\n",
    "!git commit -m \"correct and optimize some code\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0809da8b-07fe-447a-b1fd-d7520e79d721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全局共享函数，运行主程序前需要运行\n"
     ]
    }
   ],
   "source": [
    "############print(\"程序0.000 全局共享函数，运行主程序前需要运行\")\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import pickle  \n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "#######开始为功能函数\n",
    "print(\"全局共享函数，运行主程序前需要运行\")\n",
    "def dtFitAndSave(x,y,saveName):\n",
    "    str1=\"dtFitAndSave,用于决策树拟合和识别\"\n",
    "    \n",
    "    dt = tree.DecisionTreeClassifier(max_depth=7,min_samples_leaf=100)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    #data=tree.export_graphviz(dt, out_file=None,class_names=None,filled=True) \n",
    "    #graph = graphviz.Source(data)\n",
    "    #graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(\"纯决策树的识别\\n\",tmp1)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    #text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    return dt,yPredict\n",
    "\n",
    "########################################################################################################################\n",
    "###简单模型3，resnet_like\n",
    "def local_model(num_labels, dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(num_labels, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def global_model(dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='sigmoid',name=\"global\"))\n",
    "    return model\n",
    "\n",
    "def softmax_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='softmax',name=\"global\"))\n",
    "    return model\n",
    "'''\n",
    "############################################################################\n",
    "############################################################################\n",
    "#单层模型\n",
    "def kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels,saveName):\n",
    "    str1=\"kerasFitAndSaveSimple3LikeResnet,用于resnet_like的神经网络拟合和识别\"\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 512\n",
    "    dropout_rate = 0.05\n",
    "    hierarchy = [1,1,1,1]#四层，对于当前数据集已经足够了\n",
    "    global_models = []\n",
    "   \n",
    "    \n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(len(hierarchy)):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "\n",
    "   \n",
    "    \n",
    "    p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "       build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file='KerasSimple3_likeResnet_4lay512nodes.jpg', show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=1500, batch_size=40000*1)#GPU用这个\n",
    "    #saveName = \"KerasSimple3_likeResnet.h5\"\n",
    "    build_model.save(saveName)\n",
    "    plot_model(build_model, to_file='KerasSimple3_likeResnet_4lay512nodes.jpg', show_shapes=True)\n",
    "    return build_model\n",
    "\n",
    "############################################################################\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，\n",
    "def kerasFitAndSaveHierSimple4LikeResnet(x,yOneHot,num_labels,saveName):\n",
    "    str1=\"kerasFitAndSaveHierSimple4LikeResnet,用于resnet_like的 神经网络拟合和识别\"\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    beta = 0.5\n",
    "    hierarchy = [2,4,6,8,9]#5层，对于当前数据集已经足够了\n",
    "    global_models = []\n",
    "    local_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(len(hierarchy)):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "    \n",
    "    for i in range(len(hierarchy)):\n",
    "        local_models.append(local_model(hierarchy[i], dropout_rate, relu_size)(global_models[i]))\n",
    "        \n",
    "        \n",
    "    #显示只有局部局模型的情况(部分全局)\n",
    "    p_loc = layers.concatenate(local_models)\n",
    "    #modelTmp2 = tf.keras.Model(inputs=[features], outputs=[p_loc])\n",
    "    #modelTmp2.summary()#\n",
    "    #plot_model(modelTmp2, to_file='Flatten2.png', show_shapes=True)\n",
    "    p_glob1 = layers.Lambda(lambda x: x*beta,name=\"global\")(p_glob)\n",
    "    p_loc1 = layers.Lambda(lambda x: x*(1-beta),name=\"local\")(p_loc)\n",
    "\n",
    "    labels = layers.add([p_glob1, p_loc1])\n",
    "\n",
    "\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[labels])\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file='hmcnf1.jpg', show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=3500, batch_size=40000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "'''\n",
    "############################################################################\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，所以采用分离式\n",
    "###每一层的识别模型都是4层模型\n",
    "def g_sepHier1(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 10,srelu_size = 256,dropout_rate = 0.05):\n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.01\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    if 0:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=40000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "def getKerasResnetRVL(x,enc,saveName):\n",
    "    model_name = saveName \n",
    "    model = keras.models.load_model(model_name)\n",
    "    y= model.predict([x], batch_size=2560)\n",
    "    nSamples = y.shape[0]\n",
    "    ###需要将预测出的值，转换01整数,并转为数字式\n",
    "    for i in range(y.shape[0]):\n",
    "        tmp = y[i]\n",
    "        index=  np.argmax(tmp)\n",
    "        y[i] = [0]*y.shape[1]\n",
    "        y[i,index]=1\n",
    "   \n",
    "\n",
    "    ###  \n",
    "    y= enc.inverse_transform(y)\n",
    "    y= y.reshape(-1,nSamples)[0]\n",
    "    \n",
    "    \n",
    "    return y\n",
    "\n",
    "def string2int(inputString):\n",
    "     #print(inputString)\n",
    "     tmp = 0\n",
    "     try:\n",
    "         strTmp=[str(ord(x)) for x in inputString]\n",
    "         tmp=tmp.join(strTmp)\n",
    "         tmp = float(tmp)/(len(inputString)*128)\n",
    "     except:\n",
    "         #print(inputString)\n",
    "         strTmp = inputString\n",
    "         tmp= \"0\"\n",
    "         tmp = 0\n",
    "     return tmp\n",
    " ## 根据经验以及最佳正确率的合并方法\n",
    " #第一次合并为0,1的合并\n",
    "\n",
    "########################################################################################################################\n",
    "##手工确定层次结构，以前测试时候为5层，根据论文为9层\n",
    "def convertY2Hieral(y):\n",
    "    #mat2acc\n",
    "    # [[0.914 0.009 0.017 0.007 0.032 0.    0.    0.    0.   ]\n",
    "    # [0.027 0.984 0.006 0.007 0.018 0.    0.    0.    0.   ]\n",
    "    # [0.02  0.006 0.972 0.    0.011 0.    0.    0.    0.   ]\n",
    "    # [0.036 0.002 0.    0.986 0.014 0.    0.002 0.    0.   ]\n",
    "    # [0.003 0.    0.    0.    0.925 0.    0.    0.    0.   ]\n",
    "    # [0.    0.    0.    0.    0.    1.    0.005 0.    0.   ]\n",
    "    # [0.    0.    0.    0.    0.    0.    0.993 0.    0.004]\n",
    "    # [0.    0.    0.    0.    0.    0.    0.    0.996 0.   ]\n",
    "    # [0.    0.    0.004 0.    0.    0.    0.    0.004 0.996]]\n",
    "    \n",
    "    \n",
    "    #hierarchy = [2,4,6,8,9]\n",
    "   # labelDict = {\"0\":[\"01234\",\"0123\",\"012\",\"01\",\"0\"],\\\n",
    "   #               \"1\":[\"01234\",\"0123\",\"012\",\"01\",\"1\"],\\\n",
    "   #               \"2\":[\"01234\",\"0123\",\"012\",\"2\",\"2\"],\\\n",
    "   #               \"3\":[\"01234\",\"0123\",\"3\",\"3\",\"3\"],\\\n",
    "   #              \"4\":[\"01234\",\"4\",    \"4\",\"4\",\"4\"],\\\n",
    "   #              \"5\":[\"5678\",\"5\",     \"5\",\"5\",\"5\"],\\\n",
    "   #              \"6\":[\"5678\",\"678\",   \"67\",\"6\",\"6\"],\\\n",
    "   #              \"7\":[\"5678\",\"678\",   \"67\",\"7\",\"7\"],\\\n",
    "   #              \"8\":[\"5678\",\"678\",   \"8\",\"8\",\"8\"],\\\n",
    "   #               }\n",
    "    \n",
    "    hierarchy = [2,3,4,5,6,7,8,9]\n",
    "    labelDict = {\"0\":[\"01234\",        \"01234\",        \"01234\",   \"01234\",      \"0123\",\"012\",\"01\",\"0\"],\\\n",
    "                  \"1\":[\"01234\",        \"01234\",        \"01234\",  \"01234\",     \"0123\",\"012\",\"01\",\"1\"],\\\n",
    "                  \"2\":[\"01234\",          \"01234\",      \"01234\",  \"01234\",     \"0123\",\"012\",\"2\",\"2\"],\\\n",
    "                  \"3\":[\"01234\",         \"01234\",       \"01234\",  \"01234\",    \"0123\",\"3\",\"3\",\"3\"],\\\n",
    "                 \"4\":[\"01234\",          \"01234\",       \"01234\",  \"01234\" ,     \"4\", \"4\",\"4\",\"4\"],\\\n",
    "                 \"5\":[\"5678\",               \"5\",        \"5\" ,      \"5\",       \"5\", \"5\",\"5\",\"5\"],\\\n",
    "                 \"6\":[\"5678\",            \"678\",        \"6\",        \"6\",       \"6\", \"6\",\"6\",\"6\"],\\\n",
    "                 \"7\":[\"5678\",             \"678\",       \"78\",       \"7\",       \"7\", \"7\",\"7\",\"7\"],\\\n",
    "                 \"8\":[\"5678\",             \"678\",       \"78\" ,      \"8\",        \"8\", \"8\",\"8\",\"8\"],\\\n",
    "                  }\n",
    "    '''\n",
    "    hierarchy = [5,9]\n",
    "    labelDict = {\"0\":[\"01\",\"0\"],\\\n",
    "                  \"1\":[\"01\",\"1\"],\\\n",
    "                  \"2\":[\"2\",\"2\"],\\\n",
    "                  \"3\":[\"34\",\"3\"],\\\n",
    "                \"4\":[\"34\",\"4\"],\\\n",
    "                 \"5\":[\"56\",\"5\"],\\\n",
    "                 \"6\":[\"56\",\"6\"],\\\n",
    "                 \"7\":[\"78\",\"7\"],\\\n",
    "                 \"8\":[\"78\",\"8\"],\\\n",
    "                 }\n",
    "    '''\n",
    "\n",
    "    y1 = [list(labelDict[str(x)]) for x in y]\n",
    "   \n",
    "    #print(\"!!!y1.type:\", type(y1))\n",
    "    #print(y1[:2])\n",
    "    #y2 = [t1[0] for t1 in y1]\n",
    "    #print(len(y2))\n",
    "  \n",
    "\n",
    "    return y1,hierarchy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dbf652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T14:33:49.232503Z",
     "start_time": "2023-02-13T14:31:39.188869Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##主程序开始######################################################################################################################\n",
    "print(\"0.主程序开始，建立多层嵌套决策树模型，3080ti的GPU是AMD2400CPU 运算速度100倍\")\n",
    "print(\"0.这是简化程序，原始带有更多测试和原始模型的程序在mainTestCSVMLP3(hmcnf_keras).ipynb\")\n",
    "print(\"程序编号为0\")\n",
    "########################################################################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import pickle  \n",
    "\n",
    "\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，所以采用分离式\n",
    "###每一层的识别模型都是4层模型\n",
    "def sepHier1(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 10,srelu_size = 256,dropout_rate = 0.05):\n",
    "    \n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    featuresInput = layers.Input(shape=(features_size,))\n",
    "    features = layers.BatchNormalization()(featuresInput)\n",
    "    #features=featuresInput\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[featuresInput], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        #build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=160000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "print(\"0.主程序开始, 建立多层嵌套决策树模型,3080ti的GPU是AMD2400CPU 运算速度100倍\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "#from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "#policy = mixed_precision.Policy('mixed_float16')\n",
    "#mixed_precision.set_policy(policy)\n",
    "\n",
    " \n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "print(\"读取France数据并且把数据进行onehot处理\")\n",
    "\n",
    "#file1 = \"../trainData/france_0_allSamples1.csv\"\n",
    "file1 = \"../trainData/france_0_allSamples1_2slot.csv\"\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "h,w = xyData.shape\n",
    "#x = xyData[:,1:23]#简单处理与SUMO数据库一致\n",
    "x0rigin = xyData[:,1:w-1]#用所有的数据,第0列为vehID,不要\n",
    "y0rigin  = xyData[:,w-1]\n",
    "\n",
    "x0rigin[:,6] = [string2int(inputString) for inputString in x0rigin[:,6] ]#字符串vehLaneID 变为整数\n",
    "\n",
    "x0rigin =x0rigin.astype(np.float32)#GPU 加这个\n",
    "y0rigin =y0rigin.astype(np.int64)#GPU 加这个\n",
    "\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "x0,y0= ros.fit_resample(x0rigin , y0rigin)#对数据不平衡进行处理，保证样本数一致\n",
    "\n",
    "x0=x0.astype(np.float32)#GPU 加这个\n",
    "y0=y0.astype(np.int64)#GPU 加这个\n",
    "yl5 = y0\n",
    "print(\"x0.shape:\",x0.shape,\"y0.shape:\",y0.shape,\"y0.type:\", type(y0) )\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "###现在暂时不训练多层模型，只训练9label模型\n",
    "if 0:\n",
    "    print(\"训练4层, 9 label 模型\")\n",
    "    x=x0\n",
    "    y=yl5\n",
    "    x=x.astype(np.float32)#GPU 加这个\n",
    "    y=y.astype(np.int64)#GPU 加这个\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    \n",
    "    num_labels = 9 \n",
    "    nSamples,nFeatures =  x.shape\n",
    "    enc = OneHotEncoder()\n",
    "    y= y.reshape(nSamples,-1)\n",
    "    \n",
    "    print(\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    enc.fit(y)\n",
    "    yOneHot=enc.transform(y).toarray()\n",
    "    saveName = \"../trainedModes/model-9label-4lays-512nodes-2slots-gpu1.h5\"\n",
    "    if 0:\n",
    "        kerasModel3_5label = kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels,saveName)     \n",
    "    yKeras_5label=getKerasResnetRVL(x,enc,saveName)\n",
    "    \n",
    "    print('keras\\n')\n",
    "    mat1num = confusion_matrix(y, yKeras_5label)\n",
    "    mat2acc = confusion_matrix(y, yKeras_5label,normalize='pred')\n",
    "    print('mat1num\\n',mat1num)\n",
    "    print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "    \n",
    "    \n",
    "########################################################################################################################    \n",
    "########################################################################################################################   \n",
    "########################################################################################################################    \n",
    "########################################################################################################################    \n",
    "print(\"##############################################################################################################\")\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"1.接编号为0的主程序,根据基于正确率的聚类程序或者经验将底层类别归结到上一层的类别\")\n",
    "print(\"2.程序编号为0+\")  \n",
    " \n",
    "'''\n",
    "if 0:# 训练统合样式的HMCN-F多级模型\n",
    "    print(\"训练5hieral, 4层, 9 label 模型\")\n",
    "    x=x0\n",
    "    y=yl5\n",
    "    x=x.astype(np.float32)#GPU 加这个\n",
    "    y=y.astype(np.int64)#GPU 加这个\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    yH1 = convertY2Hieral(y)\n",
    "    hierarchy = [2,4,6,8,9]#5层，对于当前数据集已经足够了\n",
    "    \n",
    "    yH1= np.array(yH1)\n",
    "    \n",
    "    nSamples,nFeatures =  x.shape\n",
    "    enc = OneHotEncoder()\n",
    "    yH1= yH1.reshape(nSamples,-1)\n",
    "    #print(yH1[:3])\n",
    "    print(\"yH1.shape:\",yH1 .shape,\"yH1.type:\", type(yH1) )\n",
    "    enc.fit(yH1)\n",
    "    #print(enc.categories_,enc.get_feature_names())\n",
    "    yOneHot=enc.transform(yH1).toarray()\n",
    "    #print(yOneHot[:3])\n",
    "    \n",
    "    num_labels = yOneHot.shape[1] \n",
    "    print(num_labels)\n",
    "    saveName = \"../trainedModes/model-5hier-9label-5lays-128nodes-2slots-gpu1.h5\"\n",
    "    kerasModel4_5hier_9label = kerasFitAndSaveHierSimple4LikeResnet(x,yOneHot,num_labels,saveName)   \n",
    "'''    \n",
    "    \n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if 1:# 训练多级模型\n",
    "    print(\"训练分离式多级模型\")\n",
    "    \n",
    "    #准备字典，用于保存训练后的数据\"\n",
    "    xFloors=  dict()\n",
    "    yFloors =  dict()\n",
    "    xTestFloors =dict()\n",
    "    yTestFloors = dict()\n",
    "    modSaveNameFloors =dict()\n",
    "    encLevels= dict()\n",
    "    yKerasFloors = dict()\n",
    "    x=x0\n",
    "    y=yl5\n",
    "    x=x.astype(np.float32)#GPU 加这个\n",
    "    y=y.astype(np.int64)#GPU 加这个\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    print(y)\n",
    "    \n",
    "    #hierarchy = [2,4,6,8,9]\n",
    "    #hierarchy = [2,3,4,5,6,7,8,9]\n",
    "    yH1,hierarchy = convertY2Hieral(y)\n",
    "    \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, yH1, test_size=0.5, random_state=0)\n",
    "   \n",
    "    nSamples,nFeatures =  x_train.shape\n",
    "    \n",
    "    \n",
    "    numEpochs =30 #1500/60/60*5 = 2houer\n",
    "    \n",
    "    \n",
    "    for i in range(len(hierarchy)):\n",
    "        print(\"\\n\\n levelIndex\",i,\"nSamples,nFeatures\",x_train.shape)\n",
    "        levelIndex = i\n",
    "        numLayers = 4\n",
    "        enc = OneHotEncoder()\n",
    "        nSamples,nFeatures =  x_train.shape\n",
    "       \n",
    "            \n",
    "        yCurLayer1 = [t1[i] for t1 in y_train]\n",
    "        \n",
    "        yCurLayer1 = np.array(yCurLayer1)\n",
    "        print(\"yCurLayer1.shape:\",yCurLayer1.shape)\n",
    "        \n",
    "        yCurLayer1= yCurLayer1.reshape(nSamples,-1)\n",
    "        enc.fit(yCurLayer1)\n",
    "        \n",
    "        yOneHot=enc.transform(yCurLayer1).toarray()\n",
    "        print(enc.categories_,enc.get_feature_names())\n",
    "        print(yOneHot[:1])\n",
    "        \n",
    "        \n",
    "        num_labels = hierarchy[i] \n",
    "        print(\"num_labels:\", num_labels)\n",
    "        saveName = \"../trainedModes/modelSep-9level%d-%dlayer-2slots-gpu1.h5\" %(i,numLayers)\n",
    "        #saveName = \"../trainedModes/modelSep-2level%d-%dlayer-2slots-gpu1.h5\" %(i,numLayers)#基于拥堵定义的2层结构\n",
    "        print(saveName)\n",
    "        sepHier1(x_train,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs)\n",
    "        \n",
    "        encLevels[str(i)] = enc\n",
    "        xFloors[str(i)] = x_train\n",
    "        yFloors[str(i)] = yCurLayer1\n",
    "        \n",
    "        \n",
    "        nSamplesTest,nFeaturesT =  x_test.shape\n",
    "        yCurLayerTest = [t1[i] for t1 in y_test]\n",
    "        yCurLayerTest = np.array(yCurLayerTest)\n",
    "        yCurLayerTest= yCurLayerTest.reshape(nSamplesTest,-1)\n",
    "        \n",
    "        xTestFloors[str(i)] = x_test\n",
    "        yTestFloors[str(i)] = yCurLayerTest\n",
    "        modSaveNameFloors[str(i)] = saveName\n",
    "        \n",
    "    #######保存为pickle文件,用于后期的SUMO和数据分析\n",
    "\n",
    "    fpk=open('samples1.pkf','wb+')  \n",
    "    pickle.dump([xFloors,yFloors,modSaveNameFloors,encLevels,xTestFloors, yTestFloors],fpk)  \n",
    "    fpk.close() \n",
    "\n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "#####用现有训练模型进行预测\n",
    "\n",
    "fpk=open('samples1.pkf','rb')   \n",
    "[xFloors,yFloors,modSaveNameFloors,encLevels,xTestFloors, yTestFloors]=pickle.load(fpk)  \n",
    "fpk.close()  \n",
    "\n",
    "\n",
    "yKerasFloors = dict()\n",
    "\n",
    "for i in range(len(hierarchy)):\n",
    "        levelIndex = i\n",
    "        x = xFloors[str(i)]\n",
    "        yCurLayer1 =  yFloors[str(i)]\n",
    "        \n",
    "        x = xTestFloors[str(i)]\n",
    "        yCurLayer1 =  yTestFloors[str(i)]\n",
    "        \n",
    "        saveName =  modSaveNameFloors[str(i)] \n",
    "        enc = encLevels[str(i)]\n",
    "        yOneHot=enc.transform(yCurLayer1).toarray()\n",
    "        yPredict=getKerasResnetRVL(x,enc,saveName)\n",
    "        print(\"分离式多层识别结果:第%d层\\n\" %i)\n",
    "        mat1num = confusion_matrix(yCurLayer1,yPredict)\n",
    "        print(mat1num)\n",
    "        mat2acc = confusion_matrix(yCurLayer1,yPredict,normalize='pred')  \n",
    "        print(np.around(mat2acc , decimals=3))\n",
    "        yKerasFloors[str(i)] =  yPredict\n",
    "        \n",
    "        df = pd.DataFrame(np.around(mat2acc , decimals=3))\n",
    "        fs = \"test_mat2acc%d.csv\" %i\n",
    "        df.to_csv(fs,index= False, header= False)\n",
    "        \n",
    "fpk=open('samples2.pkf','wb+')  \n",
    "pickle.dump([xFloors,yFloors,modSaveNameFloors,encLevels,yKerasFloors,xTestFloors,yTestFloors],fpk)  \n",
    "fpk.close() \n",
    "\n",
    " \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8674b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T14:36:52.438549Z",
     "start_time": "2023-02-13T14:36:16.048928Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "########################################################################################################################\n",
    "print(\"1.接编号为0的主程序,先找出低概率样本，\")\n",
    "print(\"2.对较低概率的样本进行蒙特卡洛模拟分析，原始对应程序为mainSimSumoFranceDatra\")\n",
    "print(\"3.最终进行分析，程序编号为1\")\n",
    "print(\"3.最终进行分析，程序编号为1\")\n",
    "########################################################################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "#import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "      \n",
    "\n",
    "\n",
    "    \n",
    "########################################################################################################################\n",
    "print(\"1.1 主程序开始\")\n",
    "########################################################################################################################\n",
    "\n",
    "########################################################################################################################\n",
    "#####用现有训练模型进行预测\n",
    "\n",
    "fpk=open('samples2.pkf','rb')   \n",
    "[xFloors,yFloors,modSaveNameFloors,encLevels,yKerasFloors,xTestFloors,yTestFloors]=pickle.load(fpk)  \n",
    "fpk.close()  \n",
    "\n",
    "hierarchy=[2,3,4,5,6,7,8,9]\n",
    "for i in [7]:\n",
    "#for i in range(len(hierarchy)):\n",
    "        levelIndex = i \n",
    "        x = xTestFloors[str(i)]\n",
    "        yCurLayer1 =  yTestFloors[str(i)]\n",
    "        \n",
    "        modeSaveName = \"../trainedModes/modelSep-9level7-4layer-2slots-gpu1.h5\"\n",
    "        model = keras.models.load_model(modeSaveName)\n",
    "        yPredictOut= model.predict([x], batch_size=2560)\n",
    "        yPredictOut = np.around(yPredictOut , decimals=3)\n",
    "        #print(yPredictOut)\n",
    "        ymax1=np.max(yPredictOut,axis=1)\n",
    "        ymax2=np.argmax(yPredictOut,axis=1)\n",
    "        \n",
    "        index = np.where(ymax1<0.8)[0]#提取最大值小于0.95的例子\n",
    "        \n",
    "        ylowpraPredictNN=yPredictOut[index]#对较低概率的样本\n",
    "        xlowpra=x[index]\n",
    "        ylowpraLabel = yCurLayer1[index]\n",
    "        print(\"xlowpra.shape\",xlowpra.shape)\n",
    "        \n",
    "        \n",
    "        fpk=open('lowprobSamples.pkf','wb+')  \n",
    "        pickle.dump([xlowpra,ylowpraLabel,ylowPredictLabel,ylowpraPredictNN],fpk)  \n",
    "        fpk.close() \n",
    "        \n",
    "        df = pd.DataFrame(xlowpra)\n",
    "        fs = \"lowprobSamplesX.csv\"\n",
    "        df.to_csv(fs,index= False, header= False)\n",
    "       \n",
    "        ylowPredictLabel = ymax2[index].reshape(-1,1)\n",
    "        \n",
    "        df = pd.DataFrame(np.concatenate([ylowpraLabel,ylowPredictLabel,ylowpraPredictNN],axis=1))\n",
    "        fs = \"lowprobSamplesY.csv\"\n",
    "        df.to_csv(fs,index= False, header=['ylowpraLabel','ylowPredictLabel','0','1','2','3','4','5','6','7','8'])\n",
    "        \n",
    "        print(\"levelIndex:\",i,\"numLabels:\", ,\"xlowpra.shape\",xlowpra.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c074866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T09:51:42.931165Z",
     "start_time": "2023-01-28T09:51:42.771943Z"
    }
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "print(\"接程序1: 对较低概率的样本进行蒙特卡洛模拟分析,原始对应程序为mainSimSumoFranceDatra\")\n",
    "print(\"因为配置失误，采用将低概率的样本进行保存为文件，然后再root用户下命令行模式用SUMO模拟（不使用conda）\")\n",
    "print(\"输出为sumoSimData？？？.csv,里面有每个样本的sumo输出，kerasNN输出以及原始的输入输出\")\n",
    "print(\"程序编号为2\")\n",
    "print(\"程序编号为2\")\n",
    "########################################################################################################################\n",
    "!python3 runSumoSimFun.py#运行runSumoSimFun.py 中test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "becda4ae-fe95-4636-9b74-0f775aec3d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "接程序2: 综合SUMO输出，对keras输出进行优化。程序输入为程序2的输出\n",
      "优化选择1.NN。 2 回归分析。3 概率分析。\n",
      "程序编号为3\n",
      "##############################################################################################################\n",
      "程序编号为3.1，主程序开始运行\n",
      "(1928, 16)\n",
      "Index(['sampleIndex', 'outputAvgSpeed', 'originOutput', 'sumoOutputSpeedTag',\n",
      "       'kerasPredictLabel', 'smv1', 'smv2', 'NN0', 'NN1', 'NN2', 'NN3', 'NN4',\n",
      "       'NN5', 'NN6', 'NN7', 'NN8'],\n",
      "      dtype='object')\n",
      "(1928, 11)\n",
      "(1928, 1)\n",
      "[[1]\n",
      " [3]\n",
      " [3]\n",
      " ...\n",
      " [2]\n",
      " [3]\n",
      " [1]]\n",
      "#############################\n",
      "数据预处理\n",
      "\n",
      "#############################\n",
      "原生keras\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.78      0.65       767\n",
      "           1       0.88      0.09      0.16       257\n",
      "           2       0.65      0.12      0.20       249\n",
      "           3       0.40      0.83      0.54       309\n",
      "           4       0.75      0.19      0.30       253\n",
      "           5       1.00      1.00      1.00        31\n",
      "           6       1.00      1.00      1.00        25\n",
      "           7       1.00      1.00      1.00        18\n",
      "           8       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.54      1928\n",
      "   macro avg       0.81      0.67      0.65      1928\n",
      "weighted avg       0.64      0.54      0.48      1928\n",
      "\n",
      "[[443  66  68 105  83   2   0   0   0]\n",
      " [  2 136   0  76  43   0   0   0   0]\n",
      " [ 74  10  88   7  62   5   0   0   3]\n",
      " [ 27  22   0 210  50   0   0   0   0]\n",
      " [  0  20   0   0 233   0   0   0   0]\n",
      " [  0   0   0   0   0  19   0  12   0]\n",
      " [  0   0   0   0   0   0  25   0   0]\n",
      " [  0   0   0   0   0   0   0  18   0]\n",
      " [  0   0   0   0   0   0   0   0  19]]\n",
      "[[0.811 0.26  0.436 0.264 0.176 0.077 0.    0.    0.   ]\n",
      " [0.004 0.535 0.    0.191 0.091 0.    0.    0.    0.   ]\n",
      " [0.136 0.039 0.564 0.018 0.132 0.192 0.    0.    0.136]\n",
      " [0.049 0.087 0.    0.528 0.106 0.    0.    0.    0.   ]\n",
      " [0.    0.079 0.    0.    0.495 0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.731 0.    0.4   0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.6   0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.864]]\n",
      "#############################\n",
      "决策树\n",
      "\n",
      "纯决策树的识别\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.78      0.74       767\n",
      "           1       0.88      0.37      0.52       257\n",
      "           2       0.40      0.41      0.41       249\n",
      "           3       0.62      0.52      0.57       309\n",
      "           4       0.52      0.94      0.67       253\n",
      "           5       0.00      0.00      0.00        31\n",
      "           6       0.00      0.00      0.00        25\n",
      "           7       0.00      0.00      0.00        18\n",
      "           8       0.00      0.00      0.00        19\n",
      "\n",
      "    accuracy                           0.62      1928\n",
      "   macro avg       0.35      0.34      0.32      1928\n",
      "weighted avg       0.62      0.62      0.60      1928\n",
      "\n",
      "[[599  10  61  34  63   0   0   0   0]\n",
      " [ 76  94   2  57  28   0   0   0   0]\n",
      " [ 70   3 103   7  66   0   0   0   0]\n",
      " [ 86   0   0 161  62   0   0   0   0]\n",
      " [ 16   0   0   0 237   0   0   0   0]\n",
      " [  0   0  31   0   0   0   0   0   0]\n",
      " [  0   0  25   0   0   0   0   0   0]\n",
      " [  0   0  18   0   0   0   0   0   0]\n",
      " [  0   0  19   0   0   0   0   0   0]]\n",
      "[[0.707 0.093 0.236 0.131 0.138 0.    0.    0.    0.   ]\n",
      " [0.09  0.879 0.008 0.22  0.061 0.    0.    0.    0.   ]\n",
      " [0.083 0.028 0.398 0.027 0.145 0.    0.    0.    0.   ]\n",
      " [0.102 0.    0.    0.622 0.136 0.    0.    0.    0.   ]\n",
      " [0.019 0.    0.    0.    0.52  0.    0.    0.    0.   ]\n",
      " [0.    0.    0.12  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.097 0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.069 0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.073 0.    0.    0.    0.    0.    0.   ]]\n",
      "#############################\n",
      "逻辑回归\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.76      0.72       767\n",
      "           1       0.81      0.37      0.51       257\n",
      "           2       0.57      0.39      0.46       249\n",
      "           3       0.56      0.57      0.57       309\n",
      "           4       0.57      0.82      0.67       253\n",
      "           5       0.89      0.77      0.83        31\n",
      "           6       1.00      1.00      1.00        25\n",
      "           7       0.68      0.83      0.75        18\n",
      "           8       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.65      1928\n",
      "   macro avg       0.75      0.72      0.72      1928\n",
      "weighted avg       0.66      0.65      0.63      1928\n",
      "\n",
      "[[585  15  53  63  51   0   0   0   0]\n",
      " [ 70  96   0  70  21   0   0   0   0]\n",
      " [ 99   3  96   7  44   0   0   0   0]\n",
      " [ 79   5   4 177  44   0   0   0   0]\n",
      " [ 29   0  16   0 208   0   0   0   0]\n",
      " [  0   0   0   0   0  24   0   7   0]\n",
      " [  0   0   0   0   0   0  25   0   0]\n",
      " [  0   0   0   0   0   3   0  15   0]\n",
      " [  0   0   0   0   0   0   0   0  19]]\n",
      "[[0.679 0.126 0.314 0.199 0.139 0.    0.    0.    0.   ]\n",
      " [0.081 0.807 0.    0.221 0.057 0.    0.    0.    0.   ]\n",
      " [0.115 0.025 0.568 0.022 0.12  0.    0.    0.    0.   ]\n",
      " [0.092 0.042 0.024 0.558 0.12  0.    0.    0.    0.   ]\n",
      " [0.034 0.    0.095 0.    0.565 0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.889 0.    0.318 0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.111 0.    0.682 0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    1.   ]]\n",
      "#############################\n",
      "贝叶斯高斯回归\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.78      0.65       767\n",
      "           1       0.88      0.09      0.16       257\n",
      "           2       0.65      0.12      0.20       249\n",
      "           3       0.40      0.83      0.54       309\n",
      "           4       0.75      0.19      0.30       253\n",
      "           5       1.00      1.00      1.00        31\n",
      "           6       1.00      1.00      1.00        25\n",
      "           7       1.00      1.00      1.00        18\n",
      "           8       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.54      1928\n",
      "   macro avg       0.81      0.67      0.65      1928\n",
      "weighted avg       0.64      0.54      0.48      1928\n",
      "\n",
      "[[596   0   8 160   3   0   0   0   0]\n",
      " [108  23   0 126   0   0   0   0   0]\n",
      " [202   3  30   1  13   0   0   0   0]\n",
      " [ 53   0   0 256   0   0   0   0   0]\n",
      " [103   0   8  94  48   0   0   0   0]\n",
      " [  0   0   0   0   0  31   0   0   0]\n",
      " [  0   0   0   0   0   0  25   0   0]\n",
      " [  0   0   0   0   0   0   0  18   0]\n",
      " [  0   0   0   0   0   0   0   0  19]]\n",
      "[[0.561 0.    0.174 0.251 0.047 0.    0.    0.    0.   ]\n",
      " [0.102 0.885 0.    0.198 0.    0.    0.    0.    0.   ]\n",
      " [0.19  0.115 0.652 0.002 0.203 0.    0.    0.    0.   ]\n",
      " [0.05  0.    0.    0.402 0.    0.    0.    0.    0.   ]\n",
      " [0.097 0.    0.174 0.148 0.75  0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    1.   ]]\n",
      "#############################\n",
      "kerasNN\n",
      "\n",
      "(1928, 11)\n",
      "(1928, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step - loss: 2.2291 - accuracy: 0.0866\n",
      "Epoch 1/500\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.9773 - accuracy: 0.4704\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.7633 - accuracy: 0.5908\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.5735 - accuracy: 0.6131\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.4046 - accuracy: 0.6219\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.2568 - accuracy: 0.6291\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1326 - accuracy: 0.6338\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0412 - accuracy: 0.6354\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.9739 - accuracy: 0.6395\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.9252 - accuracy: 0.6271\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8953 - accuracy: 0.6312\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8769 - accuracy: 0.6349\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8623 - accuracy: 0.6266\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8512 - accuracy: 0.6307\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8347 - accuracy: 0.6369\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8295 - accuracy: 0.6359\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8174 - accuracy: 0.6369\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8129 - accuracy: 0.6343\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8181 - accuracy: 0.6291\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7996 - accuracy: 0.6328\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7970 - accuracy: 0.6390\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7911 - accuracy: 0.6359\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7869 - accuracy: 0.6432\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7930 - accuracy: 0.6468\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7823 - accuracy: 0.6473\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7786 - accuracy: 0.6395\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7769 - accuracy: 0.6364\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7663 - accuracy: 0.6442\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7668 - accuracy: 0.6515\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7622 - accuracy: 0.6515\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7673 - accuracy: 0.6437\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7565 - accuracy: 0.6504\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7589 - accuracy: 0.6504\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7598 - accuracy: 0.6483\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7482 - accuracy: 0.6572\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7493 - accuracy: 0.6546\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7445 - accuracy: 0.6655\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7477 - accuracy: 0.6587\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7494 - accuracy: 0.6452\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7425 - accuracy: 0.6566\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7403 - accuracy: 0.6561\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7404 - accuracy: 0.6608\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7283 - accuracy: 0.6649\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7261 - accuracy: 0.6618\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7293 - accuracy: 0.6634\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7335 - accuracy: 0.6644\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7253 - accuracy: 0.6608\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7261 - accuracy: 0.6665\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7226 - accuracy: 0.6566\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7253 - accuracy: 0.6494\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7223 - accuracy: 0.6623\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7224 - accuracy: 0.6629\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7237 - accuracy: 0.6483\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7103 - accuracy: 0.6649\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7138 - accuracy: 0.6701\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7163 - accuracy: 0.6566\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7112 - accuracy: 0.6686\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7148 - accuracy: 0.6634\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7157 - accuracy: 0.6598\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7179 - accuracy: 0.6577\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7070 - accuracy: 0.6644\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7068 - accuracy: 0.6660\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7013 - accuracy: 0.6738\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6989 - accuracy: 0.6722\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6964 - accuracy: 0.6717\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7015 - accuracy: 0.6675\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6973 - accuracy: 0.6779\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6996 - accuracy: 0.6613\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6996 - accuracy: 0.6821\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6944 - accuracy: 0.6644\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.6826\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.6789\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6893 - accuracy: 0.6805\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6822 - accuracy: 0.6779\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6862 - accuracy: 0.6836\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6859 - accuracy: 0.6789\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6797 - accuracy: 0.6769\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6953 - accuracy: 0.6753\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6860 - accuracy: 0.6800\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6852 - accuracy: 0.6836\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6810 - accuracy: 0.6738\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6862 - accuracy: 0.6826\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6828 - accuracy: 0.6815\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6780 - accuracy: 0.6758\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6813 - accuracy: 0.6779\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6776 - accuracy: 0.6878\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6792 - accuracy: 0.6795\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6785 - accuracy: 0.6769\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6746 - accuracy: 0.6831\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6774 - accuracy: 0.6717\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6751 - accuracy: 0.6712\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6727 - accuracy: 0.6800\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6810 - accuracy: 0.6831\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6724 - accuracy: 0.6810\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6769 - accuracy: 0.6763\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6699 - accuracy: 0.6945\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6692 - accuracy: 0.6779\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6739 - accuracy: 0.6836\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6665 - accuracy: 0.6898\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6673 - accuracy: 0.6826\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6592 - accuracy: 0.6893\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6659 - accuracy: 0.6929\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6737 - accuracy: 0.6862\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6564 - accuracy: 0.6878\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6629 - accuracy: 0.6789\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6586 - accuracy: 0.6883\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6585 - accuracy: 0.6935\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6622 - accuracy: 0.6789\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6592 - accuracy: 0.6878\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6575 - accuracy: 0.6878\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6627 - accuracy: 0.6831\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6487 - accuracy: 0.6852\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6561 - accuracy: 0.6929\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6566 - accuracy: 0.6919\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6474 - accuracy: 0.6966\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6531 - accuracy: 0.6924\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6578 - accuracy: 0.6784\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6539 - accuracy: 0.6821\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6535 - accuracy: 0.6935\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6517 - accuracy: 0.6888\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6541 - accuracy: 0.6914\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6507 - accuracy: 0.6950\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6512 - accuracy: 0.6883\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6578 - accuracy: 0.6867\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6477 - accuracy: 0.6862\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6471 - accuracy: 0.6914\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6496 - accuracy: 0.6966\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6493 - accuracy: 0.6935\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6506 - accuracy: 0.6878\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6451 - accuracy: 0.6909\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6425 - accuracy: 0.6966\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6390 - accuracy: 0.7018\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6470 - accuracy: 0.6924\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6502 - accuracy: 0.6893\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6424 - accuracy: 0.6883\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6410 - accuracy: 0.6888\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6421 - accuracy: 0.6955\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6409 - accuracy: 0.7002\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6406 - accuracy: 0.6888\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6431 - accuracy: 0.7007\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6396 - accuracy: 0.6904\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6398 - accuracy: 0.6950\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6386 - accuracy: 0.6992\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6364 - accuracy: 0.7012\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6364 - accuracy: 0.6955\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6379 - accuracy: 0.7012\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.6857\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6292 - accuracy: 0.7023\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6334 - accuracy: 0.6981\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6310 - accuracy: 0.6997\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6298 - accuracy: 0.6987\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6399 - accuracy: 0.6935\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6256 - accuracy: 0.7018\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6297 - accuracy: 0.6987\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6226 - accuracy: 0.7127\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6308 - accuracy: 0.6981\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6351 - accuracy: 0.6940\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6328 - accuracy: 0.7049\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6316 - accuracy: 0.7033\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6303 - accuracy: 0.7033\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6357 - accuracy: 0.6961\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6369 - accuracy: 0.6862\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6355 - accuracy: 0.6945\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6323 - accuracy: 0.6971\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6339 - accuracy: 0.7007\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6229 - accuracy: 0.7012\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6233 - accuracy: 0.6981\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6317 - accuracy: 0.6945\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6304 - accuracy: 0.6935\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6307 - accuracy: 0.6971\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6279 - accuracy: 0.6987\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6222 - accuracy: 0.7054\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6298 - accuracy: 0.6914\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6246 - accuracy: 0.7064\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6258 - accuracy: 0.7116\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6186 - accuracy: 0.7023\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6199 - accuracy: 0.7038\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6215 - accuracy: 0.7049\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6190 - accuracy: 0.7023\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6290 - accuracy: 0.6945\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6175 - accuracy: 0.7111\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6233 - accuracy: 0.6987\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6197 - accuracy: 0.7033\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6184 - accuracy: 0.7007\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6231 - accuracy: 0.7028\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6227 - accuracy: 0.7033\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6119 - accuracy: 0.7054\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6198 - accuracy: 0.7121\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6187 - accuracy: 0.7028\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6239 - accuracy: 0.6987\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6228 - accuracy: 0.6971\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6173 - accuracy: 0.6997\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6194 - accuracy: 0.6971\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6189 - accuracy: 0.7007\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6182 - accuracy: 0.6961\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6155 - accuracy: 0.7023\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6218 - accuracy: 0.7116\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6184 - accuracy: 0.7085\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6140 - accuracy: 0.7085\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6135 - accuracy: 0.7101\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6056 - accuracy: 0.7095\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6147 - accuracy: 0.7054\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6174 - accuracy: 0.7142\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6200 - accuracy: 0.6966\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6129 - accuracy: 0.7054\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6132 - accuracy: 0.7054\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6060 - accuracy: 0.7116\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6157 - accuracy: 0.6981\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6099 - accuracy: 0.7121\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6131 - accuracy: 0.7064\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6087 - accuracy: 0.7095\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6068 - accuracy: 0.6997\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6099 - accuracy: 0.7090\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6067 - accuracy: 0.7018\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6062 - accuracy: 0.6966\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6068 - accuracy: 0.7121\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6090 - accuracy: 0.7127\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6096 - accuracy: 0.6997\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6105 - accuracy: 0.7023\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6121 - accuracy: 0.7116\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6069 - accuracy: 0.7054\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6061 - accuracy: 0.7095\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6060 - accuracy: 0.7142\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6063 - accuracy: 0.7075\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6130 - accuracy: 0.7085\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6017 - accuracy: 0.7121\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6072 - accuracy: 0.7132\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6088 - accuracy: 0.6992\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6083 - accuracy: 0.7095\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6034 - accuracy: 0.7210\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6079 - accuracy: 0.7173\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6061 - accuracy: 0.7075\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5956 - accuracy: 0.7184\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6008 - accuracy: 0.7085\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6074 - accuracy: 0.7127\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6014 - accuracy: 0.7111\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6002 - accuracy: 0.7132\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6076 - accuracy: 0.7121\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6040 - accuracy: 0.7085\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6008 - accuracy: 0.7106\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6046 - accuracy: 0.7075\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6013 - accuracy: 0.7116\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6043 - accuracy: 0.7085\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6042 - accuracy: 0.7101\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5950 - accuracy: 0.7163\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6002 - accuracy: 0.7121\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6023 - accuracy: 0.7158\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6027 - accuracy: 0.7184\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6031 - accuracy: 0.7121\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5992 - accuracy: 0.7178\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6030 - accuracy: 0.7168\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5999 - accuracy: 0.7173\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5990 - accuracy: 0.7111\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6050 - accuracy: 0.7012\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5988 - accuracy: 0.7127\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6059 - accuracy: 0.7246\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5968 - accuracy: 0.7127\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5962 - accuracy: 0.7168\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6045 - accuracy: 0.7054\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6024 - accuracy: 0.7085\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5983 - accuracy: 0.7163\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5976 - accuracy: 0.7147\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5975 - accuracy: 0.7152\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5983 - accuracy: 0.7002\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6016 - accuracy: 0.7147\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5959 - accuracy: 0.7127\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5971 - accuracy: 0.7189\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5991 - accuracy: 0.7199\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5973 - accuracy: 0.7215\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5978 - accuracy: 0.7147\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5923 - accuracy: 0.7204\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6026 - accuracy: 0.7085\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5907 - accuracy: 0.7215\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5888 - accuracy: 0.7189\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5913 - accuracy: 0.7137\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5977 - accuracy: 0.7054\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5893 - accuracy: 0.7189\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6027 - accuracy: 0.7111\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5937 - accuracy: 0.7152\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5968 - accuracy: 0.7127\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5944 - accuracy: 0.7204\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5838 - accuracy: 0.7199\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5944 - accuracy: 0.7173\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5934 - accuracy: 0.7147\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5912 - accuracy: 0.7199\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5912 - accuracy: 0.7101\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5892 - accuracy: 0.7230\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5949 - accuracy: 0.7173\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5888 - accuracy: 0.7111\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5970 - accuracy: 0.7230\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5888 - accuracy: 0.7178\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5895 - accuracy: 0.7132\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5961 - accuracy: 0.7194\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5876 - accuracy: 0.7184\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5874 - accuracy: 0.7225\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5915 - accuracy: 0.7189\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5924 - accuracy: 0.7137\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5845 - accuracy: 0.7189\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5926 - accuracy: 0.7132\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5912 - accuracy: 0.7147\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5960 - accuracy: 0.7189\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5878 - accuracy: 0.7173\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5901 - accuracy: 0.7168\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5907 - accuracy: 0.7168\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5894 - accuracy: 0.7147\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5890 - accuracy: 0.7168\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5937 - accuracy: 0.7168\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5881 - accuracy: 0.7215\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5916 - accuracy: 0.7215\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5880 - accuracy: 0.7199\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5859 - accuracy: 0.7230\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5907 - accuracy: 0.7261\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5842 - accuracy: 0.7303\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5896 - accuracy: 0.7142\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5785 - accuracy: 0.7303\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5845 - accuracy: 0.7194\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5886 - accuracy: 0.7127\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5920 - accuracy: 0.7178\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5845 - accuracy: 0.7282\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5834 - accuracy: 0.7204\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5804 - accuracy: 0.7204\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5895 - accuracy: 0.7241\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5854 - accuracy: 0.7163\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5903 - accuracy: 0.7116\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5873 - accuracy: 0.7189\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5826 - accuracy: 0.7235\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5832 - accuracy: 0.7334\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5863 - accuracy: 0.7194\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5853 - accuracy: 0.7376\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5879 - accuracy: 0.7101\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5824 - accuracy: 0.7215\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5825 - accuracy: 0.7199\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5790 - accuracy: 0.7324\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5831 - accuracy: 0.7173\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5892 - accuracy: 0.7095\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5879 - accuracy: 0.7168\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5798 - accuracy: 0.7184\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5845 - accuracy: 0.7267\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5847 - accuracy: 0.7199\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5821 - accuracy: 0.7142\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5910 - accuracy: 0.7152\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5796 - accuracy: 0.7277\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5797 - accuracy: 0.7256\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5764 - accuracy: 0.7215\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5824 - accuracy: 0.7220\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5836 - accuracy: 0.7267\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5789 - accuracy: 0.7210\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5816 - accuracy: 0.7287\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5843 - accuracy: 0.7235\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5830 - accuracy: 0.7261\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5792 - accuracy: 0.7163\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5879 - accuracy: 0.7127\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5848 - accuracy: 0.7168\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5848 - accuracy: 0.7210\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5864 - accuracy: 0.7293\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5816 - accuracy: 0.7127\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5809 - accuracy: 0.7215\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5851 - accuracy: 0.7194\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5746 - accuracy: 0.7308\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5739 - accuracy: 0.7235\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5766 - accuracy: 0.7246\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5840 - accuracy: 0.7163\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5763 - accuracy: 0.7344\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5730 - accuracy: 0.7313\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5769 - accuracy: 0.7225\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5745 - accuracy: 0.7303\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5812 - accuracy: 0.7261\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5736 - accuracy: 0.7256\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5781 - accuracy: 0.7251\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5753 - accuracy: 0.7256\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5773 - accuracy: 0.7251\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5765 - accuracy: 0.7339\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5800 - accuracy: 0.7220\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5799 - accuracy: 0.7168\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5756 - accuracy: 0.7293\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5811 - accuracy: 0.7225\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5735 - accuracy: 0.7246\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5715 - accuracy: 0.7267\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5857 - accuracy: 0.7173\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5791 - accuracy: 0.7225\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5793 - accuracy: 0.7147\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5799 - accuracy: 0.7210\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5738 - accuracy: 0.7220\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5697 - accuracy: 0.7350\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5718 - accuracy: 0.7318\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5756 - accuracy: 0.7235\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5793 - accuracy: 0.7303\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5675 - accuracy: 0.7267\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5692 - accuracy: 0.7303\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5765 - accuracy: 0.7282\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5805 - accuracy: 0.7168\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5753 - accuracy: 0.7277\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5767 - accuracy: 0.7277\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5751 - accuracy: 0.7173\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5790 - accuracy: 0.7235\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5818 - accuracy: 0.7272\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5737 - accuracy: 0.7194\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5741 - accuracy: 0.7241\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5698 - accuracy: 0.7261\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5766 - accuracy: 0.7210\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5787 - accuracy: 0.7168\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5728 - accuracy: 0.7256\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5760 - accuracy: 0.7287\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5777 - accuracy: 0.7339\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5735 - accuracy: 0.7272\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5689 - accuracy: 0.7303\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5721 - accuracy: 0.7267\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5749 - accuracy: 0.7256\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5739 - accuracy: 0.7168\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5733 - accuracy: 0.7261\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5747 - accuracy: 0.7365\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5747 - accuracy: 0.7189\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5799 - accuracy: 0.7127\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5743 - accuracy: 0.7215\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5663 - accuracy: 0.7334\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5691 - accuracy: 0.7298\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5778 - accuracy: 0.7199\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5772 - accuracy: 0.7189\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5685 - accuracy: 0.7308\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5752 - accuracy: 0.7272\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5652 - accuracy: 0.7277\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5759 - accuracy: 0.7163\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5676 - accuracy: 0.7220\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5718 - accuracy: 0.7235\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5696 - accuracy: 0.7241\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5711 - accuracy: 0.7308\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5625 - accuracy: 0.7334\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5709 - accuracy: 0.7282\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5757 - accuracy: 0.7277\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5776 - accuracy: 0.7318\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5749 - accuracy: 0.7261\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5702 - accuracy: 0.7287\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5714 - accuracy: 0.7152\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5751 - accuracy: 0.7303\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5713 - accuracy: 0.7241\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5705 - accuracy: 0.7287\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5690 - accuracy: 0.7215\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5717 - accuracy: 0.7230\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5750 - accuracy: 0.7199\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5629 - accuracy: 0.7376\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5652 - accuracy: 0.7334\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5703 - accuracy: 0.7355\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5693 - accuracy: 0.7324\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5644 - accuracy: 0.7329\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5674 - accuracy: 0.7277\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5658 - accuracy: 0.7287\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5671 - accuracy: 0.7199\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5659 - accuracy: 0.7277\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5657 - accuracy: 0.7339\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5644 - accuracy: 0.7303\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5681 - accuracy: 0.7329\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5639 - accuracy: 0.7293\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5671 - accuracy: 0.7251\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5595 - accuracy: 0.7350\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5673 - accuracy: 0.7386\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5671 - accuracy: 0.7241\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5705 - accuracy: 0.7324\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5659 - accuracy: 0.7256\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5684 - accuracy: 0.7282\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5680 - accuracy: 0.7287\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5659 - accuracy: 0.7277\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5655 - accuracy: 0.7251\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5684 - accuracy: 0.7324\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5633 - accuracy: 0.7277\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5625 - accuracy: 0.7235\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5663 - accuracy: 0.7324\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5616 - accuracy: 0.7324\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5632 - accuracy: 0.7293\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5659 - accuracy: 0.7329\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5587 - accuracy: 0.7318\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5605 - accuracy: 0.7360\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5672 - accuracy: 0.7241\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5630 - accuracy: 0.7318\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5677 - accuracy: 0.7261\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5619 - accuracy: 0.7272\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5625 - accuracy: 0.7282\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5585 - accuracy: 0.7334\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5647 - accuracy: 0.7256\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5651 - accuracy: 0.7282\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5579 - accuracy: 0.7438\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5636 - accuracy: 0.7381\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5728 - accuracy: 0.7308\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5625 - accuracy: 0.7324\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5664 - accuracy: 0.7256\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5543 - accuracy: 0.7453\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5686 - accuracy: 0.7256\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5634 - accuracy: 0.7401\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5653 - accuracy: 0.7287\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5640 - accuracy: 0.7235\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5590 - accuracy: 0.7256\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5574 - accuracy: 0.7360\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5676 - accuracy: 0.7199\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5644 - accuracy: 0.7360\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5671 - accuracy: 0.7199\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5613 - accuracy: 0.7318\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5565 - accuracy: 0.7370\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5629 - accuracy: 0.7355\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5634 - accuracy: 0.7318\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5650 - accuracy: 0.7261\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5606 - accuracy: 0.7376\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5608 - accuracy: 0.7407\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "print(\"接程序2: 综合SUMO输出，对keras输出进行优化。程序输入为程序2的输出\")\n",
    "print(\"优化选择1.NN。 2 回归分析。3 概率分析。\")\n",
    "print(\"程序编号为3\")\n",
    "########################################################################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import pickle \n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import naive_bayes\n",
    "############################################################################\n",
    "############################################################################\n",
    "####建立NN模型，与sepHier2一样\n",
    "def sepHier2(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 10,srelu_size = 256,dropout_rate = 0.05):\n",
    "    \n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    featuresInput = layers.Input(shape=(features_size,))\n",
    "    features = layers.BatchNormalization()(featuresInput)\n",
    "    #features=featuresInput\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[featuresInput], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    if 0:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        #build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=20000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "\n",
    "def dtFit(x,y):\n",
    "    str1=\"dtFitAndSave,用于决策树拟合和识别\"\n",
    "    \n",
    "    dt = tree.DecisionTreeClassifier(max_depth=7,min_samples_leaf=100)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    #data=tree.export_graphviz(dt, out_file=None,class_names=None,filled=True) \n",
    "    #graph = graphviz.Source(data)\n",
    "    #graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(\"纯决策树的识别\\n\",tmp1)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    #text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    return dt,yPredict\n",
    "########################################################################################################################   \n",
    "########################################################################################################################    \n",
    "print(\"##############################################################################################################\")\n",
    "\n",
    "print(\"程序编号为3.1，主程序开始运行\")\n",
    "\n",
    "   \n",
    "\n",
    "df = pd.read_csv('sumoSimData1999.csv', sep=',')\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "numSamples,numFeatures = df.shape\n",
    "\n",
    "##['sampleIndex','outputAvgSpeed','originOutput','sumoOutputSpeedTag','kerasPredictLabel','smv1','smv2',\\\n",
    " ##                                              'NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8'])\n",
    "sumoOutput='sumoOutputSpeedTag'\n",
    "yKerasOutput='kerasPredictLabel'\n",
    "originOutput ='originOutput'\n",
    "sumoOutList = ['smv1','smv2']\n",
    "outputListNN = ['NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8']\n",
    "\n",
    "df1 = df[sumoOutput]\n",
    "x1 = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "df1 = df[yKerasOutput]\n",
    "x2 = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "df1 = df[outputListNN]\n",
    "x3 = df1.iloc[0:numSamples]\n",
    "\n",
    "\n",
    "df1 = df[originOutput]\n",
    "y = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y)\n",
    "yOneHot=enc.transform(y).toarray()\n",
    "\n",
    "x = np.concatenate([x1,x2,x3],axis=1)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"#############################\\n数据预处理\\n\")\n",
    "#数据预处理\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, yOneHot, test_size=0.5, random_state=0)\n",
    "   \n",
    "\n",
    "rN,cN= np.where(np.isnan(x))\n",
    "#print(rN,cN)\n",
    "#print(rN.shape)\n",
    "\n",
    "for i in range(rN.shape[0]):\n",
    "    x[rN[i],cN[i]] = 0\n",
    " \n",
    "\n",
    "print(\"#############################\\n原生keras\\n\")\n",
    "tmp1 = classification_report(y,yPredict)\n",
    "yPredict = x2\n",
    "mat1num = confusion_matrix(y,yPredict)\n",
    "mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "    \n",
    "print(\"#############################\\n决策树\\n\")\n",
    "dtFit(x,y)\n",
    "   \n",
    "print(\"#############################\\n逻辑回归\\n\")    \n",
    "model = LogisticRegression()\n",
    "model.fit(x,y)\n",
    "yPredict = model.predict(x)\n",
    "\n",
    "tmp1 = classification_report(y,yPredict)\n",
    "mat1num = confusion_matrix(y,yPredict)\n",
    "mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "\n",
    "print(\"#############################\\n贝叶斯高斯回归\\n\")\n",
    "nb_cls = naive_bayes.GaussianNB().fit(x,y)\n",
    "yPredict = nb_cls.predict(x) \n",
    "\n",
    "tmp1 = classification_report(y,yPredict)\n",
    "mat1num = confusion_matrix(y,yPredict)\n",
    "mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "\n",
    "print(\"#############################\\nkerasNN\\n\")\n",
    "print(x.shape)\n",
    "print(yOneHot.shape)\n",
    "num_labels = yOneHot.shape[1]\n",
    "numLayers = 4\n",
    "numEpochs = 500\n",
    "saveName =\"../trainedModes/stage2_1.h5\";\n",
    "levelIndex = 7\n",
    "\n",
    "sepHier2(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fpk=open('samples2-stage2.pkf','wb')  \n",
    "pickle.dump([df,x,y,yOneHot,x_train, x_test, y_train, y_test,enc,saveName],fpk)  \n",
    "fpk.close() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add02d9c-af03-4a00-8629-42b8a751033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "print(\"辅助程序 对模拟后的数据进行分析，计算正确率\")\n",
    "########################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"对于低概率样本的识别结果，采用keras和MCS的识别结果对比\")\n",
    "file1 = \"./data-Copy1.csv\"\n",
    "xyDataTmp = pd.read_csv(file1,index_col=0)\n",
    "\n",
    "print(xyDataTmp.head(3))\n",
    "print(xyDataTmp.info())\n",
    "\n",
    "file1 = \"。./trainData/france_0_allSamples1.csv\"\n",
    "xyOrigin = pd.read_csv(file1,index_col=0)\n",
    "\n",
    "originlabel =  xyDataTmp.iloc[:,1].to_numpy()\n",
    "keraslabel =   xyDataTmp.iloc[:,2].to_numpy()      \n",
    "mcslabel =     xyDataTmp.iloc[:,3].to_numpy()\n",
    "\n",
    "#\n",
    "    \n",
    "print('\\norigin_mcs')\n",
    "mat1num = confusion_matrix(originlabel, mcslabel)\n",
    "mat2acc = confusion_matrix(originlabel, mcslabel,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "      \n",
    "print('\\nmcs_keras')\n",
    "mat1num = confusion_matrix(mcslabel, keraslabel)\n",
    "mat2acc = confusion_matrix(mcslabel, keraslabel,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "\n",
    "print('\\norgin_keras')\n",
    "mat1num = confusion_matrix(originlabel, keraslabel)\n",
    "mat2acc = confusion_matrix(originlabel ,keraslabel,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))      \n",
    "\n",
    "##用于分析实际标记类别大于预测标记类别\n",
    "def analyzing1(tmp, xyDataTmp,xyOrigin): \n",
    "    dfTmp1 = xyDataTmp[tmp]\n",
    "    #print(dfTmp1.head(5))\n",
    "    \n",
    "    \n",
    "    \n",
    "    df2 =  xyOrigin.iloc[dfTmp1.originIndex,:]   \n",
    "    plt.show()\n",
    "    df2[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    #print(df2.info())\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    \n",
    "    df2.to_csv(\"tmpForAnalyzing.csv\")\n",
    "    \n",
    "    tmp1 = df2['redLightTime'] - df2['arriveTime2'] >1.5 #红灯时间大于到达时间\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 = df2[df2['redLightTime'] - df2['arriveTime2'] >1.5] #红灯时间大于到达时间 ,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing3.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    \n",
    "   \n",
    "    tmp1 = df2['speed'] < 5/3.6 #本身速度就小于5/3.6\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df2['speed'] > 5/3.6 #本身速度就小于5/3.6,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    df3.to_csv(\"tmpForAnalyzing4.csv\")\n",
    "    \n",
    "    tmp1 = df2['redLightTime'] - df2['arriveTime2'] >1.5  #红灯时间大于到达时间\n",
    "    tmp1 = tmp1 | (df2['speed'] < 5/3.6) #本身速度就小于5/3.6\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"红灯时间大于到达时间  or 本身速度就小于5/3.6,df3 shape:\",df3.shape,\"占输入样本比例为:\",df3.shape[0]/df2.shape[0])\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    df3.to_csv(\"tmpForAnalyzing5.csv\")\n",
    "    \n",
    "    \n",
    "    tmp1 = df2['arriveTime2'] - df2['redLightTime'] >0 #到达时间大于红灯时间\n",
    "    tmp1 = tmp1 & (df2['speed'] > 5/3.6) #本身速度就大于于5/3.6\n",
    "    tmp1 = tmp1 & (df2['vehPos_2'] > 0) #\n",
    "    tmp1 = tmp1 & (df2['vehSpeed_2'] < 5/3.6) #\n",
    "    tmp1 = tmp1 & (df2['vehPos_3'] >0) #\n",
    "    tmp1 = tmp1 & (df2['vehSpeed_3'] <5/3.6) #\n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"到达时间大于红灯时间  and 本身速度就大于5/3.6,df3 shape:\",df3.shape,\"占输入样本比例为:\",df3.shape[0]/df2.shape[0])\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    df3.to_csv(\"tmpForAnalyzing6.csv\")\n",
    "\n",
    "def extractStillVeh2(df):\n",
    "    df=df.rename(columns={'ArrTimeDivRedTime':'numStillVeh'})\n",
    "    df=df.rename(columns={'lanAvgSpeed':'predictStats'})\n",
    "    df['numStillVeh'] = 0\n",
    "    df['predictStats'] = \"unknown\"\n",
    "    for i in range(df.shape[0]):\n",
    "        numStillVeh = 0\n",
    "        tmp = df.iloc[i]\n",
    "        redTime = tmp.iloc[0]\n",
    "        vPosObj = tmp.iloc[1]\n",
    "        predictStats = -1\n",
    "\n",
    "        for j in range(20):\n",
    "           \n",
    "            vehPos = tmp.iloc[2*j+8]\n",
    "            vehVeh = tmp.iloc[2*j+1+8]\n",
    "            \n",
    "            if vehPos >0 and vehVeh<5/3.6:#经验数据,参数\n",
    "                numStillVeh = numStillVeh + 1\n",
    "            elif vehPos >0 and  vPosObj > vehPos:\n",
    "                timeTmp1 =(vehPos-j*6.5)/(vehVeh+0.001)#经验公式，到固定位置后，启动需要的时间\n",
    "                if timeTmp1  < redTime +numStillVeh*1.5:\n",
    "                    numStillVeh = numStillVeh + 1\n",
    "\n",
    "            if vehPos >0 and vPosObj == vehPos and vehVeh<5/3.6:\n",
    "                predictStats = \"stop\"#目标车要听停止\n",
    "               \n",
    "\n",
    "            if vehPos >0 and vPosObj == vehPos and vehVeh>5/3.6 :    \n",
    "                timeTmp1 =(vehPos-j*6.5)/(vehVeh+0.001)#经验公式，到固定位置后需要的时间\n",
    "                if timeTmp1  <= redTime +numStillVeh*1.5+1.5:#小于虚拟红灯结束时间\n",
    "                     predictStats = \"stop\" #目标车要听停止\n",
    "                else:        \n",
    "                     predictStats = \"no stop\"  #目标车要不要停止\n",
    "                \n",
    "     \n",
    "        df['numStillVeh'][i] = numStillVeh\n",
    "        df['predictStats'][i] = predictStats\n",
    "\n",
    "    return df\n",
    "##用于分析实际标记类别小于预测标记类别， xyDataTmp[\"predicted Labels By MCS\"] - xyDataTmp[\"origin speedFlag\"]>0\n",
    "def analyzing2(tmp, xyDataTmp,xyOrigin): \n",
    "    dfTmp1 = xyDataTmp[tmp]\n",
    "    #print(dfTmp1.head(5))\n",
    "    \n",
    "    \n",
    "    #1\n",
    "    df2 =  xyOrigin.iloc[dfTmp1.originIndex,:]   \n",
    "    plt.show()\n",
    "    df2[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    #print(df2.info())\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df2 = extractStillVeh2( df2)\n",
    "    df2.to_csv(\"tmpForAnalyzing.csv\")\n",
    "    \n",
    "    \n",
    "    ######################################\n",
    "    tmp1 = (df2['speedFlag'] == 0)  & (df2['predictStats'] == \"stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing1.csv\")\n",
    "    \n",
    "    tmp1 = (df2['speedFlag'] > 0)  & (df2['predictStats'] == \"no stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing2.csv\")\n",
    "    \n",
    "    \n",
    "    tmp1 = (df2['speedFlag'] == 0)  & (df2['predictStats'] == \"no stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing3.csv\")\n",
    "    \n",
    "    tmp1 = (df2['speedFlag'] > 0)  & (df2['predictStats'] == \"stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing4.csv\")\n",
    "    return\n",
    "    \n",
    "    '''\n",
    "    #2\n",
    "    tmp1 = df2['redLightTime'] - df2['arriveTime2'] >0 #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 = df2[df2['redLightTime'] - df2['arriveTime2'] >0] #红灯时间大于到达时间 ,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing2.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    \n",
    "    #3\n",
    "    tmp1 =df2['arriveTime2'] - df2['redLightTime'] >3 #红灯时间小于到达时间3，\n",
    "    tmp1 = tmp1 & (df2['speedFlag'] == 0) \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"#红灯时间小于于到达时间 ,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing3.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))  \n",
    "    \n",
    "   \n",
    "    #4据静止汽车数目，分析在df2['speedFlag'] > 0情况下，虚拟红灯时间小于于到达时间情况，也就是目标车可能不需要停下来\n",
    "    \n",
    "    tmp1 =(df2['speedFlag'] == 0) \n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 < df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 = tmp1 & tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing4.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))  \n",
    "    \n",
    "    #5 据静止汽车数目，分析在df2['speedFlag'] > 0情况下，虚拟红灯时间大于到达时间情况，也就是目标车可能需要停下来\n",
    "    tmp1 =(df2['speedFlag'] > 0) \n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 >= df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 = tmp1 & tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing5.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))  \n",
    "    \n",
    "    \n",
    "    #6 据静止汽车数目，分析虚拟红灯时间大于到达时间情况，也就是目标车可能需要停下来\n",
    "  \n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 >= df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 =  tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing6.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    \n",
    "    \n",
    "    #7 根据静止汽车数目，分析虚拟红灯时间小于到达时间情况，也就是目标车可能不需要停下来\n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']+1.5\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 < df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 =  tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing7.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4)) \n",
    "   '''\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "tmp = (xyDataTmp[\"origin speedFlag\"] - xyDataTmp[\"predicted Labels By MCS\"] >0) \n",
    "\n",
    "#analyzing1(tmp, xyDataTmp,xyOrigin)\n",
    "\n",
    "tmp = xyDataTmp[\"origin speedFlag\"] - xyDataTmp[\"predicted Labels By MCS\"]  >=3 \n",
    "#analyzing(tmp, xyDataTmp,xyOrigin)\n",
    "\n",
    "tmp = xyDataTmp[\"predicted Labels By MCS\"] - xyDataTmp[\"origin speedFlag\"]>0\n",
    "analyzing2(tmp, xyDataTmp,xyOrigin)\n",
    "      \n",
    "tmp = xyDataTmp[\"predicted Labels By MCS\"] - xyDataTmp[\"origin speedFlag\"]>=3\n",
    "#analyzing2(tmp, xyDataTmp,xyOrigin)\n",
    "\n",
    "#手动修改\n",
    "\n",
    "    \n",
    "      \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46c252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T10:10:55.559866Z",
     "start_time": "2023-01-27T10:10:53.398090Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c877a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T09:04:50.440620Z",
     "start_time": "2023-01-28T09:04:50.239643Z"
    }
   },
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b3f88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-29T03:05:13.857103Z",
     "start_time": "2023-01-29T03:05:13.853125Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "timestr= datetime.now()\n",
    "print(timestr)\n",
    "\n",
    "!conda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281fc84-e6c1-4671-9323-70c1fdbb7210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf tmp*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75a25a-12fd-4f04-85e2-c2a78f766298",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[11, 3, 4 ,5],[6, 7, 8, 9]])\n",
    "print(np.where(arr < 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0697b-8759-40f4-8556-8ec1bfa76aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor23py36gpu",
   "language": "python",
   "name": "tensor23py36gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b27f224da048d073ae2b306b979c73d2559eaa860bf21b792b51024f42769a7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
