{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f6c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T03:55:17.555121Z",
     "start_time": "2023-01-26T03:55:17.435109Z"
    }
   },
   "outputs": [],
   "source": [
    "#一些常用的命令\n",
    "!nvidia-smi\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#一些常用的命令\n",
    "!git status\n",
    "!git add .\n",
    "!git commit -m \"correct and optimize some code\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0809da8b-07fe-447a-b1fd-d7520e79d721",
   "metadata": {},
   "outputs": [],
   "source": [
    "############print(\"程序0.000 全局共享函数，运行主程序前需要运行\")\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import pickle  \n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "#######开始为功能函数\n",
    "print(\"全局共享函数，运行主程序前需要运行\")\n",
    "def dtFitAndSave(x,y,saveName):\n",
    "    str1=\"dtFitAndSave,用于决策树拟合和识别\"\n",
    "    \n",
    "    dt = tree.DecisionTreeClassifier(max_depth=7,min_samples_leaf=100)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    #data=tree.export_graphviz(dt, out_file=None,class_names=None,filled=True) \n",
    "    #graph = graphviz.Source(data)\n",
    "    #graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(\"纯决策树的识别\\n\",tmp1)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    #text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    return dt,yPredict\n",
    "\n",
    "########################################################################################################################\n",
    "###简单模型3，resnet_like\n",
    "def local_model(num_labels, dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(num_labels, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def global_model(dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='sigmoid',name=\"global\"))\n",
    "    return model\n",
    "\n",
    "def softmax_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='softmax',name=\"global\"))\n",
    "    return model\n",
    "'''\n",
    "############################################################################\n",
    "############################################################################\n",
    "#单层模型\n",
    "def kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels,saveName):\n",
    "    str1=\"kerasFitAndSaveSimple3LikeResnet,用于resnet_like的神经网络拟合和识别\"\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 512\n",
    "    dropout_rate = 0.05\n",
    "    hierarchy = [1,1,1,1]#四层，对于当前数据集已经足够了\n",
    "    global_models = []\n",
    "   \n",
    "    \n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(len(hierarchy)):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "\n",
    "   \n",
    "    \n",
    "    p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "       build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file='KerasSimple3_likeResnet_4lay512nodes.jpg', show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=1500, batch_size=40000*1)#GPU用这个\n",
    "    #saveName = \"KerasSimple3_likeResnet.h5\"\n",
    "    build_model.save(saveName)\n",
    "    plot_model(build_model, to_file='KerasSimple3_likeResnet_4lay512nodes.jpg', show_shapes=True)\n",
    "    return build_model\n",
    "\n",
    "############################################################################\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，\n",
    "def kerasFitAndSaveHierSimple4LikeResnet(x,yOneHot,num_labels,saveName):\n",
    "    str1=\"kerasFitAndSaveHierSimple4LikeResnet,用于resnet_like的 神经网络拟合和识别\"\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    beta = 0.5\n",
    "    hierarchy = [2,4,6,8,9]#5层，对于当前数据集已经足够了\n",
    "    global_models = []\n",
    "    local_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(len(hierarchy)):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "    \n",
    "    for i in range(len(hierarchy)):\n",
    "        local_models.append(local_model(hierarchy[i], dropout_rate, relu_size)(global_models[i]))\n",
    "        \n",
    "        \n",
    "    #显示只有局部局模型的情况(部分全局)\n",
    "    p_loc = layers.concatenate(local_models)\n",
    "    #modelTmp2 = tf.keras.Model(inputs=[features], outputs=[p_loc])\n",
    "    #modelTmp2.summary()#\n",
    "    #plot_model(modelTmp2, to_file='Flatten2.png', show_shapes=True)\n",
    "    p_glob1 = layers.Lambda(lambda x: x*beta,name=\"global\")(p_glob)\n",
    "    p_loc1 = layers.Lambda(lambda x: x*(1-beta),name=\"local\")(p_loc)\n",
    "\n",
    "    labels = layers.add([p_glob1, p_loc1])\n",
    "\n",
    "\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[labels])\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file='hmcnf1.jpg', show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=3500, batch_size=40000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "'''\n",
    "############################################################################\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，所以采用分离式\n",
    "###每一层的识别模型都是4层模型\n",
    "def g_sepHier1(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 10,srelu_size = 256,dropout_rate = 0.05):\n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.01\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    if 0:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=40000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "def getKerasResnetRVL(x,enc,saveName):\n",
    "    model_name = saveName \n",
    "    model = keras.models.load_model(model_name)\n",
    "    y= model.predict([x], batch_size=2560)\n",
    "    nSamples = y.shape[0]\n",
    "    ###需要将预测出的值，转换01整数,并转为数字式\n",
    "    for i in range(y.shape[0]):\n",
    "        tmp = y[i]\n",
    "        index=  np.argmax(tmp)\n",
    "        y[i] = [0]*y.shape[1]\n",
    "        y[i,index]=1\n",
    "   \n",
    "\n",
    "    ###  \n",
    "    y= enc.inverse_transform(y)\n",
    "    y= y.reshape(-1,nSamples)[0]\n",
    "    \n",
    "    \n",
    "    return y\n",
    "\n",
    "def string2int(inputString):\n",
    "     #print(inputString)\n",
    "     tmp = 0\n",
    "     try:\n",
    "         strTmp=[str(ord(x)) for x in inputString]\n",
    "         tmp=tmp.join(strTmp)\n",
    "         tmp = float(tmp)/(len(inputString)*128)\n",
    "     except:\n",
    "         #print(inputString)\n",
    "         strTmp = inputString\n",
    "         tmp= \"0\"\n",
    "         tmp = 0\n",
    "     return tmp\n",
    " ## 根据经验以及最佳正确率的合并方法\n",
    " #第一次合并为0,1的合并\n",
    "\n",
    "########################################################################################################################\n",
    "##手工确定层次结构，以前测试时候为5层，根据论文为9层\n",
    "def convertY2Hieral(y):\n",
    "    #mat2acc\n",
    "    # [[0.914 0.009 0.017 0.007 0.032 0.    0.    0.    0.   ]\n",
    "    # [0.027 0.984 0.006 0.007 0.018 0.    0.    0.    0.   ]\n",
    "    # [0.02  0.006 0.972 0.    0.011 0.    0.    0.    0.   ]\n",
    "    # [0.036 0.002 0.    0.986 0.014 0.    0.002 0.    0.   ]\n",
    "    # [0.003 0.    0.    0.    0.925 0.    0.    0.    0.   ]\n",
    "    # [0.    0.    0.    0.    0.    1.    0.005 0.    0.   ]\n",
    "    # [0.    0.    0.    0.    0.    0.    0.993 0.    0.004]\n",
    "    # [0.    0.    0.    0.    0.    0.    0.    0.996 0.   ]\n",
    "    # [0.    0.    0.004 0.    0.    0.    0.    0.004 0.996]]\n",
    "    \n",
    "    \n",
    "    #hierarchy = [2,4,6,8,9]\n",
    "   # labelDict = {\"0\":[\"01234\",\"0123\",\"012\",\"01\",\"0\"],\\\n",
    "   #               \"1\":[\"01234\",\"0123\",\"012\",\"01\",\"1\"],\\\n",
    "   #               \"2\":[\"01234\",\"0123\",\"012\",\"2\",\"2\"],\\\n",
    "   #               \"3\":[\"01234\",\"0123\",\"3\",\"3\",\"3\"],\\\n",
    "   #              \"4\":[\"01234\",\"4\",    \"4\",\"4\",\"4\"],\\\n",
    "   #              \"5\":[\"5678\",\"5\",     \"5\",\"5\",\"5\"],\\\n",
    "   #              \"6\":[\"5678\",\"678\",   \"67\",\"6\",\"6\"],\\\n",
    "   #              \"7\":[\"5678\",\"678\",   \"67\",\"7\",\"7\"],\\\n",
    "   #              \"8\":[\"5678\",\"678\",   \"8\",\"8\",\"8\"],\\\n",
    "   #               }\n",
    "    \n",
    "    hierarchy = [2,3,4,5,6,7,8,9]\n",
    "    labelDict = {\"0\":[\"01234\",        \"01234\",        \"01234\",   \"01234\",      \"0123\",\"012\",\"01\",\"0\"],\\\n",
    "                  \"1\":[\"01234\",        \"01234\",        \"01234\",  \"01234\",     \"0123\",\"012\",\"01\",\"1\"],\\\n",
    "                  \"2\":[\"01234\",          \"01234\",      \"01234\",  \"01234\",     \"0123\",\"012\",\"2\",\"2\"],\\\n",
    "                  \"3\":[\"01234\",         \"01234\",       \"01234\",  \"01234\",    \"0123\",\"3\",\"3\",\"3\"],\\\n",
    "                 \"4\":[\"01234\",          \"01234\",       \"01234\",  \"01234\" ,     \"4\", \"4\",\"4\",\"4\"],\\\n",
    "                 \"5\":[\"5678\",               \"5\",        \"5\" ,      \"5\",       \"5\", \"5\",\"5\",\"5\"],\\\n",
    "                 \"6\":[\"5678\",            \"678\",        \"6\",        \"6\",       \"6\", \"6\",\"6\",\"6\"],\\\n",
    "                 \"7\":[\"5678\",             \"678\",       \"78\",       \"7\",       \"7\", \"7\",\"7\",\"7\"],\\\n",
    "                 \"8\":[\"5678\",             \"678\",       \"78\" ,      \"8\",        \"8\", \"8\",\"8\",\"8\"],\\\n",
    "                  }\n",
    "    '''\n",
    "    hierarchy = [5,9]\n",
    "    labelDict = {\"0\":[\"01\",\"0\"],\\\n",
    "                  \"1\":[\"01\",\"1\"],\\\n",
    "                  \"2\":[\"2\",\"2\"],\\\n",
    "                  \"3\":[\"34\",\"3\"],\\\n",
    "                \"4\":[\"34\",\"4\"],\\\n",
    "                 \"5\":[\"56\",\"5\"],\\\n",
    "                 \"6\":[\"56\",\"6\"],\\\n",
    "                 \"7\":[\"78\",\"7\"],\\\n",
    "                 \"8\":[\"78\",\"8\"],\\\n",
    "                 }\n",
    "    '''\n",
    "\n",
    "    y1 = [list(labelDict[str(x)]) for x in y]\n",
    "   \n",
    "    #print(\"!!!y1.type:\", type(y1))\n",
    "    #print(y1[:2])\n",
    "    #y2 = [t1[0] for t1 in y1]\n",
    "    #print(len(y2))\n",
    "  \n",
    "\n",
    "    return y1,hierarchy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dbf652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T14:33:49.232503Z",
     "start_time": "2023-02-13T14:31:39.188869Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##主程序开始######################################################################################################################\n",
    "print(\"0.主程序开始，建立多层嵌套决策树模型，3080ti的GPU是AMD2400CPU 运算速度100倍\")\n",
    "print(\"0.这是简化程序，原始带有更多测试和原始模型的程序在mainTestCSVMLP3(hmcnf_keras).ipynb\")\n",
    "print(\"程序编号为0\")\n",
    "########################################################################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import pickle  \n",
    "\n",
    "\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，所以采用分离式\n",
    "###每一层的识别模型都是4层模型\n",
    "def sepHier1(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 10,srelu_size = 256,dropout_rate = 0.05):\n",
    "    \n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    featuresInput = layers.Input(shape=(features_size,))\n",
    "    features = layers.BatchNormalization()(featuresInput)\n",
    "    #features=featuresInput\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[featuresInput], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        #build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=160000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "print(\"0.主程序开始, 建立多层嵌套决策树模型,3080ti的GPU是AMD2400CPU 运算速度100倍\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "#from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "#policy = mixed_precision.Policy('mixed_float16')\n",
    "#mixed_precision.set_policy(policy)\n",
    "\n",
    " \n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "print(\"读取France数据并且把数据进行onehot处理\")\n",
    "\n",
    "#file1 = \"../trainData/france_0_allSamples1.csv\"\n",
    "file1 = \"../trainData/france_0_allSamples1_2slot.csv\"\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "h,w = xyData.shape\n",
    "#x = xyData[:,1:23]#简单处理与SUMO数据库一致\n",
    "x0rigin = xyData[:,1:w-1]#用所有的数据,第0列为vehID,不要\n",
    "y0rigin  = xyData[:,w-1]\n",
    "\n",
    "x0rigin[:,6] = [string2int(inputString) for inputString in x0rigin[:,6] ]#字符串vehLaneID 变为整数\n",
    "\n",
    "x0rigin =x0rigin.astype(np.float32)#GPU 加这个\n",
    "y0rigin =y0rigin.astype(np.int64)#GPU 加这个\n",
    "\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "x0,y0= ros.fit_resample(x0rigin , y0rigin)#对数据不平衡进行处理，保证样本数一致\n",
    "\n",
    "x0=x0.astype(np.float32)#GPU 加这个\n",
    "y0=y0.astype(np.int64)#GPU 加这个\n",
    "yl5 = y0\n",
    "print(\"x0.shape:\",x0.shape,\"y0.shape:\",y0.shape,\"y0.type:\", type(y0) )\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "###现在暂时不训练多层模型，只训练9label模型\n",
    "if 0:\n",
    "    print(\"训练4层, 9 label 模型\")\n",
    "    x=x0\n",
    "    y=yl5\n",
    "    x=x.astype(np.float32)#GPU 加这个\n",
    "    y=y.astype(np.int64)#GPU 加这个\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    \n",
    "    num_labels = 9 \n",
    "    nSamples,nFeatures =  x.shape\n",
    "    enc = OneHotEncoder()\n",
    "    y= y.reshape(nSamples,-1)\n",
    "    \n",
    "    print(\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    enc.fit(y)\n",
    "    yOneHot=enc.transform(y).toarray()\n",
    "    saveName = \"../trainedModes/model-9label-4lays-512nodes-2slots-gpu1.h5\"\n",
    "    if 0:\n",
    "        kerasModel3_5label = kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels,saveName)     \n",
    "    yKeras_5label=getKerasResnetRVL(x,enc,saveName)\n",
    "    \n",
    "    print('keras\\n')\n",
    "    mat1num = confusion_matrix(y, yKeras_5label)\n",
    "    mat2acc = confusion_matrix(y, yKeras_5label,normalize='pred')\n",
    "    print('mat1num\\n',mat1num)\n",
    "    print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "    \n",
    "    \n",
    "########################################################################################################################    \n",
    "########################################################################################################################   \n",
    "########################################################################################################################    \n",
    "########################################################################################################################    \n",
    "print(\"##############################################################################################################\")\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"1.接编号为0的主程序,根据基于正确率的聚类程序或者经验将底层类别归结到上一层的类别\")\n",
    "print(\"2.程序编号为0+\")  \n",
    " \n",
    "'''\n",
    "if 0:# 训练统合样式的HMCN-F多级模型\n",
    "    print(\"训练5hieral, 4层, 9 label 模型\")\n",
    "    x=x0\n",
    "    y=yl5\n",
    "    x=x.astype(np.float32)#GPU 加这个\n",
    "    y=y.astype(np.int64)#GPU 加这个\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    yH1 = convertY2Hieral(y)\n",
    "    hierarchy = [2,4,6,8,9]#5层，对于当前数据集已经足够了\n",
    "    \n",
    "    yH1= np.array(yH1)\n",
    "    \n",
    "    nSamples,nFeatures =  x.shape\n",
    "    enc = OneHotEncoder()\n",
    "    yH1= yH1.reshape(nSamples,-1)\n",
    "    #print(yH1[:3])\n",
    "    print(\"yH1.shape:\",yH1 .shape,\"yH1.type:\", type(yH1) )\n",
    "    enc.fit(yH1)\n",
    "    #print(enc.categories_,enc.get_feature_names())\n",
    "    yOneHot=enc.transform(yH1).toarray()\n",
    "    #print(yOneHot[:3])\n",
    "    \n",
    "    num_labels = yOneHot.shape[1] \n",
    "    print(num_labels)\n",
    "    saveName = \"../trainedModes/model-5hier-9label-5lays-128nodes-2slots-gpu1.h5\"\n",
    "    kerasModel4_5hier_9label = kerasFitAndSaveHierSimple4LikeResnet(x,yOneHot,num_labels,saveName)   \n",
    "'''    \n",
    "    \n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if 1:# 训练多级模型\n",
    "    print(\"训练分离式多级模型\")\n",
    "    \n",
    "    #准备字典，用于保存训练后的数据\"\n",
    "    xFloors=  dict()\n",
    "    yFloors =  dict()\n",
    "    xTestFloors =dict()\n",
    "    yTestFloors = dict()\n",
    "    modSaveNameFloors =dict()\n",
    "    encLevels= dict()\n",
    "    yKerasFloors = dict()\n",
    "    x=x0\n",
    "    y=yl5\n",
    "    x=x.astype(np.float32)#GPU 加这个\n",
    "    y=y.astype(np.int64)#GPU 加这个\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    print(y)\n",
    "    \n",
    "    #hierarchy = [2,4,6,8,9]\n",
    "    #hierarchy = [2,3,4,5,6,7,8,9]\n",
    "    yH1,hierarchy = convertY2Hieral(y)\n",
    "    \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, yH1, test_size=0.5, random_state=0)\n",
    "   \n",
    "    nSamples,nFeatures =  x_train.shape\n",
    "    \n",
    "    \n",
    "    numEpochs =30 #1500/60/60*5 = 2houer\n",
    "    \n",
    "    \n",
    "    for i in range(len(hierarchy)):\n",
    "        print(\"\\n\\n levelIndex\",i,\"nSamples,nFeatures\",x_train.shape)\n",
    "        levelIndex = i\n",
    "        numLayers = 4\n",
    "        enc = OneHotEncoder()\n",
    "        nSamples,nFeatures =  x_train.shape\n",
    "       \n",
    "            \n",
    "        yCurLayer1 = [t1[i] for t1 in y_train]\n",
    "        \n",
    "        yCurLayer1 = np.array(yCurLayer1)\n",
    "        print(\"yCurLayer1.shape:\",yCurLayer1.shape)\n",
    "        \n",
    "        yCurLayer1= yCurLayer1.reshape(nSamples,-1)\n",
    "        enc.fit(yCurLayer1)\n",
    "        \n",
    "        yOneHot=enc.transform(yCurLayer1).toarray()\n",
    "        print(enc.categories_,enc.get_feature_names())\n",
    "        print(yOneHot[:1])\n",
    "        \n",
    "        \n",
    "        num_labels = hierarchy[i] \n",
    "        print(\"num_labels:\", num_labels)\n",
    "        saveName = \"../trainedModes/modelSep-9level%d-%dlayer-2slots-gpu1.h5\" %(i,numLayers)\n",
    "        #saveName = \"../trainedModes/modelSep-2level%d-%dlayer-2slots-gpu1.h5\" %(i,numLayers)#基于拥堵定义的2层结构\n",
    "        print(saveName)\n",
    "        sepHier1(x_train,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs)\n",
    "        \n",
    "        encLevels[str(i)] = enc\n",
    "        xFloors[str(i)] = x_train\n",
    "        yFloors[str(i)] = yCurLayer1\n",
    "        \n",
    "        \n",
    "        nSamplesTest,nFeaturesT =  x_test.shape\n",
    "        yCurLayerTest = [t1[i] for t1 in y_test]\n",
    "        yCurLayerTest = np.array(yCurLayerTest)\n",
    "        yCurLayerTest= yCurLayerTest.reshape(nSamplesTest,-1)\n",
    "        \n",
    "        xTestFloors[str(i)] = x_test\n",
    "        yTestFloors[str(i)] = yCurLayerTest\n",
    "        modSaveNameFloors[str(i)] = saveName\n",
    "        \n",
    "    #######保存为pickle文件,用于后期的SUMO和数据分析\n",
    "\n",
    "    fpk=open('samples1.pkf','wb+')  \n",
    "    pickle.dump([xFloors,yFloors,modSaveNameFloors,encLevels,xTestFloors, yTestFloors],fpk)  \n",
    "    fpk.close() \n",
    "\n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "#####用现有训练模型进行预测\n",
    "\n",
    "fpk=open('samples1.pkf','rb')   \n",
    "[xFloors,yFloors,modSaveNameFloors,encLevels,xTestFloors, yTestFloors]=pickle.load(fpk)  \n",
    "fpk.close()  \n",
    "\n",
    "\n",
    "yKerasFloors = dict()\n",
    "\n",
    "for i in range(len(hierarchy)):\n",
    "        levelIndex = i\n",
    "        #x = xFloors[str(i)]\n",
    "        #yCurLayer1 =  yFloors[str(i)]\n",
    "        \n",
    "        x = xTestFloors[str(i)]\n",
    "        yCurLayer1 =  yTestFloors[str(i)]\n",
    "        \n",
    "        saveName =  modSaveNameFloors[str(i)] \n",
    "        enc = encLevels[str(i)]\n",
    "        yOneHot=enc.transform(yCurLayer1).toarray()\n",
    "        yPredict=getKerasResnetRVL(x,enc,saveName)\n",
    "        print(\"分离式多层识别结果:第%d层\\n\" %i)\n",
    "        mat1num = confusion_matrix(yCurLayer1,yPredict)\n",
    "        print(mat1num)\n",
    "        mat2acc = confusion_matrix(yCurLayer1,yPredict,normalize='pred')  \n",
    "        print(np.around(mat2acc , decimals=3))\n",
    "        yKerasFloors[str(i)] =  yPredict\n",
    "        \n",
    "        df = pd.DataFrame(np.around(mat2acc , decimals=3))\n",
    "        fs = \"test_mat2acc%d.csv\" %i\n",
    "        df.to_csv(fs,index= False, header= False)\n",
    "        \n",
    "fpk=open('samples2.pkf','wb+')  \n",
    "pickle.dump([xFloors,yFloors,modSaveNameFloors,encLevels,yKerasFloors,xTestFloors,yTestFloors],fpk)  \n",
    "fpk.close() \n",
    "\n",
    " \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8674b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T14:36:52.438549Z",
     "start_time": "2023-02-13T14:36:16.048928Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "########################################################################################################################\n",
    "print(\"1.接编号为0的主程序,先找出低概率样本，\")\n",
    "print(\"2.对较低概率的样本进行蒙特卡洛模拟分析，原始对应程序为mainSimSumoFranceDatra\")\n",
    "print(\"3.最终进行分析，程序编号为1\")\n",
    "print(\"3.最终进行分析，程序编号为1\")\n",
    "########################################################################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "#import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "      \n",
    "\n",
    "\n",
    "    \n",
    "########################################################################################################################\n",
    "print(\"1.1 主程序开始\")\n",
    "########################################################################################################################\n",
    "\n",
    "########################################################################################################################\n",
    "#####用现有训练模型进行预测\n",
    "\n",
    "fpk=open('samples2.pkf','rb')   \n",
    "[xFloors,yFloors,modSaveNameFloors,encLevels,yKerasFloors,xTestFloors,yTestFloors]=pickle.load(fpk)  \n",
    "fpk.close()  \n",
    "\n",
    "hierarchy=[2,3,4,5,6,7,8,9]\n",
    "for i in [7]:\n",
    "#for i in range(len(hierarchy)):\n",
    "        levelIndex = i \n",
    "        x = xTestFloors[str(i)]\n",
    "        yCurLayer1 =  yTestFloors[str(i)]\n",
    "        #yP = yKerasFloors[str(i)]\n",
    "        \n",
    "        modeSaveName = \"../trainedModes/modelSep-9level7-4layer-2slots-gpu1.h5\"\n",
    "        model = keras.models.load_model(modeSaveName)\n",
    "        yPredictOut= model.predict([x], batch_size=2560)\n",
    "        yPredictOut = np.around(yPredictOut , decimals=3)\n",
    "        #print(yPredictOut)\n",
    "        ymax1=np.max(yPredictOut,axis=1)\n",
    "        ymax2=np.argmax(yPredictOut,axis=1)\n",
    "        \n",
    "        index = np.where(ymax1<0.5)[0]#提取最大值小于0.95的例子\n",
    "        \n",
    "        ylowpraPredictNN=yPredictOut[index]#对较低概率的样本\n",
    "        xlowpra=x[index]\n",
    "        ylowpraLabel = yCurLayer1[index]\n",
    "        ylowPredictLabel = ymax2[index].reshape(-1,1)\n",
    "        \n",
    "        \n",
    "        print(\"xlowpra.shape\",xlowpra.shape)\n",
    "        \n",
    "        \n",
    "        fpk=open('lowprobSamples.pkf','wb+')  \n",
    "        pickle.dump([xlowpra,ylowpraLabel,ylowPredictLabel,ylowpraPredictNN],fpk)  \n",
    "        fpk.close() \n",
    "        \n",
    "        df = pd.DataFrame(xlowpra)\n",
    "        fs = \"lowprobSamplesX.csv\"\n",
    "        df.to_csv(fs,index= False, header= False)\n",
    "       \n",
    "        ylowPredictLabel = ymax2[index].reshape(-1,1)\n",
    "        \n",
    "        df = pd.DataFrame(np.concatenate([ylowpraLabel,ylowPredictLabel,ylowpraPredictNN],axis=1))\n",
    "        fs = \"lowprobSamplesY.csv\"\n",
    "        df.to_csv(fs,index= False, header=['ylowpraLabel','ylowPredictLabel','0','1','2','3','4','5','6','7','8'])\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c074866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T09:51:42.931165Z",
     "start_time": "2023-01-28T09:51:42.771943Z"
    }
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "print(\"接程序1: 对较低概率的样本进行蒙特卡洛模拟分析,原始对应程序为mainSimSumoFranceDatra\")\n",
    "print(\"因为配置失误，采用将低概率的样本进行保存为文件，然后再root用户下命令行模式用SUMO模拟（不使用conda）\")\n",
    "print(\"输出为sumoSimData？？？.csv,里面有每个样本的sumo输出，kerasNN输出以及原始的输入输出\")\n",
    "print(\"程序编号为2\")\n",
    "print(\"程序编号为2\")\n",
    "########################################################################################################################\n",
    "!python3 sumoSimByFrance.py#运行runSumoSimFun.py 中test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "becda4ae-fe95-4636-9b74-0f775aec3d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "接程序2: 综合SUMO输出，对keras输出进行优化。程序输入为程序2的输出\n",
      "优化选择1.NN。 2 回归分析。3 概率分析。\n",
      "程序编号为3\n",
      "##############################################################################################################\n",
      "程序编号为3.1，主程序开始运行\n",
      "(10591, 16)\n",
      "Index(['sampleIndex', 'outputAvgSpeed', 'originOutput', 'sumoOutputSpeedTag',\n",
      "       'kerasPredictLabel', 'NN0', 'NN1', 'NN2', 'NN3', 'NN4', 'NN5', 'NN6',\n",
      "       'NN7', 'NN8', 'smv1', 'smv2'],\n",
      "      dtype='object')\n",
      "(10591, 11)\n",
      "(10591, 1)\n",
      "#############################\n",
      "数据预处理\n",
      "\n",
      "#############################\n",
      "原生keras\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.17      0.30      3029\n",
      "           1       0.30      0.21      0.24      2437\n",
      "           2       0.00      0.00      0.00       298\n",
      "           3       0.44      0.60      0.51      2752\n",
      "           4       0.37      0.81      0.51      2075\n",
      "           5       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.41     10591\n",
      "   macro avg       0.30      0.26      0.22     10591\n",
      "weighted avg       0.54      0.41      0.37     10591\n",
      "\n",
      "[[ 530  394   64 1092  944    4    1]\n",
      " [   0  502    0 1001  934    0    0]\n",
      " [   0  165    0    0  133    0    0]\n",
      " [   0  229    0 1648  875    0    0]\n",
      " [   0  395    0    0 1680    0    0]\n",
      " [   0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0]]\n",
      "[[1.    0.234 1.    0.292 0.207 1.    1.   ]\n",
      " [0.    0.298 0.    0.268 0.205 0.    0.   ]\n",
      " [0.    0.098 0.    0.    0.029 0.    0.   ]\n",
      " [0.    0.136 0.    0.441 0.192 0.    0.   ]\n",
      " [0.    0.234 0.    0.    0.368 0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.   ]]\n",
      "#############################\n",
      "决策树\n",
      "\n",
      "纯决策树的识别\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.51      0.66      3029\n",
      "           1       0.64      0.21      0.32      2437\n",
      "           2       0.00      0.00      0.00       298\n",
      "           3       0.54      0.60      0.57      2752\n",
      "           4       0.41      1.00      0.59      2075\n",
      "\n",
      "    accuracy                           0.55     10591\n",
      "   macro avg       0.50      0.47      0.43     10591\n",
      "weighted avg       0.63      0.55      0.52     10591\n",
      "\n",
      "[[1559  293    0  429  748]\n",
      " [ 139  516    0 1001  781]\n",
      " [   0    0    0    0  298]\n",
      " [   0    0    0 1648 1104]\n",
      " [   0    0    0    0 2075]]\n",
      "[[0.918 0.362 0.    0.139 0.149]\n",
      " [0.082 0.638 0.    0.325 0.156]\n",
      " [0.    0.    0.    0.    0.06 ]\n",
      " [0.    0.    0.    0.535 0.221]\n",
      " [0.    0.    0.    0.    0.415]]\n",
      "#############################\n",
      "逻辑回归\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.54      0.55      3029\n",
      "           1       0.38      0.20      0.26      2437\n",
      "           2       0.00      0.00      0.00       298\n",
      "           3       0.49      0.43      0.46      2752\n",
      "           4       0.43      0.82      0.56      2075\n",
      "\n",
      "    accuracy                           0.47     10591\n",
      "   macro avg       0.37      0.40      0.37     10591\n",
      "weighted avg       0.46      0.47      0.45     10591\n",
      "\n",
      "[[1634  386    3  442  564]\n",
      " [ 582  482    0  781  592]\n",
      " [  84    0    0    0  214]\n",
      " [ 458  219    0 1184  891]\n",
      " [ 202  176    0    0 1697]]\n",
      "[[0.552 0.306 1.    0.184 0.142]\n",
      " [0.197 0.382 0.    0.324 0.15 ]\n",
      " [0.028 0.    0.    0.    0.054]\n",
      " [0.155 0.173 0.    0.492 0.225]\n",
      " [0.068 0.139 0.    0.    0.429]]\n",
      "#############################\n",
      "贝叶斯高斯回归\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.04      0.07      3029\n",
      "           1       0.23      0.11      0.15      2437\n",
      "           2       0.20      0.55      0.30       298\n",
      "           3       0.39      0.60      0.48      2752\n",
      "           4       0.39      0.82      0.53      2075\n",
      "\n",
      "    accuracy                           0.37     10591\n",
      "   macro avg       0.44      0.42      0.30     10591\n",
      "weighted avg       0.52      0.37      0.29     10591\n",
      "\n",
      "[[ 108  653  258 1304  706]\n",
      " [   0  260   79 1226  872]\n",
      " [   0   54  163    0   81]\n",
      " [   0   56   46 1648 1002]\n",
      " [   0  122  259    0 1694]]\n",
      "[[1.    0.57  0.32  0.312 0.162]\n",
      " [0.    0.227 0.098 0.293 0.2  ]\n",
      " [0.    0.047 0.202 0.    0.019]\n",
      " [0.    0.049 0.057 0.394 0.23 ]\n",
      " [0.    0.107 0.322 0.    0.389]]\n",
      "#############################\n",
      "kerasNN\n",
      "\n",
      "(10591, 11)\n",
      "(10591, 5)\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.9758 - accuracy: 0.5424\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.9697 - accuracy: 0.5442\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9725 - accuracy: 0.5431\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9801 - accuracy: 0.5418\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9792 - accuracy: 0.5400\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9779 - accuracy: 0.5388\n",
      "##############################################################################################################\n",
      "程序编号为3.2，手动权值分配\n",
      "[[0.    4.    0.14  ... 0.    0.    0.   ]\n",
      " [0.    4.    0.178 ... 0.    0.    0.   ]\n",
      " [0.    4.    0.185 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    1.    0.184 ... 0.    0.    0.   ]\n",
      " [0.    1.    0.184 ... 0.    0.    0.   ]\n",
      " [0.    3.    0.342 ... 0.    0.    0.   ]]\n",
      "1 0.0 4.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-1abdc845eeb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;31m#for i in range(x3.shape[0]):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mnn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0msumoOut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0mkerasOut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "print(\"接程序2: 综合SUMO输出，对keras输出进行优化。程序输入为程序2的输出\")\n",
    "print(\"优化选择1.NN。 2 回归分析。3 概率分析。\")\n",
    "print(\"程序编号为3\")\n",
    "########################################################################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import pickle \n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import naive_bayes\n",
    "############################################################################\n",
    "############################################################################\n",
    "####建立NN模型，与sepHier2一样\n",
    "def sepHier2(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 10,srelu_size = 256,dropout_rate = 0.05):\n",
    "    \n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    featuresInput = layers.Input(shape=(features_size,))\n",
    "    features = layers.BatchNormalization()(featuresInput)\n",
    "    #features=featuresInput\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[featuresInput], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        #build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=20000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "\n",
    "def dtFit(x,y):\n",
    "    str1=\"dtFitAndSave,用于决策树拟合和识别\"\n",
    "    \n",
    "    dt = tree.DecisionTreeClassifier(max_depth=7,min_samples_leaf=100)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    #data=tree.export_graphviz(dt, out_file=None,class_names=None,filled=True) \n",
    "    #graph = graphviz.Source(data)\n",
    "    #graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(\"纯决策树的识别\\n\",tmp1)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    #text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    return dt,yPredict\n",
    "########################################################################################################################   \n",
    "########################################################################################################################    \n",
    "print(\"##############################################################################################################\")\n",
    "\n",
    "print(\"程序编号为3.1，主程序开始运行\")\n",
    "\n",
    "   \n",
    "\n",
    "df = pd.read_csv('sumoSimData.csv', sep=',')\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "numSamples,numFeatures = df.shape\n",
    "\n",
    "##['sampleIndex','outputAvgSpeed','originOutput','sumoOutputSpeedTag','kerasPredictLabel','smv1','smv2',\\\n",
    " ##                                              'NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8'])\n",
    "sumoOutput='sumoOutputSpeedTag'\n",
    "yKerasOutput='kerasPredictLabel'\n",
    "originOutput ='originOutput'\n",
    "sumoOutList = ['smv1','smv2']\n",
    "outputListNN = ['NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8']\n",
    "\n",
    "df1 = df[sumoOutput]\n",
    "x1 = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "df1 = df[yKerasOutput]\n",
    "x2 = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "df1 = df[outputListNN]\n",
    "x3 = df1.iloc[0:numSamples].to_numpy()\n",
    "\n",
    "\n",
    "df1 = df[originOutput]\n",
    "y = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y)\n",
    "yOneHot=enc.transform(y).toarray()\n",
    "\n",
    "x = np.concatenate([x1,x2,x3],axis=1)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "#print(y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"#############################\\n数据预处理\\n\")\n",
    "#数据预处理\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, yOneHot, test_size=0.5, random_state=0)\n",
    "   \n",
    "\n",
    "rN,cN= np.where(np.isnan(x))\n",
    "#print(rN,cN)\n",
    "#print(rN.shape)\n",
    "\n",
    "for i in range(rN.shape[0]):\n",
    "    x[rN[i],cN[i]] = 0\n",
    " \n",
    "\n",
    "print(\"#############################\\n原生keras\\n\")\n",
    "yPredict = x2\n",
    "tmp1 = classification_report(y,yPredict)\n",
    "mat1num = confusion_matrix(y,yPredict)\n",
    "mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "    \n",
    "print(\"#############################\\n决策树\\n\")\n",
    "dtFit(x,y)\n",
    "   \n",
    "print(\"#############################\\n逻辑回归\\n\")    \n",
    "model = LogisticRegression()\n",
    "model.fit(x,y)\n",
    "yPredict = model.predict(x)\n",
    "\n",
    "tmp1 = classification_report(y,yPredict)\n",
    "mat1num = confusion_matrix(y,yPredict)\n",
    "mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "\n",
    "print(\"#############################\\n贝叶斯高斯回归\\n\")\n",
    "nb_cls = naive_bayes.GaussianNB().fit(x,y)\n",
    "yPredict = nb_cls.predict(x) \n",
    "\n",
    "tmp1 = classification_report(y,yPredict)\n",
    "mat1num = confusion_matrix(y,yPredict)\n",
    "mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "\n",
    "print(\"#############################\\nkerasNN\\n\")\n",
    "print(x.shape)\n",
    "print(yOneHot.shape)\n",
    "num_labels = yOneHot.shape[1]\n",
    "numLayers = 4\n",
    "numEpochs = 5\n",
    "saveName =\"../trainedModes/stage2_1.h5\";\n",
    "levelIndex = 7\n",
    "\n",
    "sepHier2(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fpk=open('samples2-stage2.pkf','wb')  \n",
    "pickle.dump([df,x,y,yOneHot,x_train, x_test, y_train, y_test,enc,saveName],fpk)  \n",
    "fpk.close() \n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################   \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "1a367eac-c740-4be5-8f8a-5bdba5969e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################################################################\n",
      "程序编号为3.2，手动权值分配\n",
      "(10591, 16)\n",
      "Index(['sampleIndex', 'outputAvgSpeed', 'originOutput', 'sumoOutputSpeedTag',\n",
      "       'kerasPredictLabel', 'NN0', 'NN1', 'NN2', 'NN3', 'NN4', 'NN5', 'NN6',\n",
      "       'NN7', 'NN8', 'smv1', 'smv2'],\n",
      "      dtype='object')\n",
      "[[0.    4.    0.14  ... 0.    0.    0.   ]\n",
      " [0.    4.    0.178 ... 0.    0.    0.   ]\n",
      " [0.    4.    0.185 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    1.    0.184 ... 0.    0.    0.   ]\n",
      " [0.    1.    0.184 ... 0.    0.    0.   ]\n",
      " [0.    3.    0.342 ... 0.    0.    0.   ]]\n",
      "sumoOut kerasOut originOut finalIndex\n",
      "0 4 3 4\n",
      "nn1: [0.14  0.214 0.001 0.247 0.397 0.    0.    0.    0.   ]\n",
      "p  : [1.    0.875 0.75  0.625 0.5   0.4   0.3   0.2   0.1  ]\n",
      "y3 : [0.14  0.187 0.001 0.154 0.198 0.    0.    0.    0.   ]\n",
      "sumoOut kerasOut originOut finalIndex\n",
      "0 4 1 1\n",
      "nn1: [0.178 0.237 0.001 0.236 0.348 0.    0.    0.    0.   ]\n",
      "p  : [1.    0.875 0.75  0.625 0.5   0.4   0.3   0.2   0.1  ]\n",
      "y3 : [0.178 0.207 0.001 0.148 0.174 0.    0.    0.    0.   ]\n",
      "sumoOut kerasOut originOut finalIndex\n",
      "0 4 0 3\n",
      "nn1: [0.185 0.166 0.001 0.299 0.349 0.    0.    0.    0.   ]\n",
      "p  : [1.    0.875 0.75  0.625 0.5   0.4   0.3   0.2   0.1  ]\n",
      "y3 : [0.185 0.145 0.001 0.187 0.174 0.    0.    0.    0.   ]\n",
      "sumoOut kerasOut originOut finalIndex\n",
      "0 4 3 1\n",
      "nn1: [0.179 0.226 0.002 0.241 0.352 0.    0.    0.    0.   ]\n",
      "p  : [1.    0.875 0.75  0.625 0.5   0.4   0.3   0.2   0.1  ]\n",
      "y3 : [0.179 0.198 0.002 0.151 0.176 0.    0.    0.    0.   ]\n",
      "sumoOut kerasOut originOut finalIndex\n",
      "0 3 1 3\n",
      "nn1: [0.242 0.311 0.    0.447 0.    0.    0.    0.    0.   ]\n",
      "p  : [1.    0.875 0.75  0.625 0.5   0.4   0.3   0.2   0.1  ]\n",
      "y3 : [0.242 0.272 0.    0.279 0.    0.    0.    0.    0.   ]\n",
      "sumoOut kerasOut originOut finalIndex\n",
      "0 3 1 1\n",
      "nn1: [0.266 0.328 0.    0.406 0.    0.    0.    0.    0.   ]\n",
      "p  : [1.    0.875 0.75  0.625 0.5   0.4   0.3   0.2   0.1  ]\n",
      "y3 : [0.266 0.287 0.    0.254 0.    0.    0.    0.    0.   ]\n",
      "sumoOut kerasOut originOut finalIndex\n",
      "0 1 3 1\n",
      "nn1: [0.224 0.269 0.073 0.182 0.252 0.    0.    0.    0.   ]\n",
      "p  : [1.    0.875 0.75  0.625 0.5   0.4   0.3   0.2   0.1  ]\n",
      "y3 : [0.224 0.235 0.055 0.114 0.126 0.    0.    0.    0.   ]\n",
      "sumoOut kerasOut originOut finalIndex\n",
      "1 4 4 4\n",
      "nn1: [0.171 0.    0.344 0.001 0.484 0.    0.    0.    0.   ]\n",
      "p  : [0.    1.    0.875 0.75  0.625 0.5   0.375 0.25  0.125]\n",
      "y3 : [0.    0.    0.301 0.001 0.302 0.    0.    0.    0.   ]\n",
      "sumoOut kerasOut originOut finalIndex\n",
      "0 3 3 3\n",
      "nn1: [0.217 0.298 0.    0.485 0.    0.    0.    0.    0.   ]\n",
      "p  : [1.    0.875 0.75  0.625 0.5   0.4   0.3   0.2   0.1  ]\n",
      "y3 : [0.217 0.261 0.    0.303 0.    0.    0.    0.    0.   ]\n",
      "sumoOut kerasOut originOut finalIndex\n",
      "0 4 0 4\n",
      "nn1: [0.168 0.188 0.001 0.27  0.373 0.    0.    0.    0.   ]\n",
      "p  : [1.    0.875 0.75  0.625 0.5   0.4   0.3   0.2   0.1  ]\n",
      "y3 : [0.168 0.164 0.001 0.169 0.186 0.    0.    0.    0.   ]\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################   \n",
    "########################################################################################################################   \n",
    "import scipy.interpolate as si\n",
    "\n",
    "print(\"##############################################################################################################\")\n",
    "\n",
    "print(\"程序编号为3.2，手动权值分配\")\n",
    "\n",
    "df = pd.read_csv('sumoSimData.csv', sep=',')\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "numSamples,numFeatures = df.shape\n",
    "\n",
    "##['sampleIndex','outputAvgSpeed','originOutput','sumoOutputSpeedTag','kerasPredictLabel','smv1','smv2',\\\n",
    " ##                                              'NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8'])\n",
    "sumoOutput='sumoOutputSpeedTag'\n",
    "yKerasOutput='kerasPredictLabel'\n",
    "originOutput ='originOutput'\n",
    "sumoOutList = ['smv1','smv2']\n",
    "outputListNN = ['NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8']\n",
    "\n",
    "\n",
    "\n",
    "df1 = df[originOutput]\n",
    "yo = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "df1 = df[sumoOutput]\n",
    "x1 = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "df1 = df[yKerasOutput]\n",
    "x2 = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "df1 = df[outputListNN]\n",
    "x3 = df1.iloc[0:numSamples].to_numpy()\n",
    "\n",
    "x = np.concatenate([x1,x2,x3],axis=1)\n",
    "print(x)\n",
    "\n",
    "manualOut = np.zeros((x3.shape[0],1))\n",
    "\n",
    "#for i in range(10):\n",
    "for i in range(x3.shape[0]):\n",
    "    nn1 = x3[i]\n",
    "    sumoOut = x1[i][0]\n",
    "    kerasOut = x2[i][0]\n",
    "    originOut = yo[i][0]\n",
    "\n",
    "    xIntp=[0,sumoOut,min(8,sumoOut+4),9]\n",
    "    yIntp =[0,1,0.5,0]\n",
    "    f = si.interp1d(xIntp,  yIntp,kind=1)\n",
    "    xi = [0,1,2,3,4,5,6,7,8]\n",
    "    p= f(xi)\n",
    "    yTmp = np.multiply(p,nn1)\n",
    "    finalIndex = np.argmax(yTmp)\n",
    "    manualOut[i] = finalIndex\n",
    "    \n",
    "    print('sumoOut','kerasOut','originOut','finalIndex')\n",
    "    print(sumoOut,kerasOut,originOut,finalIndex)\n",
    "    print(\"nn1:\",nn1)\n",
    "    print(\"p  :\",np.round(p,decimals=3))\n",
    "    print(\"y3 :\",np.round(y3,decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add02d9c-af03-4a00-8629-42b8a751033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "print(\"辅助程序 对模拟后的数据进行分析，计算正确率\")\n",
    "########################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"对于低概率样本的识别结果，采用keras和MCS的识别结果对比\")\n",
    "file1 = \"./data-Copy1.csv\"\n",
    "xyDataTmp = pd.read_csv(file1,index_col=0)\n",
    "\n",
    "print(xyDataTmp.head(3))\n",
    "print(xyDataTmp.info())\n",
    "\n",
    "file1 = \"。./trainData/france_0_allSamples1.csv\"\n",
    "xyOrigin = pd.read_csv(file1,index_col=0)\n",
    "\n",
    "originlabel =  xyDataTmp.iloc[:,1].to_numpy()\n",
    "keraslabel =   xyDataTmp.iloc[:,2].to_numpy()      \n",
    "mcslabel =     xyDataTmp.iloc[:,3].to_numpy()\n",
    "\n",
    "#\n",
    "    \n",
    "print('\\norigin_mcs')\n",
    "mat1num = confusion_matrix(originlabel, mcslabel)\n",
    "mat2acc = confusion_matrix(originlabel, mcslabel,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "      \n",
    "print('\\nmcs_keras')\n",
    "mat1num = confusion_matrix(mcslabel, keraslabel)\n",
    "mat2acc = confusion_matrix(mcslabel, keraslabel,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "\n",
    "print('\\norgin_keras')\n",
    "mat1num = confusion_matrix(originlabel, keraslabel)\n",
    "mat2acc = confusion_matrix(originlabel ,keraslabel,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))      \n",
    "\n",
    "##用于分析实际标记类别大于预测标记类别\n",
    "def analyzing1(tmp, xyDataTmp,xyOrigin): \n",
    "    dfTmp1 = xyDataTmp[tmp]\n",
    "    #print(dfTmp1.head(5))\n",
    "    \n",
    "    \n",
    "    \n",
    "    df2 =  xyOrigin.iloc[dfTmp1.originIndex,:]   \n",
    "    plt.show()\n",
    "    df2[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    #print(df2.info())\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    \n",
    "    df2.to_csv(\"tmpForAnalyzing.csv\")\n",
    "    \n",
    "    tmp1 = df2['redLightTime'] - df2['arriveTime2'] >1.5 #红灯时间大于到达时间\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 = df2[df2['redLightTime'] - df2['arriveTime2'] >1.5] #红灯时间大于到达时间 ,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing3.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    \n",
    "   \n",
    "    tmp1 = df2['speed'] < 5/3.6 #本身速度就小于5/3.6\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df2['speed'] > 5/3.6 #本身速度就小于5/3.6,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    df3.to_csv(\"tmpForAnalyzing4.csv\")\n",
    "    \n",
    "    tmp1 = df2['redLightTime'] - df2['arriveTime2'] >1.5  #红灯时间大于到达时间\n",
    "    tmp1 = tmp1 | (df2['speed'] < 5/3.6) #本身速度就小于5/3.6\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"红灯时间大于到达时间  or 本身速度就小于5/3.6,df3 shape:\",df3.shape,\"占输入样本比例为:\",df3.shape[0]/df2.shape[0])\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    df3.to_csv(\"tmpForAnalyzing5.csv\")\n",
    "    \n",
    "    \n",
    "    tmp1 = df2['arriveTime2'] - df2['redLightTime'] >0 #到达时间大于红灯时间\n",
    "    tmp1 = tmp1 & (df2['speed'] > 5/3.6) #本身速度就大于于5/3.6\n",
    "    tmp1 = tmp1 & (df2['vehPos_2'] > 0) #\n",
    "    tmp1 = tmp1 & (df2['vehSpeed_2'] < 5/3.6) #\n",
    "    tmp1 = tmp1 & (df2['vehPos_3'] >0) #\n",
    "    tmp1 = tmp1 & (df2['vehSpeed_3'] <5/3.6) #\n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"到达时间大于红灯时间  and 本身速度就大于5/3.6,df3 shape:\",df3.shape,\"占输入样本比例为:\",df3.shape[0]/df2.shape[0])\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    df3.to_csv(\"tmpForAnalyzing6.csv\")\n",
    "\n",
    "def extractStillVeh2(df):\n",
    "    df=df.rename(columns={'ArrTimeDivRedTime':'numStillVeh'})\n",
    "    df=df.rename(columns={'lanAvgSpeed':'predictStats'})\n",
    "    df['numStillVeh'] = 0\n",
    "    df['predictStats'] = \"unknown\"\n",
    "    for i in range(df.shape[0]):\n",
    "        numStillVeh = 0\n",
    "        tmp = df.iloc[i]\n",
    "        redTime = tmp.iloc[0]\n",
    "        vPosObj = tmp.iloc[1]\n",
    "        predictStats = -1\n",
    "\n",
    "        for j in range(20):\n",
    "           \n",
    "            vehPos = tmp.iloc[2*j+8]\n",
    "            vehVeh = tmp.iloc[2*j+1+8]\n",
    "            \n",
    "            if vehPos >0 and vehVeh<5/3.6:#经验数据,参数\n",
    "                numStillVeh = numStillVeh + 1\n",
    "            elif vehPos >0 and  vPosObj > vehPos:\n",
    "                timeTmp1 =(vehPos-j*6.5)/(vehVeh+0.001)#经验公式，到固定位置后，启动需要的时间\n",
    "                if timeTmp1  < redTime +numStillVeh*1.5:\n",
    "                    numStillVeh = numStillVeh + 1\n",
    "\n",
    "            if vehPos >0 and vPosObj == vehPos and vehVeh<5/3.6:\n",
    "                predictStats = \"stop\"#目标车要听停止\n",
    "               \n",
    "\n",
    "            if vehPos >0 and vPosObj == vehPos and vehVeh>5/3.6 :    \n",
    "                timeTmp1 =(vehPos-j*6.5)/(vehVeh+0.001)#经验公式，到固定位置后需要的时间\n",
    "                if timeTmp1  <= redTime +numStillVeh*1.5+1.5:#小于虚拟红灯结束时间\n",
    "                     predictStats = \"stop\" #目标车要听停止\n",
    "                else:        \n",
    "                     predictStats = \"no stop\"  #目标车要不要停止\n",
    "                \n",
    "     \n",
    "        df['numStillVeh'][i] = numStillVeh\n",
    "        df['predictStats'][i] = predictStats\n",
    "\n",
    "    return df\n",
    "##用于分析实际标记类别小于预测标记类别， xyDataTmp[\"predicted Labels By MCS\"] - xyDataTmp[\"origin speedFlag\"]>0\n",
    "def analyzing2(tmp, xyDataTmp,xyOrigin): \n",
    "    dfTmp1 = xyDataTmp[tmp]\n",
    "    #print(dfTmp1.head(5))\n",
    "    \n",
    "    \n",
    "    #1\n",
    "    df2 =  xyOrigin.iloc[dfTmp1.originIndex,:]   \n",
    "    plt.show()\n",
    "    df2[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    #print(df2.info())\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df2 = extractStillVeh2( df2)\n",
    "    df2.to_csv(\"tmpForAnalyzing.csv\")\n",
    "    \n",
    "    \n",
    "    ######################################\n",
    "    tmp1 = (df2['speedFlag'] == 0)  & (df2['predictStats'] == \"stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing1.csv\")\n",
    "    \n",
    "    tmp1 = (df2['speedFlag'] > 0)  & (df2['predictStats'] == \"no stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing2.csv\")\n",
    "    \n",
    "    \n",
    "    tmp1 = (df2['speedFlag'] == 0)  & (df2['predictStats'] == \"no stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing3.csv\")\n",
    "    \n",
    "    tmp1 = (df2['speedFlag'] > 0)  & (df2['predictStats'] == \"stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing4.csv\")\n",
    "    return\n",
    "    \n",
    "    '''\n",
    "    #2\n",
    "    tmp1 = df2['redLightTime'] - df2['arriveTime2'] >0 #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 = df2[df2['redLightTime'] - df2['arriveTime2'] >0] #红灯时间大于到达时间 ,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing2.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    \n",
    "    #3\n",
    "    tmp1 =df2['arriveTime2'] - df2['redLightTime'] >3 #红灯时间小于到达时间3，\n",
    "    tmp1 = tmp1 & (df2['speedFlag'] == 0) \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"#红灯时间小于于到达时间 ,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing3.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))  \n",
    "    \n",
    "   \n",
    "    #4据静止汽车数目，分析在df2['speedFlag'] > 0情况下，虚拟红灯时间小于于到达时间情况，也就是目标车可能不需要停下来\n",
    "    \n",
    "    tmp1 =(df2['speedFlag'] == 0) \n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 < df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 = tmp1 & tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing4.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))  \n",
    "    \n",
    "    #5 据静止汽车数目，分析在df2['speedFlag'] > 0情况下，虚拟红灯时间大于到达时间情况，也就是目标车可能需要停下来\n",
    "    tmp1 =(df2['speedFlag'] > 0) \n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 >= df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 = tmp1 & tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing5.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))  \n",
    "    \n",
    "    \n",
    "    #6 据静止汽车数目，分析虚拟红灯时间大于到达时间情况，也就是目标车可能需要停下来\n",
    "  \n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 >= df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 =  tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing6.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    \n",
    "    \n",
    "    #7 根据静止汽车数目，分析虚拟红灯时间小于到达时间情况，也就是目标车可能不需要停下来\n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']+1.5\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 < df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 =  tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing7.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4)) \n",
    "   '''\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "tmp = (xyDataTmp[\"origin speedFlag\"] - xyDataTmp[\"predicted Labels By MCS\"] >0) \n",
    "\n",
    "#analyzing1(tmp, xyDataTmp,xyOrigin)\n",
    "\n",
    "tmp = xyDataTmp[\"origin speedFlag\"] - xyDataTmp[\"predicted Labels By MCS\"]  >=3 \n",
    "#analyzing(tmp, xyDataTmp,xyOrigin)\n",
    "\n",
    "tmp = xyDataTmp[\"predicted Labels By MCS\"] - xyDataTmp[\"origin speedFlag\"]>0\n",
    "analyzing2(tmp, xyDataTmp,xyOrigin)\n",
    "      \n",
    "tmp = xyDataTmp[\"predicted Labels By MCS\"] - xyDataTmp[\"origin speedFlag\"]>=3\n",
    "#analyzing2(tmp, xyDataTmp,xyOrigin)\n",
    "\n",
    "#手动修改\n",
    "\n",
    "    \n",
    "      \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46c252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T10:10:55.559866Z",
     "start_time": "2023-01-27T10:10:53.398090Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c877a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T09:04:50.440620Z",
     "start_time": "2023-01-28T09:04:50.239643Z"
    }
   },
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b3f88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-29T03:05:13.857103Z",
     "start_time": "2023-01-29T03:05:13.853125Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "timestr= datetime.now()\n",
    "print(timestr)\n",
    "\n",
    "!conda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281fc84-e6c1-4671-9323-70c1fdbb7210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf tmp*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75a25a-12fd-4f04-85e2-c2a78f766298",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[11, 3, 4 ,5],[6, 7, 8, 9]])\n",
    "print(np.where(arr < 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0697b-8759-40f4-8556-8ec1bfa76aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.interpolate as si\n",
    "\n",
    "%matplotlib inline\n",
    "from numpy import polyfit, poly1d\n",
    "x=[0,3,8]\n",
    "y =[0,1,0]\n",
    "coeff = polyfit(x, y, 2)\n",
    "print(coeff)\n",
    " \n",
    "p = plt.plot(x, y, 'rx')\n",
    "\n",
    "x=[0,3,8]\n",
    "y =[0.5,1,0.5]\n",
    "\n",
    "x1 = np.linspace(0, 8, 100)\n",
    "y1 = np.polyval(coeff, x1)\n",
    "p = plt.plot(x1,y1, 'k-')\n",
    "\n",
    "\n",
    "f = si.interp1d(x, y,kind=1)\n",
    "y2= f(x1)  #调用经由interp1d返回的函数\n",
    "p = plt.plot(x1,y2, 'b-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8290fe-e417-434f-b337-6a5cb390c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5])\n",
    "b = np.array([5,5,6,7,8])\n",
    "c = a*b\n",
    "c\n",
    "\n",
    "a = [2,2,3,4,1]\n",
    "b = [5,5,6,7,8]\n",
    "d = np.multiply(a,b)\n",
    "d\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor23py36gpu",
   "language": "python",
   "name": "tensor23py36gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b27f224da048d073ae2b306b979c73d2559eaa860bf21b792b51024f42769a7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
