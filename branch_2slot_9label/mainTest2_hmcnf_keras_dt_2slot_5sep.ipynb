{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f6c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T03:55:17.555121Z",
     "start_time": "2023-01-26T03:55:17.435109Z"
    }
   },
   "outputs": [],
   "source": [
    "#一些常用的命令\n",
    "!nvidia-smi\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#一些常用的命令\n",
    "!git status\n",
    "!git add .\n",
    "!git commit -m \"correct and optimize some code\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0809da8b-07fe-447a-b1fd-d7520e79d721",
   "metadata": {},
   "outputs": [],
   "source": [
    "############print(\"程序0.000 全局共享函数，运行主程序前需要运行\")\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import pickle  \n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "#######开始为功能函数\n",
    "print(\"全局共享函数，运行主程序前需要运行\")\n",
    "def dtFitAndSave(x,y,saveName):\n",
    "    str1=\"dtFitAndSave,用于决策树拟合和识别\"\n",
    "    \n",
    "    dt = tree.DecisionTreeClassifier(max_depth=7,min_samples_leaf=100)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    #data=tree.export_graphviz(dt, out_file=None,class_names=None,filled=True) \n",
    "    #graph = graphviz.Source(data)\n",
    "    #graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(\"纯决策树的识别\\n\",tmp1)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    #text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    return dt,yPredict\n",
    "\n",
    "########################################################################################################################\n",
    "###简单模型3，resnet_like\n",
    "def local_model(num_labels, dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(num_labels, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def global_model(dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='sigmoid',name=\"global\"))\n",
    "    return model\n",
    "\n",
    "def softmax_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='softmax',name=\"global\"))\n",
    "    return model\n",
    "'''\n",
    "############################################################################\n",
    "############################################################################\n",
    "#单层模型\n",
    "def kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels,saveName):\n",
    "    str1=\"kerasFitAndSaveSimple3LikeResnet,用于resnet_like的神经网络拟合和识别\"\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 512\n",
    "    dropout_rate = 0.05\n",
    "    hierarchy = [1,1,1,1]#四层，对于当前数据集已经足够了\n",
    "    global_models = []\n",
    "   \n",
    "    \n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(len(hierarchy)):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "\n",
    "   \n",
    "    \n",
    "    p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "       build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file='KerasSimple3_likeResnet_4lay512nodes.jpg', show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=1500, batch_size=40000*1)#GPU用这个\n",
    "    #saveName = \"KerasSimple3_likeResnet.h5\"\n",
    "    build_model.save(saveName)\n",
    "    plot_model(build_model, to_file='KerasSimple3_likeResnet_4lay512nodes.jpg', show_shapes=True)\n",
    "    return build_model\n",
    "\n",
    "############################################################################\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，\n",
    "def kerasFitAndSaveHierSimple4LikeResnet(x,yOneHot,num_labels,saveName):\n",
    "    str1=\"kerasFitAndSaveHierSimple4LikeResnet,用于resnet_like的 神经网络拟合和识别\"\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    beta = 0.5\n",
    "    hierarchy = [2,4,6,8,9]#5层，对于当前数据集已经足够了\n",
    "    global_models = []\n",
    "    local_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(len(hierarchy)):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "    \n",
    "    for i in range(len(hierarchy)):\n",
    "        local_models.append(local_model(hierarchy[i], dropout_rate, relu_size)(global_models[i]))\n",
    "        \n",
    "        \n",
    "    #显示只有局部局模型的情况(部分全局)\n",
    "    p_loc = layers.concatenate(local_models)\n",
    "    #modelTmp2 = tf.keras.Model(inputs=[features], outputs=[p_loc])\n",
    "    #modelTmp2.summary()#\n",
    "    #plot_model(modelTmp2, to_file='Flatten2.png', show_shapes=True)\n",
    "    p_glob1 = layers.Lambda(lambda x: x*beta,name=\"global\")(p_glob)\n",
    "    p_loc1 = layers.Lambda(lambda x: x*(1-beta),name=\"local\")(p_loc)\n",
    "\n",
    "    labels = layers.add([p_glob1, p_loc1])\n",
    "\n",
    "\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[labels])\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file='hmcnf1.jpg', show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=3500, batch_size=40000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "'''\n",
    "############################################################################\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，所以采用分离式\n",
    "###每一层的识别模型都是4层模型\n",
    "def g_sepHier1(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 10,srelu_size = 256,dropout_rate = 0.05):\n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.01\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    if 0:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=40000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "def getKerasResnetRVL(x,enc,saveName):\n",
    "    model_name = saveName \n",
    "    model = keras.models.load_model(model_name)\n",
    "    y= model.predict([x], batch_size=2560)\n",
    "    nSamples = y.shape[0]\n",
    "    ###需要将预测出的值，转换01整数,并转为数字式\n",
    "    for i in range(y.shape[0]):\n",
    "        tmp = y[i]\n",
    "        index=  np.argmax(tmp)\n",
    "        y[i] = [0]*y.shape[1]\n",
    "        y[i,index]=1\n",
    "   \n",
    "\n",
    "    ###  \n",
    "    y= enc.inverse_transform(y)\n",
    "    y= y.reshape(-1,nSamples)[0]\n",
    "    \n",
    "    \n",
    "    return \n",
    "\n",
    "def string2int(inputString):\n",
    "     #print(inputString)\n",
    "     tmp = 0\n",
    "     try:\n",
    "         strTmp=[str(ord(x)) for x in inputString]\n",
    "         tmp=tmp.join(strTmp)\n",
    "         tmp = float(tmp)/(len(inputString)*128)\n",
    "     except:\n",
    "         #print(inputString)\n",
    "         strTmp = inputString\n",
    "         tmp= \"0\"\n",
    "         tmp = 0\n",
    "     return tmp\n",
    " ## 根据经验以及最佳正确率的合并方法\n",
    " #第一次合并为0,1的合并\n",
    "\n",
    "########################################################################################################################\n",
    "##手工确定层次结构，以前测试时候为5层，根据论文为9层\n",
    "def convertY2Hieral(y):\n",
    "    #mat2acc\n",
    "    # [[0.914 0.009 0.017 0.007 0.032 0.    0.    0.    0.   ]\n",
    "    # [0.027 0.984 0.006 0.007 0.018 0.    0.    0.    0.   ]\n",
    "    # [0.02  0.006 0.972 0.    0.011 0.    0.    0.    0.   ]\n",
    "    # [0.036 0.002 0.    0.986 0.014 0.    0.002 0.    0.   ]\n",
    "    # [0.003 0.    0.    0.    0.925 0.    0.    0.    0.   ]\n",
    "    # [0.    0.    0.    0.    0.    1.    0.005 0.    0.   ]\n",
    "    # [0.    0.    0.    0.    0.    0.    0.993 0.    0.004]\n",
    "    # [0.    0.    0.    0.    0.    0.    0.    0.996 0.   ]\n",
    "    # [0.    0.    0.004 0.    0.    0.    0.    0.004 0.996]]\n",
    "    \n",
    "    \n",
    "    #hierarchy = [2,4,6,8,9]\n",
    "   # labelDict = {\"0\":[\"01234\",\"0123\",\"012\",\"01\",\"0\"],\\\n",
    "   #               \"1\":[\"01234\",\"0123\",\"012\",\"01\",\"1\"],\\\n",
    "   #               \"2\":[\"01234\",\"0123\",\"012\",\"2\",\"2\"],\\\n",
    "   #               \"3\":[\"01234\",\"0123\",\"3\",\"3\",\"3\"],\\\n",
    "   #              \"4\":[\"01234\",\"4\",    \"4\",\"4\",\"4\"],\\\n",
    "   #              \"5\":[\"5678\",\"5\",     \"5\",\"5\",\"5\"],\\\n",
    "   #              \"6\":[\"5678\",\"678\",   \"67\",\"6\",\"6\"],\\\n",
    "   #              \"7\":[\"5678\",\"678\",   \"67\",\"7\",\"7\"],\\\n",
    "   #              \"8\":[\"5678\",\"678\",   \"8\",\"8\",\"8\"],\\\n",
    "   #               }\n",
    "    \n",
    "    hierarchy = [2,3,4,5,6,7,8,9]\n",
    "    labelDict = {\"0\":[\"01234\",        \"01234\",        \"01234\",   \"01234\",      \"0123\",\"012\",\"01\",\"0\"],\\\n",
    "                  \"1\":[\"01234\",        \"01234\",        \"01234\",  \"01234\",     \"0123\",\"012\",\"01\",\"1\"],\\\n",
    "                  \"2\":[\"01234\",          \"01234\",      \"01234\",  \"01234\",     \"0123\",\"012\",\"2\",\"2\"],\\\n",
    "                  \"3\":[\"01234\",         \"01234\",       \"01234\",  \"01234\",    \"0123\",\"3\",\"3\",\"3\"],\\\n",
    "                 \"4\":[\"01234\",          \"01234\",       \"01234\",  \"01234\" ,     \"4\", \"4\",\"4\",\"4\"],\\\n",
    "                 \"5\":[\"5678\",               \"5\",        \"5\" ,      \"5\",       \"5\", \"5\",\"5\",\"5\"],\\\n",
    "                 \"6\":[\"5678\",            \"678\",        \"6\",        \"6\",       \"6\", \"6\",\"6\",\"6\"],\\\n",
    "                 \"7\":[\"5678\",             \"678\",       \"78\",       \"7\",       \"7\", \"7\",\"7\",\"7\"],\\\n",
    "                 \"8\":[\"5678\",             \"678\",       \"78\" ,      \"8\",        \"8\", \"8\",\"8\",\"8\"],\\\n",
    "                  }\n",
    "    '''\n",
    "    hierarchy = [5,9]\n",
    "    labelDict = {\"0\":[\"01\",\"0\"],\\\n",
    "                  \"1\":[\"01\",\"1\"],\\\n",
    "                  \"2\":[\"2\",\"2\"],\\\n",
    "                  \"3\":[\"34\",\"3\"],\\\n",
    "                \"4\":[\"34\",\"4\"],\\\n",
    "                 \"5\":[\"56\",\"5\"],\\\n",
    "                 \"6\":[\"56\",\"6\"],\\\n",
    "                 \"7\":[\"78\",\"7\"],\\\n",
    "                 \"8\":[\"78\",\"8\"],\\\n",
    "                 }\n",
    "    '''\n",
    "\n",
    "    y1 = [list(labelDict[str(x)]) for x in y]\n",
    "   \n",
    "    #print(\"!!!y1.type:\", type(y1))\n",
    "    #print(y1[:2])\n",
    "    #y2 = [t1[0] for t1 in y1]\n",
    "    #print(len(y2))\n",
    "  \n",
    "\n",
    "    return y1,hierarchy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dbf652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T14:33:49.232503Z",
     "start_time": "2023-02-13T14:31:39.188869Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##主程序开始######################################################################################################################\n",
    "print(\"0.主程序开始，建立多层嵌套决策树模型，3080ti的GPU是AMD2400CPU 运算速度100倍\")\n",
    "print(\"0.这是简化程序，原始带有更多测试和原始模型的程序在mainTestCSVMLP3(hmcnf_keras).ipynb\")\n",
    "print(\"程序编号为0\")\n",
    "########################################################################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import pickle  \n",
    "\n",
    "\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，所以采用分离式\n",
    "###每一层的识别模型都是4层模型\n",
    "def sepHier1(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 10,srelu_size = 256,dropout_rate = 0.05):\n",
    "    \n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    featuresInput = layers.Input(shape=(features_size,))\n",
    "    features = layers.BatchNormalization()(featuresInput)\n",
    "    #features=featuresInput\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[featuresInput], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        #build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=160000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "print(\"0.主程序开始, 建立多层嵌套决策树模型,3080ti的GPU是AMD2400CPU 运算速度100倍\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "#from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "#policy = mixed_precision.Policy('mixed_float16')\n",
    "#mixed_precision.set_policy(policy)\n",
    "\n",
    " \n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "print(\"读取France数据并且把数据进行onehot处理\")\n",
    "\n",
    "#file1 = \"../trainData/france_0_allSamples1.csv\"\n",
    "file1 = \"../trainData/france_0_allSamples1_2slot.csv\"\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "h,w = xyData.shape\n",
    "#x = xyData[:,1:23]#简单处理与SUMO数据库一致\n",
    "x0rigin = xyData[:,1:w-1]#用所有的数据,第0列为vehID,不要\n",
    "y0rigin  = xyData[:,w-1]\n",
    "\n",
    "x0rigin[:,6] = [string2int(inputString) for inputString in x0rigin[:,6] ]#字符串vehLaneID 变为整数\n",
    "\n",
    "x0rigin =x0rigin.astype(np.float32)#GPU 加这个\n",
    "y0rigin =y0rigin.astype(np.int64)#GPU 加这个\n",
    "\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "x0,y0= ros.fit_resample(x0rigin , y0rigin)#对数据不平衡进行处理，保证样本数一致\n",
    "\n",
    "x0=x0.astype(np.float32)#GPU 加这个\n",
    "y0=y0.astype(np.int64)#GPU 加这个\n",
    "yl5 = y0\n",
    "print(\"x0.shape:\",x0.shape,\"y0.shape:\",y0.shape,\"y0.type:\", type(y0) )\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "###现在暂时不训练多层模型，只训练9label模型\n",
    "if 0:\n",
    "    print(\"训练4层, 9 label 模型\")\n",
    "    x=x0\n",
    "    y=yl5\n",
    "    x=x.astype(np.float32)#GPU 加这个\n",
    "    y=y.astype(np.int64)#GPU 加这个\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    \n",
    "    num_labels = 9 \n",
    "    nSamples,nFeatures =  x.shape\n",
    "    enc = OneHotEncoder()\n",
    "    y= y.reshape(nSamples,-1)\n",
    "    \n",
    "    print(\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    enc.fit(y)\n",
    "    yOneHot=enc.transform(y).toarray()\n",
    "    saveName = \"../trainedModes/model-9label-4lays-512nodes-2slots-gpu1.h5\"\n",
    "    if 0:\n",
    "        kerasModel3_5label = kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels,saveName)     \n",
    "    yKeras_5label=getKerasResnetRVL(x,enc,saveName)\n",
    "    \n",
    "    print('keras\\n')\n",
    "    mat1num = confusion_matrix(y, yKeras_5label)\n",
    "    mat2acc = confusion_matrix(y, yKeras_5label,normalize='pred')\n",
    "    print('mat1num\\n',mat1num)\n",
    "    print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "    \n",
    "    \n",
    "########################################################################################################################    \n",
    "########################################################################################################################   \n",
    "########################################################################################################################    \n",
    "########################################################################################################################    \n",
    "print(\"##############################################################################################################\")\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"1.接编号为0的主程序,根据基于正确率的聚类程序或者经验将底层类别归结到上一层的类别\")\n",
    "print(\"2.程序编号为0+\")  \n",
    " \n",
    "'''\n",
    "if 0:# 训练统合样式的HMCN-F多级模型\n",
    "    print(\"训练5hieral, 4层, 9 label 模型\")\n",
    "    x=x0\n",
    "    y=yl5\n",
    "    x=x.astype(np.float32)#GPU 加这个\n",
    "    y=y.astype(np.int64)#GPU 加这个\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    yH1 = convertY2Hieral(y)\n",
    "    hierarchy = [2,4,6,8,9]#5层，对于当前数据集已经足够了\n",
    "    \n",
    "    yH1= np.array(yH1)\n",
    "    \n",
    "    nSamples,nFeatures =  x.shape\n",
    "    enc = OneHotEncoder()\n",
    "    yH1= yH1.reshape(nSamples,-1)\n",
    "    #print(yH1[:3])\n",
    "    print(\"yH1.shape:\",yH1 .shape,\"yH1.type:\", type(yH1) )\n",
    "    enc.fit(yH1)\n",
    "    #print(enc.categories_,enc.get_feature_names())\n",
    "    yOneHot=enc.transform(yH1).toarray()\n",
    "    #print(yOneHot[:3])\n",
    "    \n",
    "    num_labels = yOneHot.shape[1] \n",
    "    print(num_labels)\n",
    "    saveName = \"../trainedModes/model-5hier-9label-5lays-128nodes-2slots-gpu1.h5\"\n",
    "    kerasModel4_5hier_9label = kerasFitAndSaveHierSimple4LikeResnet(x,yOneHot,num_labels,saveName)   \n",
    "'''    \n",
    "    \n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if 1:# 训练多级模型\n",
    "    print(\"训练分离式多级模型\")\n",
    "    \n",
    "    #准备字典，用于保存训练后的数据\"\n",
    "    xFloors=  dict()\n",
    "    yFloors =  dict()\n",
    "    xTestFloors =dict()\n",
    "    yTestFloors = dict()\n",
    "    modSaveNameFloors =dict()\n",
    "    encLevels= dict()\n",
    "    yKerasFloors = dict()\n",
    "    x=x0\n",
    "    y=yl5\n",
    "    x=x.astype(np.float32)#GPU 加这个\n",
    "    y=y.astype(np.int64)#GPU 加这个\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    print(y)\n",
    "    \n",
    "    #hierarchy = [2,4,6,8,9]\n",
    "    #hierarchy = [2,3,4,5,6,7,8,9]\n",
    "    yH1,hierarchy = convertY2Hieral(y)\n",
    "    \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, yH1, test_size=0.5, random_state=0)\n",
    "   \n",
    "    nSamples,nFeatures =  x_train.shape\n",
    "    \n",
    "    \n",
    "    numEpochs =30 #1500/60/60*5 = 2houer\n",
    "    \n",
    "    \n",
    "    for i in range(len(hierarchy)):\n",
    "        print(\"\\n\\n levelIndex\",i,\"nSamples,nFeatures\",x_train.shape)\n",
    "        levelIndex = i\n",
    "        numLayers = 4\n",
    "        enc = OneHotEncoder()\n",
    "        nSamples,nFeatures =  x_train.shape\n",
    "       \n",
    "            \n",
    "        yCurLayer1 = [t1[i] for t1 in y_train]\n",
    "        \n",
    "        yCurLayer1 = np.array(yCurLayer1)\n",
    "        print(\"yCurLayer1.shape:\",yCurLayer1.shape)\n",
    "        \n",
    "        yCurLayer1= yCurLayer1.reshape(nSamples,-1)\n",
    "        enc.fit(yCurLayer1)\n",
    "        \n",
    "        yOneHot=enc.transform(yCurLayer1).toarray()\n",
    "        print(enc.categories_,enc.get_feature_names())\n",
    "        print(yOneHot[:1])\n",
    "        \n",
    "        \n",
    "        num_labels = hierarchy[i] \n",
    "        print(\"num_labels:\", num_labels)\n",
    "        saveName = \"../trainedModes/modelSep-9level%d-%dlayer-2slots-gpu1.h5\" %(i,numLayers)\n",
    "        #saveName = \"../trainedModes/modelSep-2level%d-%dlayer-2slots-gpu1.h5\" %(i,numLayers)#基于拥堵定义的2层结构\n",
    "        print(saveName)\n",
    "        sepHier1(x_train,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs)\n",
    "        \n",
    "        encLevels[str(i)] = enc\n",
    "        xFloors[str(i)] = x_train\n",
    "        yFloors[str(i)] = yCurLayer1\n",
    "        \n",
    "        \n",
    "        nSamplesTest,nFeaturesT =  x_test.shape\n",
    "        yCurLayerTest = [t1[i] for t1 in y_test]\n",
    "        yCurLayerTest = np.array(yCurLayerTest)\n",
    "        yCurLayerTest= yCurLayerTest.reshape(nSamplesTest,-1)\n",
    "        \n",
    "        xTestFloors[str(i)] = x_test\n",
    "        yTestFloors[str(i)] = yCurLayerTest\n",
    "        modSaveNameFloors[str(i)] = saveName\n",
    "        \n",
    "    #######保存为pickle文件,用于后期的SUMO和数据分析\n",
    "\n",
    "    fpk=open('samples1.pkf','wb+')  \n",
    "    pickle.dump([xFloors,yFloors,modSaveNameFloors,encLevels,xTestFloors, yTestFloors],fpk)  \n",
    "    fpk.close() \n",
    "\n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "#####用现有训练模型进行预测\n",
    "\n",
    "fpk=open('samples1.pkf','rb')   \n",
    "[xFloors,yFloors,modSaveNameFloors,encLevels,xTestFloors, yTestFloors]=pickle.load(fpk)  \n",
    "fpk.close()  \n",
    "\n",
    "\n",
    "yKerasFloors = dict()\n",
    "\n",
    "for i in range(len(hierarchy)):\n",
    "        levelIndex = i\n",
    "        #x = xFloors[str(i)]\n",
    "        #yCurLayer1 =  yFloors[str(i)]\n",
    "        \n",
    "        x = xTestFloors[str(i)]\n",
    "        yCurLayer1 =  yTestFloors[str(i)]\n",
    "        \n",
    "        saveName =  modSaveNameFloors[str(i)] \n",
    "        enc = encLevels[str(i)]\n",
    "        yOneHot=enc.transform(yCurLayer1).toarray()\n",
    "        yPredict=getKerasResnetRVL(x,enc,saveName)\n",
    "        print(\"分离式多层识别结果:第%d层\\n\" %i)\n",
    "        mat1num = confusion_matrix(yCurLayer1,yPredict)\n",
    "        print(mat1num)\n",
    "        mat2acc = confusion_matrix(yCurLayer1,yPredict,normalize='pred')  \n",
    "        print(np.around(mat2acc , decimals=3))\n",
    "        yKerasFloors[str(i)] =  yPredict\n",
    "        \n",
    "        df = pd.DataFrame(np.around(mat2acc , decimals=3))\n",
    "        fs = \"test_mat2acc%d.csv\" %i\n",
    "        df.to_csv(fs,index= False, header= False)\n",
    "        \n",
    "fpk=open('samples2.pkf','wb+')  \n",
    "pickle.dump([xFloors,yFloors,modSaveNameFloors,encLevels,yKerasFloors,xTestFloors,yTestFloors],fpk)  \n",
    "fpk.close() \n",
    "\n",
    " \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8674b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T14:36:52.438549Z",
     "start_time": "2023-02-13T14:36:16.048928Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "########################################################################################################################\n",
    "print(\"1.接编号为0的主程序,先找出低概率样本，\")\n",
    "print(\"2.对较低概率的样本进行蒙特卡洛模拟分析，原始对应程序为mainSimSumoFranceDatra\")\n",
    "print(\"3.最终进行分析，程序编号为1\")\n",
    "print(\"3.最终进行分析，程序编号为1\")\n",
    "########################################################################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "#import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "      \n",
    "\n",
    "\n",
    "    \n",
    "########################################################################################################################\n",
    "print(\"1.1 主程序开始\")\n",
    "########################################################################################################################\n",
    "\n",
    "########################################################################################################################\n",
    "#####用现有训练模型进行预测\n",
    "\n",
    "fpk=open('samples2.pkf','rb')   \n",
    "[xFloors,yFloors,modSaveNameFloors,encLevels,yKerasFloors,xTestFloors,yTestFloors]=pickle.load(fpk)  \n",
    "fpk.close()  \n",
    "\n",
    "hierarchy=[2,3,4,5,6,7,8,9]\n",
    "for i in [7]:\n",
    "#for i in range(len(hierarchy)):\n",
    "        levelIndex = i \n",
    "        x = xTestFloors[str(i)]\n",
    "        yCurLayer1 =  yTestFloors[str(i)]\n",
    "        #yP = yKerasFloors[str(i)]\n",
    "        \n",
    "        modeSaveName = \"../trainedModes/modelSep-9level7-4layer-2slots-gpu1.h5\"\n",
    "        model = keras.models.load_model(modeSaveName)\n",
    "        yPredictOut= model.predict([x], batch_size=2560)\n",
    "        yPredictOut = np.around(yPredictOut , decimals=3)\n",
    "        #print(yPredictOut)\n",
    "        ymax1=np.max(yPredictOut,axis=1)\n",
    "        ymax2=np.argmax(yPredictOut,axis=1)\n",
    "        \n",
    "        index = np.where(ymax1<0.5)[0]#提取最大值小于0.95的例子\n",
    "        \n",
    "        ylowpraPredictNN=yPredictOut[index]#对较低概率的样本\n",
    "        xlowpra=x[index]\n",
    "        ylowpraLabel = yCurLayer1[index]\n",
    "        ylowPredictLabel = ymax2[index].reshape(-1,1)\n",
    "        \n",
    "        \n",
    "        print(\"xlowpra.shape\",xlowpra.shape)\n",
    "        \n",
    "        \n",
    "        fpk=open('lowprobSamples.pkf','wb+')  #只有底层（第7层）\n",
    "        pickle.dump([xlowpra,ylowpraLabel,ylowPredictLabel,ylowpraPredictNN],fpk)  \n",
    "        fpk.close() \n",
    "        \n",
    "        df = pd.DataFrame(xlowpra)\n",
    "        fs = \"lowprobSamplesX.csv\"\n",
    "        df.to_csv(fs,index= False, header= False)\n",
    "       \n",
    "        ylowPredictLabel = ymax2[index].reshape(-1,1)\n",
    "        \n",
    "        df = pd.DataFrame(np.concatenate([ylowpraLabel,ylowPredictLabel,ylowpraPredictNN],axis=1))\n",
    "        fs = \"lowprobSamplesY.csv\"\n",
    "        df.to_csv(fs,index= False, header=['ylowpraLabel','ylowPredictLabel','0','1','2','3','4','5','6','7','8'])\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c074866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T09:51:42.931165Z",
     "start_time": "2023-01-28T09:51:42.771943Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "print(\"接程序1: 对较低概率的样本进行蒙特卡洛模拟分析,原始对应程序为mainSimSumoFranceDatra\")\n",
    "print(\"因为配置失误，采用将低概率的样本进行保存为文件，然后再root用户下命令行模式用SUMO模拟（不使用conda）\")\n",
    "print(\"输出为sumoSimData？？？.csv,里面有每个样本的sumo输出，kerasNN输出以及原始的输入输出\")\n",
    "print(\"程序编号为2\")\n",
    "print(\"程序编号为2\")\n",
    "########################################################################################################################\n",
    "!python3 sumoSimByFrance.py#运行runSumoSimFun.py 中test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becda4ae-fe95-4636-9b74-0f775aec3d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "print(\"接程序2: 综合SUMO输出，对keras输出进行优化。程序输入为程序2的输出\")\n",
    "print(\"优化选择1.NN。 2 回归分析。3 概率分析。\")\n",
    "print(\"程序编号为3\")\n",
    "########################################################################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import pickle \n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "############################################################################\n",
    "############################################################################\n",
    "####建立NN模型，与sepHier2一样\n",
    "def sepHier2(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 100,srelu_size = 256,dropout_rate = 0.05):\n",
    "    \n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    featuresInput = layers.Input(shape=(features_size,))\n",
    "    features = layers.BatchNormalization()(featuresInput)\n",
    "    #features=featuresInput\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[featuresInput], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        #build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=20000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    \n",
    "    \n",
    "    ##\n",
    "    return build_model\n",
    "\n",
    "\n",
    "def dtFit(x,y):\n",
    "    str1=\"dtFitAndSave,用于决策树拟合和识别\"\n",
    "    \n",
    "    dt = tree.DecisionTreeClassifier(max_depth=7,min_samples_leaf=100)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    #data=tree.export_graphviz(dt, out_file=None,class_names=None,filled=True) \n",
    "    #graph = graphviz.Source(data)\n",
    "    #graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(\"纯决策树的识别\\n\",tmp1)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    #text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    return dt,yPredict\n",
    "########################################################################################################################   \n",
    "########################################################################################################################    \n",
    "print(\"##############################################################################################################\")\n",
    "\n",
    "print(\"程序编号为3.1，主程序开始运行\")\n",
    "\n",
    "####原始的keras训练数据中的低概率数据，阶段1\n",
    "#sample_name = 1(ID)+8(keyFeature)+40(otherVehcle)+6(keyFeatures)+40(otherVehs)+1(flag)= 96\n",
    "#xlowpra:x-name = 8(keyFeature)+40(otherVehcle)+6(keyFeatures)+40(otherVehs)= 94      \n",
    "fpk=open('lowprobSamples.pkf','rb')   \n",
    "[xlowpra,ylowpraLabel,ylowPredictLabel,ylowpraPredictNN]=pickle.load(fpk)  \n",
    "print(\"xlowpra\",xlowpra.shape)\n",
    "fpk.close()      \n",
    "\n",
    "\n",
    "\n",
    "####原始的keras训练数据，阶段2\n",
    "df = pd.read_csv('sumoSimData.csv', sep=',')\n",
    "\n",
    "print(\"sumoSimData.csv\",df.shape)\n",
    "print(\"sumoSimData.csv\",df.columns)\n",
    "numSamples,numFeatures = df.shape\n",
    "\n",
    "##['sampleIndex','outputAvgSpeed','originOutput','sumoOutputSpeedTag','kerasPredictLabel','smv1','smv2',\\\n",
    " ##                                              'NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8'])\n",
    "    \n",
    "\n",
    "sumoOutput='sumoOutputSpeedTag'\n",
    "yKerasOutput='kerasPredictLabel'\n",
    "originOutput ='originOutput'\n",
    "sumoOutList = ['smv1','smv2']\n",
    "outputListNN = ['NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8']\n",
    "outputAvgSpeed = 'outputAvgSpeed'\n",
    "\n",
    "df1 = df[ \"sampleIndex\"]\n",
    "lowprobIndex = df1.iloc[0:numSamples].to_numpy()\n",
    "lowproKerasStage1Input = xlowpra[lowprobIndex]\n",
    "#print(originLowproKerasInput[0:3].shape)\n",
    "#print(originLowproKerasInput[0:3])\n",
    "\n",
    "df1 = df[sumoOutput]\n",
    "x1 = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "df1 = df[yKerasOutput]\n",
    "x2 = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "df1 = df[outputListNN]\n",
    "x3 = df1.iloc[0:numSamples].to_numpy()\n",
    "\n",
    "df1 = df[outputAvgSpeed]\n",
    "x4 = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "df1 = df[sumoOutList]\n",
    "x5 = df1.iloc[0:numSamples].to_numpy()\n",
    "\n",
    "df1 = df[originOutput]\n",
    "y = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y)\n",
    "yOneHot=enc.transform(y).toarray()\n",
    "\n",
    "x = np.concatenate([x1,x2,x3,x4,x5],axis=1)#71%\n",
    "#x = np.concatenate([x2,x2,x3,x2,x2],axis=1)#56%\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "#print(y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"#############################\\n数据预处理\\n\")\n",
    "#数据预处理\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, yOneHot, test_size=0.5, random_state=0)\n",
    "   \n",
    "\n",
    "rN,cN= np.where(np.isnan(x))\n",
    "#print(rN,cN)\n",
    "#print(rN.shape)\n",
    "\n",
    "for i in range(rN.shape[0]):\n",
    "    x[rN[i],cN[i]] = 0\n",
    " \n",
    "\n",
    "################################################################################################################################\n",
    "################################################################################################################################ \n",
    "\n",
    "print(\"#############################\\n原生keras\\n\")\n",
    "if 0:\n",
    "    yPredict = x2\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(tmp1)\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "\n",
    "    score = accuracy_score(yPredict, y)\n",
    "    print(score) \n",
    "\n",
    "print(\"#############################\\nkerasNN\\n\")\n",
    "if 0:\n",
    "    print(x.shape)\n",
    "    print(yOneHot.shape)\n",
    "    num_labels = yOneHot.shape[1]\n",
    "    numLayers = 4\n",
    "    numEpochs = 1\n",
    "    saveName =\"../trainedModes/stage2_1.h5\";\n",
    "    levelIndex = 7\n",
    "\n",
    "    sepHier2(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs)\n",
    "\n",
    "\n",
    "    yPredict=getKerasResnetRVL(x,enc,saveName)\n",
    "\n",
    "\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(tmp1)\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "    score = accuracy_score(yPredict, y)\n",
    "    print(score)\n",
    "\n",
    "print(\"#############################\\n只训练最底层，9label,kerasNN\\n\")\n",
    "\n",
    "\n",
    "if 0:\n",
    "    print(x.shape)\n",
    "    print(yOneHot.shape)\n",
    "    num_labels = yOneHot.shape[1]\n",
    "    numLayers = 4\n",
    "    numEpochs = 1\n",
    "    saveName =\"../trainedModes/stage2Modes_addingSumoFeatures-noHierachical.h5\";\n",
    "    levelIndex = 7\n",
    "\n",
    "    sepHier2(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs)\n",
    "\n",
    "\n",
    "    yPredict=getKerasResnetRVL(x,enc,saveName)\n",
    "\n",
    "\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(tmp1)\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "    score = accuracy_score(yPredict, y)\n",
    "    print(score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################################################################\n",
    "################################################################################################################################ \n",
    "if 0:\n",
    "    print(\"#############################\\n决策树\\n\")\n",
    "    dtFit(x,y)\n",
    "\n",
    "    print(\"#############################\\n逻辑回归\\n\")    \n",
    "    model = LogisticRegression()\n",
    "    model.fit(x,y)\n",
    "    yPredict = model.predict(x)\n",
    "\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(tmp1)\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "    score = accuracy_score(yPredict, y)\n",
    "    print(score) \n",
    "    print(\"#############################\\n贝叶斯高斯回归\\n\")\n",
    "    nb_cls = naive_bayes.GaussianNB().fit(x,y)\n",
    "    yPredict = nb_cls.predict(x) \n",
    "\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(tmp1)\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "    score = accuracy_score(yPredict, y)\n",
    "    print(score) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fpk=open('stage2-LowprobSamples-addingSumoFeatures-noHierachical.pkf','wb')  \n",
    "pickle.dump([df,x,y,yOneHot,x_train, x_test, y_train, y_test,enc,saveName],fpk)  \n",
    "fpk.close() \n",
    "\n",
    "\n",
    "################################################################################################################################\n",
    "################################################################################################################################   \n",
    "print(\"#############################\\n加入新特征SUMO,对低概率样本重新训练多级独立kerasNN\\n\")\n",
    "##分层重新训练，加入特征SMV1,SMV2\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，所以采用分离式\n",
    "###每一层的识别模型都是4层模型\n",
    "def sepHier1(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 10,srelu_size = 256,dropout_rate = 0.05):\n",
    "    \n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    featuresInput = layers.Input(shape=(features_size,))\n",
    "    features = layers.BatchNormalization()(featuresInput)\n",
    "    #features=featuresInput\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[featuresInput], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    if 0:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        #build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=160000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "############################################################################\n",
    "x = np.concatenate([lowproKerasStage1Input,x1,x2,x3,x4,x5],axis=1)#71%\n",
    "y9Label = y \n",
    "print(x.shape)\n",
    "rN,cN= np.where(np.isnan(x))\n",
    "#print(rN,cN)\n",
    "#print(rN.shape)\n",
    "\n",
    "for i in range(rN.shape[0]):\n",
    "    x[rN[i],cN[i]] = 0\n",
    "    \n",
    "if 1:\n",
    "    #准备字典，用于保存训练后的数据\"\n",
    "    xFloors=  dict()\n",
    "    yFloors =  dict()\n",
    "    xTestFloors =dict()\n",
    "    yTestFloors = dict()\n",
    "    modSaveNameFloors =dict()\n",
    "    encLevels= dict()\n",
    "    yKerasFloors = dict()\n",
    "\n",
    "    y=y9Label.reshape(1,-1)[0]\n",
    "    x=x.astype(np.float32)#GPU 加这个\n",
    "    y=y.astype(np.int64)#GPU 加这个\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    print(y)\n",
    "    \n",
    "    #hierarchy = [2,4,6,8,9]\n",
    "    #hierarchy = [2,3,4,5,6,7,8,9]\n",
    "    yH1,hierarchy = convertY2Hieral(y)\n",
    "    \n",
    "    print(yH1[0:2])\n",
    "    \n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, yH1, test_size=0.01, random_state=0)\n",
    "   \n",
    "    nSamples,nFeatures =  x_train.shape\n",
    "    \n",
    "    \n",
    "    numEpochs =3000 #1500/60/60*5 = 2houer\n",
    "    \n",
    "    \n",
    "    #for i in range(len(hierarchy)):#因为上层低概率样本少，所以从第6层开始\n",
    "    for i in [6,7]:\n",
    "        print(\"\\n\\nlevelIndex:\",i,\"\\nnSamples,nFeatures:\",x_train.shape)\n",
    "        levelIndex = i\n",
    "        numLayers = 4\n",
    "        enc = OneHotEncoder()\n",
    "        nSamples,nFeatures =  x_train.shape\n",
    "       \n",
    "            \n",
    "        yCurLayer1 = [t1[i] for t1 in y_train]\n",
    "        \n",
    "        yCurLayer1 = np.array(yCurLayer1)\n",
    "        print(\"yCurLayer1.shape:\",yCurLayer1.shape)\n",
    "        \n",
    "        yCurLayer1= yCurLayer1.reshape(nSamples,-1)\n",
    "        enc.fit(yCurLayer1)\n",
    "        \n",
    "        yOneHot=enc.transform(yCurLayer1).toarray()\n",
    "        print(\"encInfo:\",enc.categories_,enc.get_feature_names())\n",
    "        print(\"enc.get_feature_names().shape,numLabel:\",enc.get_feature_names().shape[0])\n",
    "        print(\"yOneHot[:1]:\",yOneHot[:1])\n",
    "        \n",
    "        \n",
    "        #num_labels = hierarchy[i] #低概率样本下错误\n",
    "        num_labels = enc.get_feature_names().shape[0] #低概率样本下错误\n",
    "        print(\"num_labels:\", num_labels)\n",
    "        saveName = \"../trainedModes/modelSepStage2-9level%d-%dlayer-2slots-gpu1.h5\" %(i,numLayers)\n",
    "        #saveName = \"../trainedModes/modelSep-2level%d-%dlayer-2slots-gpu1.h5\" %(i,numLayers)#基于拥堵定义的2层结构\n",
    "        print(\"saveName:\",saveName)\n",
    "        \n",
    "        sepHier1(x_train,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs)\n",
    "        \n",
    "        encLevels[str(i)] = enc\n",
    "        xFloors[str(i)] = x_train\n",
    "        yFloors[str(i)] = yCurLayer1\n",
    "        \n",
    "        \n",
    "        nSamplesTest,nFeaturesT =  x_test.shape\n",
    "        yCurLayerTest = [t1[i] for t1 in y_test]\n",
    "        yCurLayerTest = np.array(yCurLayerTest)\n",
    "        yCurLayerTest= yCurLayerTest.reshape(nSamplesTest,-1)\n",
    "        \n",
    "        xTestFloors[str(i)] = x_test\n",
    "        yTestFloors[str(i)] = yCurLayerTest\n",
    "        modSaveNameFloors[str(i)] = saveName\n",
    "        \n",
    "    #######保存为pickle文件,用于后期的SUMO和数据分析\n",
    "\n",
    "    fpk=open('stage2-LowprobSamples-addingSumoFeatures-Hierachical.pkf','wb+')  \n",
    "    pickle.dump([xFloors,yFloors,modSaveNameFloors,encLevels,xTestFloors, yTestFloors],fpk)  \n",
    "    fpk.close() \n",
    "\n",
    "return\n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "#####用现有训练模型进行预测\n",
    "\n",
    "fpk=open('stage2-LowprobSamples-addingSumoFeatures-Hierachical.pkf','rb')   \n",
    "[xFloors,yFloors,modSaveNameFloors,encLevels,xTestFloors, yTestFloors]=pickle.load(fpk)  \n",
    "fpk.close()  \n",
    "\n",
    "\n",
    "yKerasFloors = dict()\n",
    "\n",
    "for i in range(len(hierarchy)):\n",
    "        levelIndex = i\n",
    "        #x = xFloors[str(i)]\n",
    "        #yCurLayer1 =  yFloors[str(i)]\n",
    "        \n",
    "        x = xTestFloors[str(i)]\n",
    "        yCurLayer1 =  yTestFloors[str(i)]\n",
    "        \n",
    "        saveName =  modSaveNameFloors[str(i)] \n",
    "        enc = encLevels[str(i)]\n",
    "        yOneHot=enc.transform(yCurLayer1).toarray()\n",
    "        yPredict=getKerasResnetRVL(x,enc,saveName)\n",
    "        print(\"分离式多层识别结果:第%d层\\n\" %i)\n",
    "        mat1num = confusion_matrix(yCurLayer1,yPredict)\n",
    "        print(mat1num)\n",
    "        mat2acc = confusion_matrix(yCurLayer1,yPredict,normalize='pred')  \n",
    "        print(np.around(mat2acc , decimals=3))\n",
    "        yKerasFloors[str(i)] =  yPredict\n",
    "        \n",
    "        df = pd.DataFrame(np.around(mat2acc , decimals=3))\n",
    "        fs = \"test_mat2acc%d.csv\" %i\n",
    "        df.to_csv(fs,index= False, header= False)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe44f65-7db7-4ac2-83e6-4d0844f99241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################################################################\n",
    "print(\"接程序2: 综合SUMO输出，对keras输出进行优化。程序输入为程序2的输出\")\n",
    "print(\"优化选择1.NN。 2 回归分析。3 概率分析。\")\n",
    "print(\"程序编号为3.2,用层次模型递推概率计算\")\n",
    "########################################################################################################################\n",
    "\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "print(\"#################################################################################\")\n",
    "print(\"Hierach Keras\")\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z - np.max(z))\n",
    "    return e_z / e_z.sum(axis=0)\n",
    "def simpleProb1(z):\n",
    "    return z / z.sum(axis=0)\n",
    "\n",
    "    \n",
    "\n",
    "fpk=open('lowprobSamples.pkf','rb')   \n",
    "[xlowpra,ylowpraLabel,ylowPredictLabel,ylowpraPredictNN]=pickle.load(fpk)  #只是第7层的输出\n",
    "fpk.close()  \n",
    "\n",
    "\n",
    "numLayers = 4\n",
    "yKerasNNout = dict()\n",
    "hierarchy=[2,3,4,5,6,7,8,9]\n",
    "\n",
    "for i in range(len(hierarchy)):\n",
    "    saveName = \"../trainedModes/modelSep-9level%d-%dlayer-2slots-gpu1.h5\" %(i,numLayers)\n",
    "    #计算当前层次下的每个样本的正确率\n",
    "    model_name = saveName \n",
    "    model2 = keras.models.load_model(model_name)\n",
    "    level = str(i)\n",
    "    yKerasNNout[level]= model2.predict([xlowpra], batch_size=2560)\n",
    "    #print(yKerasNNout[level].shape)\n",
    "\n",
    "   # hierarchy = [2,3,4,5,6,7,8,9]\n",
    "    #labelDict = {\"0\":[\"01234\",        \"01234\",        \"01234\",   \"01234\",      \"0123\",\"012\",\"01\",\"0\"],\\\n",
    "    #              \"1\":[\"01234\",        \"01234\",        \"01234\",  \"01234\",     \"0123\",\"012\",\"01\",\"1\"],\\\n",
    "    #              \"2\":[\"01234\",          \"01234\",      \"01234\",  \"01234\",     \"0123\",\"012\",\"2\",\"2\"],\\\n",
    "    #              \"3\":[\"01234\",         \"01234\",       \"01234\",  \"01234\",    \"0123\",\"3\",\"3\",\"3\"],\\\n",
    "    #             \"4\":[\"01234\",          \"01234\",       \"01234\",  \"01234\" ,     \"4\", \"4\",\"4\",\"4\"],\\\n",
    "    #             \"5\":[\"5678\",               \"5\",        \"5\" ,      \"5\",       \"5\", \"5\",\"5\",\"5\"],\\\n",
    "    #             \"6\":[\"5678\",            \"678\",        \"6\",        \"6\",       \"6\", \"6\",\"6\",\"6\"],\\\n",
    "    #             \"7\":[\"5678\",             \"678\",       \"78\",       \"7\",       \"7\", \"7\",\"7\",\"7\"],\\\n",
    "    #             \"8\":[\"5678\",             \"678\",       \"78\" ,      \"8\",        \"8\", \"8\",\"8\",\"8\"],\\\n",
    "    \n",
    "    \n",
    "numSamples = xlowpra.shape[0]\n",
    "yHierOut = dict()\n",
    "\n",
    "\n",
    "yL1HierOutLabel=[]\n",
    "yL1KeralOutLabel=[]\n",
    "\n",
    "yL2HierOutLabel=[]\n",
    "yL2KeralOutLabel=[]\n",
    "\n",
    "yL3HierOutLabel=[]\n",
    "yL3KeralOutLabel=[]\n",
    "\n",
    "yL4HierOutLabel=[]\n",
    "yL4KeralOutLabel=[]\n",
    "\n",
    "yL5HierOutLabel=[]\n",
    "yL5KeralOutLabel=[]\n",
    "\n",
    "yL6HierOutLabel=[]\n",
    "yL6KeralOutLabel=[]\n",
    "\n",
    "yL7HierOutLabel=[]\n",
    "yL7KeralOutLabel=[]\n",
    "\n",
    "\n",
    "yL7HierOutNN=[]\n",
    "yL7KeralOutNN=[]\n",
    "for i in range(numSamples):\n",
    "    \n",
    "    \n",
    "    originKerasTrainInput = xlowpra[i]\n",
    "    \n",
    "    #第0层，输出为2类，['01234','5678']\n",
    "    nn0 = yKerasNNout['0'][i]\n",
    "    #第1层，输出为3,['01234','5','678']\n",
    "    nn1 = yKerasNNout['1'][i]\n",
    "    nn0to1 = np.insert(nn0,1,nn0[1])\n",
    "    nn1A = simpleProb1(nn0to1*nn1)\n",
    "    \n",
    "    youtTmp1 = np.argmax(nn1,axis=0)\n",
    "    youtTmp2 = np.argmax(nn1A,axis=0)\n",
    "    yL1KeralOutLabel.append(youtTmp1)\n",
    "    yL1HierOutLabel.append(youtTmp2)\n",
    "    \n",
    "    #nn1 = nn1A #从上到下链式更新，如果去掉就不是链式更新\n",
    "    \n",
    "    #第2层，输出为4,['01234','5','6','78']\n",
    "    nn2 = yKerasNNout['2'][i]\n",
    "    nn1to2 = np.insert(nn1,2,nn1[2])\n",
    "    nn2A = simpleProb1(nn1to2*nn2)\n",
    "   \n",
    "    youtTmp1 = np.argmax(nn2,axis=0)\n",
    "    youtTmp2 = np.argmax(nn2A,axis=0)\n",
    "    yL2KeralOutLabel.append(youtTmp1)\n",
    "    yL2HierOutLabel.append(youtTmp2)\n",
    "    \n",
    "    \n",
    "    #nn2 = nn2A #从上到下链式更新，如果去掉就不是链式更新\n",
    "    \n",
    "    #第3层，输出为5,['01234','5','6','7','8']\n",
    "    nn3 = yKerasNNout['3'][i]\n",
    "    nn2to3 = np.insert(nn2,3,nn2[3])\n",
    "    nn3A = simpleProb1(nn2to3*nn3)\n",
    "   \n",
    "    youtTmp1 = np.argmax(nn3,axis=0)\n",
    "    youtTmp2 = np.argmax(nn3A,axis=0)\n",
    "    yL3KeralOutLabel.append(youtTmp1)\n",
    "    yL3HierOutLabel.append(youtTmp2)\n",
    "    \n",
    "    #nn3 = nn3A #从上到下链式更新，如果去掉就不是链式更新\n",
    "    \n",
    "    \n",
    "    #第4层，输出为6,['0123','4','5','6','7','8']\n",
    "    nn4 = yKerasNNout['4'][i]\n",
    "    nn3to4 = np.insert(nn3,0,nn3[0])\n",
    "    nn4A = simpleProb1(nn3to4*nn4)\n",
    "   \n",
    "    youtTmp1 = np.argmax(nn4,axis=0)\n",
    "    youtTmp2 = np.argmax(nn4A,axis=0)\n",
    "    yL4KeralOutLabel.append(youtTmp1)\n",
    "    yL4HierOutLabel.append(youtTmp2)\n",
    "    \n",
    "    #nn4 = nn4A #从上到下链式更新，如果去掉就不是链式更新\n",
    "    \n",
    "    #第5层，输出为7,['012','3','4','5','6','7','8']\n",
    "    nn5 = yKerasNNout['5'][i]\n",
    "    nn4to5 = np.insert(nn4,0,nn4[0])\n",
    "    nn5A = simpleProb1(nn4to5*nn5)\n",
    "   \n",
    "    youtTmp1 = np.argmax(nn5,axis=0)\n",
    "    youtTmp2 = np.argmax(nn5A,axis=0)\n",
    "    yL5KeralOutLabel.append(youtTmp1)\n",
    "    yL5HierOutLabel.append(youtTmp2)\n",
    "    \n",
    "    #nn5 = nn5A #从上到下链式更新，如果去掉就不是链式更新\n",
    "    \n",
    "    \n",
    "    #第6层，输出为7,['01','2','3','4','5','6','7','8']\n",
    "    nn6 = yKerasNNout['6'][i]\n",
    "    nn5to6 = np.insert(nn5,0,nn5[0])\n",
    "    nn6A = simpleProb1(nn5to6*nn6)\n",
    "    #nn6A = softmax(nn5to6*nn6)\n",
    "   \n",
    "    youtTmp1 = np.argmax(nn6,axis=0)\n",
    "    youtTmp2 = np.argmax(nn6A,axis=0)\n",
    "    yL6KeralOutLabel.append(youtTmp1)\n",
    "    yL6HierOutLabel.append(youtTmp2)\n",
    "    \n",
    "    #nn6 = nn6A #从上到下链式更新，如果去掉就不是链式更新\n",
    "    \n",
    "    #最低层7= len([2,3,4,5,6,7,8,9])-1\n",
    "    nn7 = yKerasNNout['7'][i]\n",
    "    nn6to7= np.insert(nn6,0,nn6[0])\n",
    "    nn7A= simpleProb1(nn6to7*nn7)\n",
    "    #nn7A = softmax(nn6to7*nn7)\n",
    "    \n",
    "    \n",
    "    youtTmp1 = np.argmax(nn7,axis=0)\n",
    "    youtTmp2 = np.argmax(nn7A,axis=0)\n",
    "    \n",
    "    yL7KeralOutLabel.append(youtTmp1)\n",
    "    yL7HierOutLabel.append(youtTmp2)\n",
    "   \n",
    "    \n",
    "    #print(np.round(nn6,3))\n",
    "    #print(np.round(nn7,3))\n",
    "    #print(np.round(nn6to7,3))\n",
    "    #print(np.round(softmax(nn6to7*nn7),3))\n",
    "    #print(np.round(simpleProb1(nn6to7*nn7),3))\n",
    "    \n",
    "#print(yL7HierOutLabel)\n",
    "#print(yL7KeralOutLabel)\n",
    "#print(yL7HierOutNN)\n",
    "#print(yL7KeralOutNN)\n",
    " \n",
    "\n",
    "y =ylowpraLabel.astype(int)\n",
    "\n",
    "yPredict = np.array(yL7HierOutLabel).reshape(-1,1)\n",
    "\n",
    "tmp1 = classification_report(y,yPredict)\n",
    "mat1num = confusion_matrix(y,yPredict)\n",
    "mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "score = accuracy_score(yPredict, y)\n",
    "print(score)\n",
    "\n",
    "\n",
    "yPredict = np.array(yL7KeralOutLabel).reshape(-1,1)\n",
    "\n",
    "tmp1 = classification_report(y,yPredict)\n",
    "mat1num = confusion_matrix(y,yPredict)\n",
    "mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "score = accuracy_score(yPredict, y)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a367eac-c740-4be5-8f8a-5bdba5969e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################   \n",
    "########################################################################################################################   \n",
    "import scipy.interpolate as si\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"##############################################################################################################\")\n",
    "\n",
    "print(\"程序编号为3.3，手动权值分配\")\n",
    "\n",
    "df = pd.read_csv('sumoSimData.csv', sep=',')\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "numSamples,numFeatures = df.shape\n",
    "\n",
    "##['sampleIndex','outputAvgSpeed','originOutput','sumoOutputSpeedTag','kerasPredictLabel','smv1','smv2',\\\n",
    " ##                                              'NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8'])\n",
    "sumoOutput='sumoOutputSpeedTag'\n",
    "yKerasOutput='kerasPredictLabel'\n",
    "originOutput ='originOutput'\n",
    "sumoOutList = ['smv1','smv2']\n",
    "outputListNN = ['NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8']\n",
    "\n",
    "\n",
    "\n",
    "df1 = df[originOutput]\n",
    "yo = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "df1 = df[sumoOutput]\n",
    "x1 = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "df1 = df[yKerasOutput]\n",
    "x2 = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "df1 = df[outputListNN]\n",
    "x3 = df1.iloc[0:numSamples].to_numpy()\n",
    "\n",
    "x = np.concatenate([x1,x2,x3],axis=1)\n",
    "print(x)\n",
    "\n",
    "\n",
    "print(\"#############################\\n手动给权值1,2\\n\")\n",
    "\n",
    "manualOut = np.zeros((x3.shape[0],1))\n",
    "manualOut2 = np.zeros((x3.shape[0],1))\n",
    "#for i in range(10):\n",
    "for i in range(x3.shape[0]):\n",
    "    #print(i)\n",
    "    nn1 = x3[i]\n",
    "    sumoOut = min(8,x1[i][0]+2)\n",
    "    kerasOut = x2[i][0]\n",
    "    originOut = yo[i][0]\n",
    "\n",
    "    #手动给权值1,方法1用插值模型，似乎效果不好\n",
    "    xIntp=[0,sumoOut,min(8,sumoOut+1),min(8,sumoOut+2),min(8,sumoOut+3),9]\n",
    "    yIntp =[0,1,0.5,0.4,0.3,0]\n",
    "    \n",
    "    xIntp=[-9,sumoOut-1,sumoOut,sumoOut+1,9]\n",
    "    yIntp =[0.5,0.8,1,0.8,0.5]\n",
    "    f = si.interp1d(xIntp,  yIntp,kind=1)\n",
    "    xi = [0,1,2,3,4,5,6,7,8]\n",
    "    p= f(xi)\n",
    "    yTmp = np.multiply(p,nn1)\n",
    "    finalIndex = np.argmax(yTmp)\n",
    "    manualOut[i] = finalIndex\n",
    "    \n",
    "    #print('sumoOut','kerasOut','originOut','finalIndex')\n",
    "    #print(sumoOut,kerasOut,originOut,finalIndex)\n",
    "    #print(\"nn1:\",nn1)\n",
    "    #print(\"p  :\",np.round(p,decimals=3))\n",
    "    #print(\"y3 :\",np.round(y3,decimals=3))\n",
    "    \n",
    "    #手动给权值2,方法2用集团模型\n",
    "    if sumoOut <= 1:\n",
    "        p = [1,1,1,1,1,1,0,0,0]\n",
    "        yTmp = np.multiply(p,nn1)\n",
    "        finalIndex = np.argmax(yTmp)\n",
    "        manualOut2[i] = finalIndex\n",
    "    \n",
    "    if sumoOut > 1:\n",
    "        p = [0.0,0,0,0,0,0,1,1,1]\n",
    "        yTmp = np.multiply(p,nn1)\n",
    "        finalIndex = np.argmax(yTmp)\n",
    "        manualOut2[i] = finalIndex\n",
    "\n",
    "print(\"#############################\\n手动给权值1结果\\n\")\n",
    "print(yo.shape)\n",
    "print(manualOut.shape)\n",
    "tmp1 = classification_report(yo,manualOut)\n",
    "mat1num = confusion_matrix(yo,manualOut)\n",
    "mat2acc = confusion_matrix(yo,manualOut,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "score = accuracy_score(manualOut, yo)\n",
    "print(score) \n",
    "\n",
    "\n",
    "print(\"#############################\\n手动给权值2结果\\n\")\n",
    "print(yo.shape)\n",
    "print(manualOut2.shape)\n",
    "tmp1 = classification_report(yo,manualOut2)\n",
    "mat1num = confusion_matrix(yo,manualOut2)\n",
    "mat2acc = confusion_matrix(yo,manualOut2,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "score = accuracy_score(manualOut2, yo)\n",
    "print(score) \n",
    "\n",
    "\n",
    "print(\"#############################\\n原生keras\\n\")\n",
    "yPredict = x2\n",
    "tmp1 = classification_report(yo,yPredict)\n",
    "mat1num = confusion_matrix(yo,yPredict)\n",
    "mat2acc = confusion_matrix(yo,yPredict,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "score = accuracy_score(yPredict, yo)\n",
    "print(score) \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "print(\"############################################模拟退火\")\n",
    "print(\"############################################https://www.jb51.net/article/269941.htm\")\n",
    "#https://blog.csdn.net/ljyljyok/article/details/100552618\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import dual_annealing\n",
    "\n",
    "def objective(x,*args):\n",
    "    x1,x2,x3,yo = args\n",
    "    manualOut = np.zeros((x3.shape[0],1))\n",
    "    for i in range(x3.shape[0]):\n",
    "        #print(i)\n",
    "        nn1 = x3[i]\n",
    "        sumoOut = x1[i][0]\n",
    "        kerasOut = x2[i][0]\n",
    "        originOut = yo[i][0]\n",
    "\n",
    "        \n",
    "\n",
    "        xIntp=[-9,sumoOut-x[0],sumoOut,sumoOut+x[1],9]\n",
    "        yIntp =[0.5,x[2],1,x[2],0.5]\n",
    "        f = si.interp1d(xIntp,  yIntp,kind=1)\n",
    "        xi = [0,1,2,3,4,5,6,7,8]\n",
    "        p= f(xi)\n",
    "        yTmp = np.multiply(p,nn1)\n",
    "        finalIndex = np.argmax(yTmp)\n",
    "        manualOut[i] = finalIndex\n",
    "\n",
    "    score = accuracy_score(manualOut, yo)\n",
    "    \n",
    "    return -score\n",
    "\n",
    "\n",
    "\n",
    "#args1 = (x1,x2,x3,yo)\n",
    "#x0 = [1,1,0.3]\n",
    "bounds1 = ((0, 4), (0, 4),(0.01, 0.9))\n",
    "\n",
    "#constraints = {'type': 'ineq', 'fun': cons}\n",
    "\n",
    "#res = minimize(objective, x0, args=args1,method='SLSQP',bounds=bounds1)\n",
    "#print(res.fun)\n",
    "#print(res.success)\n",
    "#print(res.x)\n",
    "#https://vimsky.com/zh-tw/examples/usage/python-scipy.optimize.dual_annealing.html\n",
    "args1 = (x1,x2,x3,yo)\n",
    "x0 = [1,1,0.3]\n",
    "bounds1 = [[0, 4], [0, 4],[0.01, 0.9]]\n",
    "res = dual_annealing(objective,bounds1,x0=x0, args=args1)\n",
    "print(res.fun)\n",
    "print(res.success)\n",
    "print(res.x)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add02d9c-af03-4a00-8629-42b8a751033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "print(\"辅助程序 对模拟后的数据进行分析，计算正确率\")\n",
    "########################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"对于低概率样本的识别结果，采用keras和MCS的识别结果对比\")\n",
    "file1 = \"./data-Copy1.csv\"\n",
    "xyDataTmp = pd.read_csv(file1,index_col=0)\n",
    "\n",
    "print(xyDataTmp.head(3))\n",
    "print(xyDataTmp.info())\n",
    "\n",
    "file1 = \"。./trainData/france_0_allSamples1.csv\"\n",
    "xyOrigin = pd.read_csv(file1,index_col=0)\n",
    "\n",
    "originlabel =  xyDataTmp.iloc[:,1].to_numpy()\n",
    "keraslabel =   xyDataTmp.iloc[:,2].to_numpy()      \n",
    "mcslabel =     xyDataTmp.iloc[:,3].to_numpy()\n",
    "\n",
    "#\n",
    "    \n",
    "print('\\norigin_mcs')\n",
    "mat1num = confusion_matrix(originlabel, mcslabel)\n",
    "mat2acc = confusion_matrix(originlabel, mcslabel,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "      \n",
    "print('\\nmcs_keras')\n",
    "mat1num = confusion_matrix(mcslabel, keraslabel)\n",
    "mat2acc = confusion_matrix(mcslabel, keraslabel,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "\n",
    "print('\\norgin_keras')\n",
    "mat1num = confusion_matrix(originlabel, keraslabel)\n",
    "mat2acc = confusion_matrix(originlabel ,keraslabel,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))      \n",
    "\n",
    "##用于分析实际标记类别大于预测标记类别\n",
    "def analyzing1(tmp, xyDataTmp,xyOrigin): \n",
    "    dfTmp1 = xyDataTmp[tmp]\n",
    "    #print(dfTmp1.head(5))\n",
    "    \n",
    "    \n",
    "    \n",
    "    df2 =  xyOrigin.iloc[dfTmp1.originIndex,:]   \n",
    "    plt.show()\n",
    "    df2[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    #print(df2.info())\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    \n",
    "    df2.to_csv(\"tmpForAnalyzing.csv\")\n",
    "    \n",
    "    tmp1 = df2['redLightTime'] - df2['arriveTime2'] >1.5 #红灯时间大于到达时间\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 = df2[df2['redLightTime'] - df2['arriveTime2'] >1.5] #红灯时间大于到达时间 ,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing3.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    \n",
    "   \n",
    "    tmp1 = df2['speed'] < 5/3.6 #本身速度就小于5/3.6\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df2['speed'] > 5/3.6 #本身速度就小于5/3.6,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    df3.to_csv(\"tmpForAnalyzing4.csv\")\n",
    "    \n",
    "    tmp1 = df2['redLightTime'] - df2['arriveTime2'] >1.5  #红灯时间大于到达时间\n",
    "    tmp1 = tmp1 | (df2['speed'] < 5/3.6) #本身速度就小于5/3.6\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"红灯时间大于到达时间  or 本身速度就小于5/3.6,df3 shape:\",df3.shape,\"占输入样本比例为:\",df3.shape[0]/df2.shape[0])\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    df3.to_csv(\"tmpForAnalyzing5.csv\")\n",
    "    \n",
    "    \n",
    "    tmp1 = df2['arriveTime2'] - df2['redLightTime'] >0 #到达时间大于红灯时间\n",
    "    tmp1 = tmp1 & (df2['speed'] > 5/3.6) #本身速度就大于于5/3.6\n",
    "    tmp1 = tmp1 & (df2['vehPos_2'] > 0) #\n",
    "    tmp1 = tmp1 & (df2['vehSpeed_2'] < 5/3.6) #\n",
    "    tmp1 = tmp1 & (df2['vehPos_3'] >0) #\n",
    "    tmp1 = tmp1 & (df2['vehSpeed_3'] <5/3.6) #\n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"到达时间大于红灯时间  and 本身速度就大于5/3.6,df3 shape:\",df3.shape,\"占输入样本比例为:\",df3.shape[0]/df2.shape[0])\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    df3.to_csv(\"tmpForAnalyzing6.csv\")\n",
    "\n",
    "def extractStillVeh2(df):\n",
    "    df=df.rename(columns={'ArrTimeDivRedTime':'numStillVeh'})\n",
    "    df=df.rename(columns={'lanAvgSpeed':'predictStats'})\n",
    "    df['numStillVeh'] = 0\n",
    "    df['predictStats'] = \"unknown\"\n",
    "    for i in range(df.shape[0]):\n",
    "        numStillVeh = 0\n",
    "        tmp = df.iloc[i]\n",
    "        redTime = tmp.iloc[0]\n",
    "        vPosObj = tmp.iloc[1]\n",
    "        predictStats = -1\n",
    "\n",
    "        for j in range(20):\n",
    "           \n",
    "            vehPos = tmp.iloc[2*j+8]\n",
    "            vehVeh = tmp.iloc[2*j+1+8]\n",
    "            \n",
    "            if vehPos >0 and vehVeh<5/3.6:#经验数据,参数\n",
    "                numStillVeh = numStillVeh + 1\n",
    "            elif vehPos >0 and  vPosObj > vehPos:\n",
    "                timeTmp1 =(vehPos-j*6.5)/(vehVeh+0.001)#经验公式，到固定位置后，启动需要的时间\n",
    "                if timeTmp1  < redTime +numStillVeh*1.5:\n",
    "                    numStillVeh = numStillVeh + 1\n",
    "\n",
    "            if vehPos >0 and vPosObj == vehPos and vehVeh<5/3.6:\n",
    "                predictStats = \"stop\"#目标车要听停止\n",
    "               \n",
    "\n",
    "            if vehPos >0 and vPosObj == vehPos and vehVeh>5/3.6 :    \n",
    "                timeTmp1 =(vehPos-j*6.5)/(vehVeh+0.001)#经验公式，到固定位置后需要的时间\n",
    "                if timeTmp1  <= redTime +numStillVeh*1.5+1.5:#小于虚拟红灯结束时间\n",
    "                     predictStats = \"stop\" #目标车要听停止\n",
    "                else:        \n",
    "                     predictStats = \"no stop\"  #目标车要不要停止\n",
    "                \n",
    "     \n",
    "        df['numStillVeh'][i] = numStillVeh\n",
    "        df['predictStats'][i] = predictStats\n",
    "\n",
    "    return df\n",
    "##用于分析实际标记类别小于预测标记类别， xyDataTmp[\"predicted Labels By MCS\"] - xyDataTmp[\"origin speedFlag\"]>0\n",
    "def analyzing2(tmp, xyDataTmp,xyOrigin): \n",
    "    dfTmp1 = xyDataTmp[tmp]\n",
    "    #print(dfTmp1.head(5))\n",
    "    \n",
    "    \n",
    "    #1\n",
    "    df2 =  xyOrigin.iloc[dfTmp1.originIndex,:]   \n",
    "    plt.show()\n",
    "    df2[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    #print(df2.info())\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df2 = extractStillVeh2( df2)\n",
    "    df2.to_csv(\"tmpForAnalyzing.csv\")\n",
    "    \n",
    "    \n",
    "    ######################################\n",
    "    tmp1 = (df2['speedFlag'] == 0)  & (df2['predictStats'] == \"stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing1.csv\")\n",
    "    \n",
    "    tmp1 = (df2['speedFlag'] > 0)  & (df2['predictStats'] == \"no stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing2.csv\")\n",
    "    \n",
    "    \n",
    "    tmp1 = (df2['speedFlag'] == 0)  & (df2['predictStats'] == \"no stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing3.csv\")\n",
    "    \n",
    "    tmp1 = (df2['speedFlag'] > 0)  & (df2['predictStats'] == \"stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing4.csv\")\n",
    "    return\n",
    "    \n",
    "    '''\n",
    "    #2\n",
    "    tmp1 = df2['redLightTime'] - df2['arriveTime2'] >0 #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 = df2[df2['redLightTime'] - df2['arriveTime2'] >0] #红灯时间大于到达时间 ,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing2.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    \n",
    "    #3\n",
    "    tmp1 =df2['arriveTime2'] - df2['redLightTime'] >3 #红灯时间小于到达时间3，\n",
    "    tmp1 = tmp1 & (df2['speedFlag'] == 0) \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"#红灯时间小于于到达时间 ,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing3.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))  \n",
    "    \n",
    "   \n",
    "    #4据静止汽车数目，分析在df2['speedFlag'] > 0情况下，虚拟红灯时间小于于到达时间情况，也就是目标车可能不需要停下来\n",
    "    \n",
    "    tmp1 =(df2['speedFlag'] == 0) \n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 < df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 = tmp1 & tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing4.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))  \n",
    "    \n",
    "    #5 据静止汽车数目，分析在df2['speedFlag'] > 0情况下，虚拟红灯时间大于到达时间情况，也就是目标车可能需要停下来\n",
    "    tmp1 =(df2['speedFlag'] > 0) \n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 >= df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 = tmp1 & tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing5.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))  \n",
    "    \n",
    "    \n",
    "    #6 据静止汽车数目，分析虚拟红灯时间大于到达时间情况，也就是目标车可能需要停下来\n",
    "  \n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 >= df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 =  tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing6.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    \n",
    "    \n",
    "    #7 根据静止汽车数目，分析虚拟红灯时间小于到达时间情况，也就是目标车可能不需要停下来\n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']+1.5\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 < df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 =  tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing7.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4)) \n",
    "   '''\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "tmp = (xyDataTmp[\"origin speedFlag\"] - xyDataTmp[\"predicted Labels By MCS\"] >0) \n",
    "\n",
    "#analyzing1(tmp, xyDataTmp,xyOrigin)\n",
    "\n",
    "tmp = xyDataTmp[\"origin speedFlag\"] - xyDataTmp[\"predicted Labels By MCS\"]  >=3 \n",
    "#analyzing(tmp, xyDataTmp,xyOrigin)\n",
    "\n",
    "tmp = xyDataTmp[\"predicted Labels By MCS\"] - xyDataTmp[\"origin speedFlag\"]>0\n",
    "analyzing2(tmp, xyDataTmp,xyOrigin)\n",
    "      \n",
    "tmp = xyDataTmp[\"predicted Labels By MCS\"] - xyDataTmp[\"origin speedFlag\"]>=3\n",
    "#analyzing2(tmp, xyDataTmp,xyOrigin)\n",
    "\n",
    "#手动修改\n",
    "\n",
    "    \n",
    "      \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46c252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T10:10:55.559866Z",
     "start_time": "2023-01-27T10:10:53.398090Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c877a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T09:04:50.440620Z",
     "start_time": "2023-01-28T09:04:50.239643Z"
    }
   },
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b3f88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-29T03:05:13.857103Z",
     "start_time": "2023-01-29T03:05:13.853125Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "timestr= datetime.now()\n",
    "print(timestr)\n",
    "\n",
    "!conda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281fc84-e6c1-4671-9323-70c1fdbb7210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf tmp*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75a25a-12fd-4f04-85e2-c2a78f766298",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[11, 3, 4 ,5],[6, 7, 8, 9]])\n",
    "print(np.where(arr < 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0697b-8759-40f4-8556-8ec1bfa76aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.interpolate as si\n",
    "\n",
    "%matplotlib inline\n",
    "from numpy import polyfit, poly1d\n",
    "x=[0,3,8]\n",
    "y =[0,1,0]\n",
    "coeff = polyfit(x, y, 2)\n",
    "print(coeff)\n",
    " \n",
    "p = plt.plot(x, y, 'rx')\n",
    "\n",
    "x=[0,3,8]\n",
    "y =[0.5,1,0.5]\n",
    "\n",
    "x1 = np.linspace(0, 8, 100)\n",
    "y1 = np.polyval(coeff, x1)\n",
    "p = plt.plot(x1,y1, 'k-')\n",
    "\n",
    "\n",
    "f = si.interp1d(x, y,kind=1)\n",
    "y2= f(x1)  #调用经由interp1d返回的函数\n",
    "p = plt.plot(x1,y2, 'b-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8290fe-e417-434f-b337-6a5cb390c97a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5])\n",
    "b = np.array([5,5,6,7,8])\n",
    "c = a*b\n",
    "c\n",
    "\n",
    "a = [2,2,3,4,1]\n",
    "b = [5,5,6,7,8]\n",
    "d = np.multiply(a,b)\n",
    "d\n",
    "\n",
    "a =np.array(['1','2'])\n",
    "b = a.astype(int)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b28932-e69e-4875-bd40-c9f6ea1a46eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor23py36gpu",
   "language": "python",
   "name": "tensor23py36gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b27f224da048d073ae2b306b979c73d2559eaa860bf21b792b51024f42769a7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
