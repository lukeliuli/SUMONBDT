{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f6c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T03:55:17.555121Z",
     "start_time": "2023-01-26T03:55:17.435109Z"
    }
   },
   "outputs": [],
   "source": [
    "#一些常用的命令\n",
    "!nvidia-smi\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#一些常用的命令\n",
    "!git status\n",
    "!git add .\n",
    "!git commit -m \"correct and optimize some code\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0809da8b-07fe-447a-b1fd-d7520e79d721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全局共享函数，运行主程序前需要运行\n",
      "\n",
      "2slot的数据列表为：headName2SlotXY96\n",
      " ['vehID', 'redLightTime', 'distToRedLight', 'speed', 'laneAvgSpeed', 'arriveTime1', 'arriveTime2', 'vehLaneID', 'ArrTimeDivRedTime', 'vehPos_1', 'vehSpeed_1', 'vehPos_2', 'vehSpeed_2', 'vehPos_3', 'vehSpeed_3', 'vehPos_4', 'vehSpeed_4', 'vehPos_5', 'vehSpeed_5', 'vehPos_6', 'vehSpeed_6', 'vehPos_7', 'vehSpeed_7', 'vehPos_8', 'vehSpeed_8', 'vehPos_9', 'vehSpeed_9', 'vehPos_10', 'vehSpeed_10', 'vehPos_11', 'vehSpeed_11', 'vehPos_12', 'vehSpeed_12', 'vehPos_13', 'vehSpeed_13', 'vehPos_14', 'vehSpeed_14', 'vehPos_15', 'vehSpeed_15', 'vehPos_16', 'vehSpeed_16', 'vehPos_17', 'vehSpeed_17', 'vehPos_18', 'vehSpeed_18', 'vehPos_19', 'vehSpeed_19', 'vehPos_20', 'vehSpeed_20', 'redLightTime', 'distToRedLight', 'speed', 'laneAvgSpeed', 'arriveTime1', 'arriveTime2', 'ArrTimeDivRedTime', 'vehPos_1', 'vehSpeed_1', 'vehPos_2', 'vehSpeed_2', 'vehPos_3', 'vehSpeed_3', 'vehPos_4', 'vehSpeed_4', 'vehPos_5', 'vehSpeed_5', 'vehPos_6', 'vehSpeed_6', 'vehPos_7', 'vehSpeed_7', 'vehPos_8', 'vehSpeed_8', 'vehPos_9', 'vehSpeed_9', 'vehPos_10', 'vehSpeed_10', 'vehPos_11', 'vehSpeed_11', 'vehPos_12', 'vehSpeed_12', 'vehPos_13', 'vehSpeed_13', 'vehPos_14', 'vehSpeed_14', 'vehPos_15', 'vehSpeed_15', 'vehPos_16', 'vehSpeed_16', 'vehPos_17', 'vehSpeed_17', 'vehPos_18', 'vehSpeed_18', 'vehPos_19', 'vehSpeed_19', 'vehPos_20', 'minSpeedFlag']\n"
     ]
    }
   ],
   "source": [
    "############print(\"程序0.000 全局共享函数，运行主程序前需要运行\")\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import pickle  \n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "#######开始为功能函数\n",
    "print(\"全局共享函数，运行主程序前需要运行\")\n",
    "def dtFitAndSave(x,y,saveName):\n",
    "    str1=\"dtFitAndSave,用于决策树拟合和识别\"\n",
    "    \n",
    "    dt = tree.DecisionTreeClassifier(max_depth=7,min_samples_leaf=100)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    #data=tree.export_graphviz(dt, out_file=None,class_names=None,filled=True) \n",
    "    #graph = graphviz.Source(data)\n",
    "    #graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(\"纯决策树的识别\\n\",tmp1)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    #text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    return dt,yPredict\n",
    "\n",
    "########################################################################################################################\n",
    "###简单模型3，resnet_like\n",
    "def local_model(num_labels, dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(num_labels, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def global_model(dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='sigmoid',name=\"global\"))\n",
    "    return model\n",
    "\n",
    "def softmax_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='softmax',name=\"global\"))\n",
    "    return model\n",
    "'''\n",
    "############################################################################\n",
    "############################################################################\n",
    "#单层模型\n",
    "def kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels,saveName):\n",
    "    str1=\"kerasFitAndSaveSimple3LikeResnet,用于resnet_like的神经网络拟合和识别\"\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 512\n",
    "    dropout_rate = 0.05\n",
    "    hierarchy = [1,1,1,1]#四层，对于当前数据集已经足够了\n",
    "    global_models = []\n",
    "   \n",
    "    \n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(len(hierarchy)):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "\n",
    "   \n",
    "    \n",
    "    p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "       build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file='KerasSimple3_likeResnet_4lay512nodes.jpg', show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=1500, batch_size=40000*1)#GPU用这个\n",
    "    #saveName = \"KerasSimple3_likeResnet.h5\"\n",
    "    build_model.save(saveName)\n",
    "    plot_model(build_model, to_file='KerasSimple3_likeResnet_4lay512nodes.jpg', show_shapes=True)\n",
    "    return build_model\n",
    "\n",
    "############################################################################\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，\n",
    "def kerasFitAndSaveHierSimple4LikeResnet(x,yOneHot,num_labels,saveName):\n",
    "    str1=\"kerasFitAndSaveHierSimple4LikeResnet,用于resnet_like的 神经网络拟合和识别\"\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    beta = 0.5\n",
    "    hierarchy = [2,4,6,8,9]#5层，对于当前数据集已经足够了\n",
    "    global_models = []\n",
    "    local_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(len(hierarchy)):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "    \n",
    "    for i in range(len(hierarchy)):\n",
    "        local_models.append(local_model(hierarchy[i], dropout_rate, relu_size)(global_models[i]))\n",
    "        \n",
    "        \n",
    "    #显示只有局部局模型的情况(部分全局)\n",
    "    p_loc = layers.concatenate(local_models)\n",
    "    #modelTmp2 = tf.keras.Model(inputs=[features], outputs=[p_loc])\n",
    "    #modelTmp2.summary()#\n",
    "    #plot_model(modelTmp2, to_file='Flatten2.png', show_shapes=True)\n",
    "    p_glob1 = layers.Lambda(lambda x: x*beta,name=\"global\")(p_glob)\n",
    "    p_loc1 = layers.Lambda(lambda x: x*(1-beta),name=\"local\")(p_loc)\n",
    "\n",
    "    labels = layers.add([p_glob1, p_loc1])\n",
    "\n",
    "\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[labels])\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file='hmcnf1.jpg', show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=3500, batch_size=40000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "'''\n",
    "############################################################################\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，所以采用分离式\n",
    "###每一层的识别模型都是4层模型\n",
    "def g_sepHier1(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 10,srelu_size = 256,dropout_rate = 0.05):\n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.01\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    if 0:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=40000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "def getKerasResnetRVL(x,enc,saveName):\n",
    "    print(saveName)\n",
    "    model_name = saveName \n",
    "    model = keras.models.load_model(model_name)\n",
    "    y= model.predict([x], batch_size=2560)\n",
    "    nSamples = y.shape[0]\n",
    "    ###需要将预测出的值，转换01整数,并转为数字式\n",
    "    for i in range(y.shape[0]):\n",
    "        tmp = y[i]\n",
    "        index=  np.argmax(tmp)\n",
    "        y[i] = [0]*y.shape[1]\n",
    "        y[i,index]=1\n",
    "   \n",
    "\n",
    "    ###  \n",
    "    y= enc.inverse_transform(y)\n",
    "    y= y.reshape(-1,nSamples)[0]\n",
    "    \n",
    "    \n",
    "    return y\n",
    "\n",
    "def string2int(inputString):\n",
    "     #print(inputString)\n",
    "     tmp = 0\n",
    "     try:\n",
    "         strTmp=[str(ord(x)) for x in inputString]\n",
    "         tmp=tmp.join(strTmp)\n",
    "         tmp = float(tmp)/(len(inputString)*128)\n",
    "     except:\n",
    "         #print(inputString)\n",
    "         strTmp = inputString\n",
    "         tmp= \"0\"\n",
    "         tmp = 0\n",
    "     return tmp\n",
    " ## 根据经验以及最佳正确率的合并方法\n",
    " #第一次合并为0,1的合并\n",
    "\n",
    "########################################################################################################################\n",
    "##手工确定层次结构，以前测试时候为5层，根据论文为9层\n",
    "def convertY2Hieral(y):\n",
    "    #mat2acc\n",
    "    # [[0.914 0.009 0.017 0.007 0.032 0.    0.    0.    0.   ]\n",
    "    # [0.027 0.984 0.006 0.007 0.018 0.    0.    0.    0.   ]\n",
    "    # [0.02  0.006 0.972 0.    0.011 0.    0.    0.    0.   ]\n",
    "    # [0.036 0.002 0.    0.986 0.014 0.    0.002 0.    0.   ]\n",
    "    # [0.003 0.    0.    0.    0.925 0.    0.    0.    0.   ]\n",
    "    # [0.    0.    0.    0.    0.    1.    0.005 0.    0.   ]\n",
    "    # [0.    0.    0.    0.    0.    0.    0.993 0.    0.004]\n",
    "    # [0.    0.    0.    0.    0.    0.    0.    0.996 0.   ]\n",
    "    # [0.    0.    0.004 0.    0.    0.    0.    0.004 0.996]]\n",
    "    \n",
    "    \n",
    "    #hierarchy = [2,4,6,8,9]\n",
    "   # labelDict = {\"0\":[\"01234\",\"0123\",\"012\",\"01\",\"0\"],\\\n",
    "   #               \"1\":[\"01234\",\"0123\",\"012\",\"01\",\"1\"],\\\n",
    "   #               \"2\":[\"01234\",\"0123\",\"012\",\"2\",\"2\"],\\\n",
    "   #               \"3\":[\"01234\",\"0123\",\"3\",\"3\",\"3\"],\\\n",
    "   #              \"4\":[\"01234\",\"4\",    \"4\",\"4\",\"4\"],\\\n",
    "   #              \"5\":[\"5678\",\"5\",     \"5\",\"5\",\"5\"],\\\n",
    "   #              \"6\":[\"5678\",\"678\",   \"67\",\"6\",\"6\"],\\\n",
    "   #              \"7\":[\"5678\",\"678\",   \"67\",\"7\",\"7\"],\\\n",
    "   #              \"8\":[\"5678\",\"678\",   \"8\",\"8\",\"8\"],\\\n",
    "   #               }\n",
    "    \n",
    "    hierarchy = [2,3,4,5,6,7,8,9]\n",
    "    labelDict = {\"0\":[\"01234\",        \"01234\",        \"01234\",   \"01234\",      \"0123\",\"012\",\"01\",\"0\"],\\\n",
    "                  \"1\":[\"01234\",        \"01234\",        \"01234\",  \"01234\",     \"0123\",\"012\",\"01\",\"1\"],\\\n",
    "                  \"2\":[\"01234\",          \"01234\",      \"01234\",  \"01234\",     \"0123\",\"012\",\"2\",\"2\"],\\\n",
    "                  \"3\":[\"01234\",         \"01234\",       \"01234\",  \"01234\",    \"0123\",\"3\",\"3\",\"3\"],\\\n",
    "                 \"4\":[\"01234\",          \"01234\",       \"01234\",  \"01234\" ,     \"4\", \"4\",\"4\",\"4\"],\\\n",
    "                 \"5\":[\"5678\",               \"5\",        \"5\" ,      \"5\",       \"5\", \"5\",\"5\",\"5\"],\\\n",
    "                 \"6\":[\"5678\",            \"678\",        \"6\",        \"6\",       \"6\", \"6\",\"6\",\"6\"],\\\n",
    "                 \"7\":[\"5678\",             \"678\",       \"78\",       \"7\",       \"7\", \"7\",\"7\",\"7\"],\\\n",
    "                 \"8\":[\"5678\",             \"678\",       \"78\" ,      \"8\",        \"8\", \"8\",\"8\",\"8\"],\\\n",
    "                  }\n",
    "    '''\n",
    "    hierarchy = [5,9]\n",
    "    labelDict = {\"0\":[\"01\",\"0\"],\\\n",
    "                  \"1\":[\"01\",\"1\"],\\\n",
    "                  \"2\":[\"2\",\"2\"],\\\n",
    "                  \"3\":[\"34\",\"3\"],\\\n",
    "                \"4\":[\"34\",\"4\"],\\\n",
    "                 \"5\":[\"56\",\"5\"],\\\n",
    "                 \"6\":[\"56\",\"6\"],\\\n",
    "                 \"7\":[\"78\",\"7\"],\\\n",
    "                 \"8\":[\"78\",\"8\"],\\\n",
    "                 }\n",
    "    '''\n",
    "\n",
    "    y1 = [list(labelDict[str(x)]) for x in y]\n",
    "   \n",
    "    #print(\"!!!y1.type:\", type(y1))\n",
    "    #print(y1[:2])\n",
    "    #y2 = [t1[0] for t1 in y1]\n",
    "    #print(len(y2))\n",
    "  \n",
    "\n",
    "    return y1,hierarchy \n",
    "\n",
    "    # 当前车道，每个红灯车的所有时刻的样本\n",
    "name1A = [\"vehID\", \"redLightTime\", \"distToRedLight\", \"speed\", \"laneAvgSpeed\",\n",
    "         \"arriveTime1\", \"arriveTime2\", \"vehLaneID\", \"ArrTimeDivRedTime\"]#9\n",
    "name1B = [\"redLightTime\", \"distToRedLight\", \"speed\", \"laneAvgSpeed\",\n",
    "         \"arriveTime1\", \"arriveTime2\", \"vehLaneID\", \"ArrTimeDivRedTime\"]#8\n",
    "name1C = [\"redLightTime\", \"distToRedLight\", \"speed\", \"laneAvgSpeed\",\n",
    "         \"arriveTime1\", \"arriveTime2\",\"ArrTimeDivRedTime\"]#7，去掉\"vehID\"，\"vehLaneID\"\n",
    "name2 = [\"vehPos_1\", \"vehSpeed_1\", \"vehPos_2\", \"vehSpeed_2\",\n",
    "         \"vehPos_3\", \"vehSpeed_3\", \"vehPos_4\", \"vehSpeed_4\"]\n",
    "name3 = [\"vehPos_5\", \"vehSpeed_5\", \"vehPos_6\", \"vehSpeed_6\",\n",
    "         \"vehPos_7\", \"vehSpeed_7\", \"vehPos_8\", \"vehSpeed_8\"]\n",
    "name4 = [\"vehPos_9\", \"vehSpeed_9\", \"vehPos_10\", \"vehSpeed_10\",\n",
    "         \"vehPos_11\", \"vehSpeed_11\", \"vehPos_12\", \"vehSpeed_12\"]\n",
    "name5 = [\"vehPos_13\", \"vehSpeed_13\", \"vehPos_14\", \"vehSpeed_14\",\n",
    "         \"vehPos_15\", \"vehSpeed_15\", \"vehPos_16\", \"vehSpeed_16\"]\n",
    "name6 = [\"vehPos_17\", \"vehSpeed_17\", \"vehPos_18\", \"vehSpeed_18\",\n",
    "         \"vehPos_19\", \"vehSpeed_19\", \"vehPos_20\", \"vehSpeed_20\"]\n",
    "\n",
    "name6_error = [\"vehPos_17\", \"vehSpeed_17\", \"vehPos_18\", \"vehSpeed_18\",\n",
    "         \"vehPos_19\", \"vehSpeed_19\", \"vehPos_20\"] #原始数据出现错误，\n",
    "vehAll = name2+name3+name4+name5+name6 #40\n",
    "headName49 = name1A+vehAll\n",
    "headName48 = name1B+vehAll\n",
    "\n",
    "headName2SlotX95 = headName49+name1C+name2+name3+name4+name5+name6_error #9+40+7+39= 95\n",
    "headName2SlotXY96 = headName2SlotX95+['minSpeedFlag'] ##96\n",
    "\n",
    "headName2SlotX94 = headName48+name1C+name2+name3+name4+name5+name6_error #48+40+7+39 =  94\n",
    "print(\"\\n2slot的数据列表为：headName2SlotXY96\\n\",headName2SlotXY96)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "44dbf652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T14:33:49.232503Z",
     "start_time": "2023-02-13T14:31:39.188869Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.主程序开始，建立多层嵌套决策树模型，3080ti的GPU是AMD2400CPU 运算速度100倍\n",
      "0.这是简化程序，原始带有更多测试和原始模型的程序在mainTestCSVMLP3(hmcnf_keras).ipynb\n",
      "程序编号为0\n",
      "0.主程序开始, 建立多层嵌套决策树模型,3080ti的GPU是AMD2400CPU 运算速度100倍\n",
      "读取France数据并且把数据进行onehot处理\n",
      "\n",
      "2slot的数据列表名为：headName2SlotXY96\n",
      "\n",
      "根据一些基本规则，需要把数据库中的一些明显不符合逻辑的数据清楚\n",
      "(134573, 96)\n",
      "根据一些基本规则，数据处理，包括把laneID设为0，把vehid去掉\n",
      "\n",
      "去掉vehID,minSpeedFlag的2slot的X数据列表为：headName2SlotX94\n",
      "\n",
      "数据X0的长度为94,x0.shape: (1177983, 94) y0.shape: (1177983,) y0.type: <class 'numpy.ndarray'>\n",
      "##############################################################################################################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.接编号为0的主程序,根据基于正确率的聚类程序或者经验将底层类别归结到上一层的类别\n",
      "2.程序编号为0+\n",
      "训练分离式多级模型\n",
      "x.shape: (1177983, 94) y.shape: (1177983,) y.type: <class 'numpy.ndarray'>\n",
      "[0 0 0 ... 8 8 8]\n",
      "\n",
      "\n",
      " levelIndex 0 nSamples,nFeatures (353394, 94)\n",
      "yCurLayer1.shape: (353394,)\n",
      "[array(['01234', '5678'], dtype='<U5')] ['x0_01234' 'x0_5678']\n",
      "[[1. 0.]]\n",
      "num_labels: 2\n",
      "../trainedModes/modelSep-9level0-4layer-2slots-gpu1.h5\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.0248 - accuracy: 0.9916\n",
      "Epoch 1/100\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 0.0196 - accuracy: 0.9933\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.0158 - accuracy: 0.9944\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 0.0125 - accuracy: 0.9960\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.0119 - accuracy: 0.9963\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.0107 - accuracy: 0.9966\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 0.0094 - accuracy: 0.9972\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.0091 - accuracy: 0.9971\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.0082 - accuracy: 0.9975\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.0083 - accuracy: 0.9974\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 0.0076 - accuracy: 0.9976\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.0072 - accuracy: 0.9978\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-3a768263ab7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;31m#saveName = \"../trainedModes/modelSep-2level%d-%dlayer-2slots-gpu1.h5\" %(i,numLayers)#基于拥堵定义的2层结构\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaveName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0msepHier1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myOneHot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaveName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlevelIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumLayers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumEpochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mencLevels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-132-3a768263ab7d>\u001b[0m in \u001b[0;36msepHier1\u001b[0;34m(x, yOneHot, num_labels, saveName, levelIndex, numLayers, numEpochs, srelu_size, dropout_rate)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mbuild_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myOneHot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumEpochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m160000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#GPU用这个\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mbuild_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaveName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##主程序开始######################################################################################################################\n",
    "print(\"0.主程序开始，建立多层嵌套决策树模型，3080ti的GPU是AMD2400CPU 运算速度100倍\")\n",
    "print(\"0.这是简化程序，原始带有更多测试和原始模型的程序在mainTestCSVMLP3(hmcnf_keras).ipynb\")\n",
    "print(\"程序编号为0\")\n",
    "########################################################################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import pickle  \n",
    "\n",
    "\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，所以采用分离式\n",
    "###每一层的识别模型都是4层模型\n",
    "def sepHier1(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 10,srelu_size = 256,dropout_rate = 0.05):\n",
    "    \n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    featuresInput = layers.Input(shape=(features_size,))\n",
    "    features = layers.BatchNormalization()(featuresInput)\n",
    "    #features=featuresInput\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[featuresInput], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        #build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=160000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def minSpeed2Tag(minSpeed):\n",
    "    \n",
    "    if minSpeed >40/3.6:\n",
    "        speedFlag = 8\n",
    "    if minSpeed <= 40/3.6 and minSpeed > 35/3.6:\n",
    "        speedFlag = 7\n",
    "    if minSpeed <= 35/3.6 and minSpeed > 30/3.6:\n",
    "        speedFlag = 6\n",
    "    if minSpeed <= 30/3.6 and minSpeed > 25/3.6:\n",
    "        speedFlag = 5\n",
    "    if minSpeed <= 25/3.6 and minSpeed > 20/3.6:\n",
    "        speedFlag = 4\n",
    "    if minSpeed <= 20/3.6 and minSpeed > 15/3.6:\n",
    "        speedFlag = 3\n",
    "    if minSpeed <= 15/3.6 and minSpeed > 10/3.6:\n",
    "        speedFlag = 2\n",
    "    if minSpeed <= 10/3.6 and minSpeed > 5/3.6:\n",
    "        speedFlag = 1\n",
    "    if minSpeed <= 5/3.6:\n",
    "        speedFlag = 0\n",
    "    return speedFlag\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "print(\"0.主程序开始, 建立多层嵌套决策树模型,3080ti的GPU是AMD2400CPU 运算速度100倍\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "#from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "#policy = mixed_precision.Policy('mixed_float16')\n",
    "#mixed_precision.set_policy(policy)\n",
    "\n",
    " \n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "\n",
    "print(\"读取France数据并且把数据进行onehot处理\")\n",
    "\n",
    "#file1 = \"../trainData/france_0_allSamples1.csv\"\n",
    "file1 = \"../trainData/france_0_allSamples1_2slot.csv\"\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "print(\"\\n2slot的数据列表名为：headName2SlotXY96\\n\")\n",
    "#print(xyDataTmp.info())\n",
    "\n",
    "##########################################################\n",
    "##########################################################\n",
    "\n",
    "print(\"根据一些基本规则，需要把数据库中的一些明显不符合逻辑的数据清楚\")\n",
    "#1.当前速度已经为0了，输出标志大于0\n",
    "#tmp1  = (xyDataTmp.iloc[:,3] == 0) & (xyDataTmp.iloc[:,-1] > 0)\n",
    "#xyDataTmp =xyDataTmp[tmp1 == False]\n",
    "h,w =  xyDataTmp.shape\n",
    "print(xyDataTmp.shape)\n",
    "droplist = []\n",
    "for i in range(h):\n",
    "    minSpeed = xyDataTmp.iloc[i,3].item()\n",
    "    tag1 = minSpeed2Tag(minSpeed)\n",
    "    tag2 = xyDataTmp.iloc[i,-1].item()\n",
    "    if tag1<tag2:\n",
    "        droplist.extend([i])\n",
    "\n",
    "#print(droplist)\n",
    "#print(max(droplist))\n",
    "xyDataTmp= xyDataTmp.drop(labels=droplist,axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################################\n",
    "##########################################################\n",
    "print(\"根据一些基本规则，数据处理，包括把laneID设为0，把vehid去掉\")\n",
    "print(\"\\n去掉vehID,minSpeedFlag的2slot的X数据列表为：headName2SlotX94\\n\")\n",
    "xyData = np.array(xyDataTmp)\n",
    "h,w = xyData.shape\n",
    "\n",
    "#x = xyData[:,1:23]#简单处理与SUMO数据库一致\n",
    "x0rigin = xyData[:,1:w-1]#用所有的数据,第0列为vehID,不要，\n",
    "y0rigin  = xyData[:,w-1]\n",
    "\n",
    "x0rigin[:,6] = [string2int(inputString) for inputString in x0rigin[:,6] ]#字符串vehLaneID 变为整数\n",
    "x0rigin[:,6] = [0 for inputString in x0rigin[:,6] ]#字符串vehLaneID 变为整数\n",
    "x0rigin =x0rigin.astype(np.float32)#GPU 加这个\n",
    "y0rigin =y0rigin.astype(np.int64)#GPU 加这个\n",
    "\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "x0,y0= ros.fit_resample(x0rigin , y0rigin)#对数据不平衡进行处理，保证样本数一致\n",
    "\n",
    "x0=x0.astype(np.float32)#GPU 加这个\n",
    "y0=y0.astype(np.int64)#GPU 加这个\n",
    "\n",
    "print(\"数据X0的长度为94,x0.shape:\",x0.shape,\"y0.shape:\",y0.shape,\"y0.type:\", type(y0) )\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "########################################################################################################################    \n",
    "########################################################################################################################   \n",
    "########################################################################################################################    \n",
    "########################################################################################################################    \n",
    "print(\"##############################################################################################################\")\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"1.接编号为0的主程序,根据基于正确率的聚类程序或者经验将底层类别归结到上一层的类别\")\n",
    "print(\"2.程序编号为0+\")  \n",
    " \n",
    "'''\n",
    "if 0:# 训练统合样式的HMCN-F多级模型\n",
    "    print(\"训练5hieral, 4层, 9 label 模型\")\n",
    "    x=x0\n",
    "    y=yl5\n",
    "    x=x.astype(np.float32)#GPU 加这个\n",
    "    y=y.astype(np.int64)#GPU 加这个\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    yH1 = convertY2Hieral(y)\n",
    "    hierarchy = [2,4,6,8,9]#5层，对于当前数据集已经足够了\n",
    "    \n",
    "    yH1= np.array(yH1)\n",
    "    \n",
    "    nSamples,nFeatures =  x.shape\n",
    "    enc = OneHotEncoder()\n",
    "    yH1= yH1.reshape(nSamples,-1)\n",
    "    #print(yH1[:3])\n",
    "    print(\"yH1.shape:\",yH1 .shape,\"yH1.type:\", type(yH1) )\n",
    "    enc.fit(yH1)\n",
    "    #print(enc.categories_,enc.get_feature_names())\n",
    "    yOneHot=enc.transform(yH1).toarray()\n",
    "    #print(yOneHot[:3])\n",
    "    \n",
    "    num_labels = yOneHot.shape[1] \n",
    "    print(num_labels)\n",
    "    saveName = \"../trainedModes/model-5hier-9label-5lays-128nodes-2slots-gpu1.h5\"\n",
    "    kerasModel4_5hier_9label = kerasFitAndSaveHierSimple4LikeResnet(x,yOneHot,num_labels,saveName)   \n",
    "'''    \n",
    "    \n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if 1:# 训练多级模型\n",
    "    print(\"训练分离式多级模型\")\n",
    "    \n",
    "    #准备字典，用于保存训练后的数据\"\n",
    "    xFloors=  dict()\n",
    "    yFloors =  dict()\n",
    "    xTestFloors =dict()\n",
    "    yTestFloors = dict()\n",
    "    modSaveNameFloors =dict()\n",
    "    encLevels= dict()\n",
    "    yKerasFloors = dict()\n",
    "    x=x0\n",
    "    y=y0\n",
    "    x=x.astype(np.float32)#GPU 加这个\n",
    "    y=y.astype(np.int64)#GPU 加这个\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    print(y)\n",
    "    \n",
    "    #hierarchy = [2,4,6,8,9]\n",
    "    #hierarchy = [2,3,4,5,6,7,8,9]\n",
    "    yH1,hierarchy = convertY2Hieral(y)\n",
    "    \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, yH1, test_size=0.7, random_state=0)\n",
    "   \n",
    "    nSamples,nFeatures =  x_train.shape\n",
    "    \n",
    "    \n",
    "    numEpochs =100 #1500/60/60*5 = 2hour\n",
    "    \n",
    "    \n",
    "    for i in range(len(hierarchy)):\n",
    "        print(\"\\n\\n levelIndex\",i,\"nSamples,nFeatures\",x_train.shape)\n",
    "        levelIndex = i\n",
    "        numLayers = 4\n",
    "        enc = OneHotEncoder()\n",
    "        nSamples,nFeatures =  x_train.shape\n",
    "       \n",
    "            \n",
    "        yCurLayer1 = [t1[i] for t1 in y_train]\n",
    "        \n",
    "        yCurLayer1 = np.array(yCurLayer1)\n",
    "        print(\"yCurLayer1.shape:\",yCurLayer1.shape)\n",
    "        \n",
    "        yCurLayer1= yCurLayer1.reshape(nSamples,-1)\n",
    "        enc.fit(yCurLayer1)\n",
    "        \n",
    "        yOneHot=enc.transform(yCurLayer1).toarray()\n",
    "        print(enc.categories_,enc.get_feature_names())\n",
    "        print(yOneHot[:1])\n",
    "        \n",
    "        \n",
    "        num_labels = hierarchy[i] \n",
    "        print(\"num_labels:\", num_labels)\n",
    "        saveName = \"../trainedModes/modelSep-9level%d-%dlayer-2slots-gpu1.h5\" %(i,numLayers)\n",
    "        #saveName = \"../trainedModes/modelSep-2level%d-%dlayer-2slots-gpu1.h5\" %(i,numLayers)#基于拥堵定义的2层结构\n",
    "        print(saveName)\n",
    "        sepHier1(x_train,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs)\n",
    "        \n",
    "        encLevels[str(i)] = enc\n",
    "        xFloors[str(i)] = x_train\n",
    "        yFloors[str(i)] = yCurLayer1\n",
    "        \n",
    "        \n",
    "        nSamplesTest,nFeaturesT =  x_test.shape\n",
    "        yCurLayerTest = [t1[i] for t1 in y_test]\n",
    "        yCurLayerTest = np.array(yCurLayerTest)\n",
    "        yCurLayerTest= yCurLayerTest.reshape(nSamplesTest,-1)\n",
    "        \n",
    "        xTestFloors[str(i)] = x_test\n",
    "        yTestFloors[str(i)] = yCurLayerTest\n",
    "        modSaveNameFloors[str(i)] = saveName\n",
    "        \n",
    "    #######保存为pickle文件,用于后期的SUMO和数据分析\n",
    "    fpk=open('samples1.pkf','wb+')  \n",
    "    pickle.dump([xFloors,yFloors,modSaveNameFloors,encLevels,xTestFloors, yTestFloors],fpk)  \n",
    "    fpk.close() \n",
    "    \n",
    "    fpk=open('sepTrainedsSamplesAll1.pkf','wb+')  \n",
    "    pickle.dump([xFloors,yFloors,modSaveNameFloors,encLevels,xTestFloors, yTestFloors],fpk)  \n",
    "    fpk.close() \n",
    "\n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "#####用现有训练模型进行预测\n",
    "\n",
    "print(\"用现有训练模型进行预测，也是论文显示层次模型结果的数据 test_mat2acc?.mat\")\n",
    "#fpk=open('samples1.pkf','rb')   \n",
    "#[xFloors,yFloors,modSaveNameFloors,encLevels,xTestFloors, yTestFloors]=pickle.load(fpk)  \n",
    "#fpk.close()  \n",
    "\n",
    "\n",
    "fpk=open('sepTrainedsSamplesAll1.pkf','rb') \n",
    "[xFloors,yFloors,modSaveNameFloors,encLevels,xTestFloors, yTestFloors]=pickle.load(fpk)  \n",
    "fpk.close()  \n",
    "\n",
    "\n",
    "yKerasFloors = dict()\n",
    "\n",
    "for i in range(len(hierarchy)):\n",
    "        levelIndex = i\n",
    "        #x = xFloors[str(i)]\n",
    "        #yCurLayer1 =  yFloors[str(i)]\n",
    "        \n",
    "        x = xTestFloors[str(i)]\n",
    "        yCurLayer1 =  yTestFloors[str(i)]\n",
    "        \n",
    "        saveName =  modSaveNameFloors[str(i)] \n",
    "        enc = encLevels[str(i)]\n",
    "        yOneHot=enc.transform(yCurLayer1).toarray()\n",
    "        yPredict=getKerasResnetRVL(x,enc,saveName)\n",
    "        print(\"分离式多层识别结果:第%d层\\n\" %i)\n",
    "        mat1num = confusion_matrix(yCurLayer1,yPredict)\n",
    "        print(mat1num)\n",
    "        mat2acc = confusion_matrix(yCurLayer1,yPredict,normalize='pred')  \n",
    "        print(np.around(mat2acc , decimals=3))\n",
    "        yKerasFloors[str(i)] =  yPredict\n",
    "        \n",
    "        df = pd.DataFrame(np.around(mat2acc , decimals=3))\n",
    "        fs = \"test_mat2acc%d.csv\" %i\n",
    "        df.to_csv(fs,index= False, header= False)\n",
    "        \n",
    "fpk=open('sepTestRVLSamples1.pkf','wb+')         \n",
    "pickle.dump([xFloors,yFloors,modSaveNameFloors,encLevels,yKerasFloors,xTestFloors,yTestFloors],fpk)  \n",
    "fpk.close() \n",
    "\n",
    " \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5c8674b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T14:36:52.438549Z",
     "start_time": "2023-02-13T14:36:16.048928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.接编号为0的主程序,先找出低概率样本，\n",
      "2.对较低概率的样本进行蒙特卡洛模拟分析，原始对应程序为mainSimSumoFranceDatra\n",
      "3.最终进行分析，程序编号为1\n",
      "3.最终进行分析，程序编号为1\n",
      "1.1 主程序开始\n",
      "xlowpra.shape (1736, 94)\n",
      "headName2SlotX94 94\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########################################################################################################################\n",
    "print(\"1.接编号为0的主程序,先找出低概率样本，\")\n",
    "print(\"2.对较低概率的样本进行蒙特卡洛模拟分析，原始对应程序为mainSimSumoFranceDatra\")\n",
    "print(\"3.最终进行分析，程序编号为1\")\n",
    "print(\"3.最终进行分析，程序编号为1\")\n",
    "########################################################################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "#import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "      \n",
    "\n",
    "\n",
    "    \n",
    "########################################################################################################################\n",
    "print(\"1.1 主程序开始\")\n",
    "########################################################################################################################\n",
    "\n",
    "########################################################################################################################\n",
    "#####用现有训练模型进行预测\n",
    "fpk=open('sepTrainedsSamplesAll1.pkf','rb') \n",
    "[xFloors,yFloors,modSaveNameFloors,encLevels,xTestFloors, yTestFloors]=pickle.load(fpk)  \n",
    "fpk.close()   \n",
    "\n",
    "fpk=open('sepTestRVLSamples1.pkf','rb') \n",
    "[xFloors,yFloors,modSaveNameFloors,encLevels,yKerasFloors,xTestFloors, yTestFloors]=pickle.load(fpk)  \n",
    "fpk.close()  \n",
    "\n",
    "hierarchy=[2,3,4,5,6,7,8,9]\n",
    "for i in [7]:\n",
    "#for i in range(len(hierarchy)):\n",
    "        levelIndex = i \n",
    "        x = xTestFloors[str(i)]\n",
    "        yCurLayer1 =  yTestFloors[str(i)]\n",
    "        #yP = yKerasFloors[str(i)]\n",
    "        \n",
    "        modeSaveName = \"../trainedModes/modelSep-9level7-4layer-2slots-gpu1.h5\"\n",
    "        model = keras.models.load_model(modeSaveName)\n",
    "        yPredictOut= model.predict([x], batch_size=2560)#预测并将onehot转为label\n",
    "        yPredictOut = np.around(yPredictOut , decimals=3)\n",
    "        #print(yPredictOut)\n",
    "        ymax1=np.max(yPredictOut,axis=1)#将onehot转为label.提取最大概率\n",
    "        ymax2=np.argmax(yPredictOut,axis=1)#将onehot转为label\n",
    "        \n",
    "        index = np.where(ymax1<0.8)[0]#提取最大值小于0.5的例子\n",
    "        \n",
    "        ylowpraPredictNN=yPredictOut[index]#对较低概率的样本\n",
    "        xlowpra=x[index]\n",
    "        ylowpraLabel = yCurLayer1[index]\n",
    "        ylowPredictLabel = ymax2[index].reshape(-1,1)\n",
    "        \n",
    "        \n",
    "        print(\"xlowpra.shape\",xlowpra.shape)\n",
    "        \n",
    "        \n",
    "        fpk=open('lowprobSamples.pkf','wb+')  #只有底层（第7层）\n",
    "        pickle.dump([xlowpra,ylowpraLabel,ylowPredictLabel,ylowpraPredictNN],fpk)  \n",
    "        fpk.close() \n",
    "        \n",
    "        df = pd.DataFrame(xlowpra)\n",
    "        fs = \"lowprobSamplesX.csv\"\n",
    "        \n",
    "        print(\"headName2SlotX94\",len(headName2SlotX94))\n",
    "        df.to_csv(fs,index= False, header= headName2SlotX94)\n",
    "       \n",
    "        ylowPredictLabel = ymax2[index].reshape(-1,1)\n",
    "        \n",
    "        df = pd.DataFrame(np.concatenate([ylowpraLabel,ylowPredictLabel,ylowpraPredictNN],axis=1))\n",
    "        fs = \"lowprobSamplesY.csv\"\n",
    "        df.to_csv(fs,index= False, header=['ylowpraLabel','ylowPredictLabel','0','1','2','3','4','5','6','7','8'])\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c074866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T09:51:42.931165Z",
     "start_time": "2023-01-28T09:51:42.771943Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "print(\"接程序1: 对较低概率的样本进行蒙特卡洛模拟分析,原始对应程序为mainSimSumoFranceDatra\")\n",
    "print(\"因为配置失误，采用将低概率的样本进行保存为文件，然后再root用户下命令行模式用SUMO模拟（不使用conda）\")\n",
    "print(\"输出为sumoSimData？？？.csv,里面有每个样本的sumo输出，kerasNN输出以及原始的输入输出\")\n",
    "print(\"程序编号为2\")\n",
    "print(\"程序编号为2\")\n",
    "########################################################################################################################\n",
    "!python3 sumoSimByFrance.py#运行runSumoSimFun.py 中test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becda4ae-fe95-4636-9b74-0f775aec3d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "print(\"接程序2: 综合SUMO输出，对keras输出进行优化。程序输入为程序2的输出\")\n",
    "print(\"优化选择1.NN。 2 回归分析。3 概率分析。\")\n",
    "print(\"程序编号为3\")\n",
    "########################################################################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import pickle \n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "############################################################################\n",
    "############################################################################\n",
    "####建立NN模型，与sepHier2一样\n",
    "def sepHier2(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 100,srelu_size = 256,dropout_rate = 0.05):\n",
    "    \n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    featuresInput = layers.Input(shape=(features_size,))\n",
    "    features = layers.BatchNormalization()(featuresInput)\n",
    "    #features=featuresInput\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[featuresInput], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        #build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=20000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    \n",
    "    \n",
    "    ##\n",
    "    return build_model\n",
    "\n",
    "\n",
    "def dtFit(x,y):\n",
    "    str1=\"dtFitAndSave,用于决策树拟合和识别\"\n",
    "    \n",
    "    dt = tree.DecisionTreeClassifier(max_depth=7,min_samples_leaf=100)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    #data=tree.export_graphviz(dt, out_file=None,class_names=None,filled=True) \n",
    "    #graph = graphviz.Source(data)\n",
    "    #graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(\"纯决策树的识别\\n\",tmp1)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    #text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    return dt,yPredict\n",
    "########################################################################################################################   \n",
    "########################################################################################################################    \n",
    "print(\"##############################################################################################################\")\n",
    "\n",
    "print(\"程序编号为3.1，主程序开始运行\")\n",
    "\n",
    "####原始的keras训练数据中的低概率数据，阶段1\n",
    "#sample_name = 1(ID)+8(keyFeature)+40(otherVehcle)+6(keyFeatures)+40(otherVehs)+1(flag)= 96\n",
    "#xlowpra:x-name = 8(keyFeature)+40(otherVehcle)+6(keyFeatures)+40(otherVehs)= 94      \n",
    "fpk=open('lowprobSamples.pkf','rb')   \n",
    "[xlowpra,ylowpraLabel,ylowPredictLabel,ylowpraPredictNN]=pickle.load(fpk)  \n",
    "print(\"xlowpra\",xlowpra.shape)\n",
    "fpk.close()      \n",
    "\n",
    "\n",
    "\n",
    "####原始的keras训练数据，阶段2\n",
    "df = pd.read_csv('sumoSimData6375.csv', sep=',')\n",
    "\n",
    "print(\"sumoSimData6375.csv\",df.shape)\n",
    "print(\"sumoSimData6375.csv\",df.columns)\n",
    "numSamples,numFeatures = df.shape\n",
    "\n",
    "##['sampleIndex','outputAvgSpeed','originOutput','sumoOutputSpeedTag','kerasPredictLabel','smv1','smv2',\\\n",
    " ##                                              'NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8'])\n",
    "    \n",
    "\n",
    "sumoOutput='sumoOutputSpeedTag'\n",
    "yKerasOutput='kerasPredictLabel'\n",
    "originOutput ='originOutput'\n",
    "sumoOutList = ['smv1','smv2']\n",
    "outputListNN = ['NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8']\n",
    "outputAvgSpeed = 'outputAvgSpeed'\n",
    "\n",
    "df1 = df[ \"sampleIndex\"]\n",
    "lowprobIndex = df1.iloc[0:numSamples].to_numpy()\n",
    "lowproKerasStage1Input = xlowpra[lowprobIndex]#这里对原始lowprobIndexd进行了筛选\n",
    "#print(originLowproKerasInput[0:3].shape)\n",
    "#print(originLowproKerasInput[0:3])\n",
    "\n",
    "df1 = df[sumoOutput]\n",
    "x1_sumoOutput = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "df1 = df[yKerasOutput]\n",
    "x2_yKerasOutput = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "df1 = df[outputListNN]\n",
    "x3_outputListNN = df1.iloc[0:numSamples].to_numpy()\n",
    "\n",
    "df1 = df[outputAvgSpeed]\n",
    "x4_outputAvgSpeed = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "df1 = df[sumoOutList]\n",
    "x5_sumoOutList = df1.iloc[0:numSamples].to_numpy()\n",
    "\n",
    "df1 = df[originOutput]\n",
    "y = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y)\n",
    "yOneHot=enc.transform(y).toarray()\n",
    "\n",
    "x = np.concatenate([x1_sumoOutput,x2_yKerasOutput,x3_outputListNN,x4_outputAvgSpeed,x5_sumoOutList],axis=1)#71%\n",
    "#x = np.concatenate([x2,x2,x3,x2,x2],axis=1)#56%\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "#print(y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"#############################\\n数据预处理\\n\")\n",
    "#数据预处理\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, yOneHot, test_size=0.5, random_state=0)\n",
    "   \n",
    "\n",
    "rN,cN= np.where(np.isnan(x))\n",
    "#print(rN,cN)\n",
    "#print(rN.shape)\n",
    "\n",
    "for i in range(rN.shape[0]):\n",
    "    x[rN[i],cN[i]] = 0\n",
    " \n",
    "\n",
    "################################################################################################################################\n",
    "################################################################################################################################ \n",
    "\n",
    "print(\"#############################\\n原生keras\\n\")\n",
    "if 0:\n",
    "    yPredict = x2_yKerasOutput\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(tmp1)\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "\n",
    "    score = accuracy_score(yPredict, y)\n",
    "    print(score) \n",
    "    \n",
    "    df = pd.DataFrame(np.around(mat2acc , decimals=3))\n",
    "    fs = \"低概率样本的原始kerasNN模型的混淆矩阵结果.csv\"\n",
    "    df.to_csv(fs,index= False, header= False)\n",
    "\n",
    "print(\"#############################\\nkerasNN\\n\")\n",
    "if 0:\n",
    "    print(x.shape)\n",
    "    print(yOneHot.shape)\n",
    "    num_labels = yOneHot.shape[1]\n",
    "    numLayers = 4\n",
    "    numEpochs = 1\n",
    "    saveName =\"../trainedModes/stage2_1.h5\";\n",
    "    levelIndex = 7\n",
    "\n",
    "    sepHier2(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs)\n",
    "\n",
    "\n",
    "    yPredict=getKerasResnetRVL(x,enc,saveName)\n",
    "\n",
    "\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(tmp1)\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "    score = accuracy_score(yPredict, y)\n",
    "    print(score)\n",
    "\n",
    "print(\"#############################\\n只训练最底层，9label,kerasNN\\n\")\n",
    "\n",
    "\n",
    "if 0:\n",
    "    print(x.shape)\n",
    "    print(yOneHot.shape)\n",
    "    num_labels = yOneHot.shape[1]\n",
    "    numLayers = 4\n",
    "    numEpochs = 1\n",
    "    saveName =\"../trainedModes/stage2Modes_addingSumoFeatures-noHierachical.h5\";\n",
    "    levelIndex = 7\n",
    "\n",
    "    sepHier2(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs)\n",
    "\n",
    "\n",
    "    yPredict=getKerasResnetRVL(x,enc,saveName)\n",
    "\n",
    "\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(tmp1)\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "    score = accuracy_score(yPredict, y)\n",
    "    print(score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################################################################\n",
    "################################################################################################################################ \n",
    "if 0:#论文没有用\n",
    "    print(\"#############################\\n决策树\\n\")\n",
    "    dtFit(x,y)\n",
    "\n",
    "    print(\"#############################\\n逻辑回归\\n\")    \n",
    "    model = LogisticRegression()\n",
    "    model.fit(x,y)\n",
    "    yPredict = model.predict(x)\n",
    "\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(tmp1)\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "    score = accuracy_score(yPredict, y)\n",
    "    print(score) \n",
    "    print(\"#############################\\n贝叶斯高斯回归\\n\")\n",
    "    nb_cls = naive_bayes.GaussianNB().fit(x,y)\n",
    "    yPredict = nb_cls.predict(x) \n",
    "\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(tmp1)\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "    score = accuracy_score(yPredict, y)\n",
    "    print(score) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#fpk=open('stage2-LowprobSamples-addingSumoFeatures-noHierachical.pkf','wb')  \n",
    "#pickle.dump([df,x,y,yOneHot,x_train, x_test, y_train, y_test,enc,saveName],fpk)  \n",
    "#fpk.close() \n",
    "\n",
    "\n",
    "################################################################################################################################\n",
    "################################################################################################################################   \n",
    "print(\"\\n#############################加入新特征SUMO+阶段1的特征,对低概率样本重新训练多级独立kerasNN\")\n",
    "##分层重新训练，加入特征SMV1,SMV2\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，所以采用分离式\n",
    "###每一层的识别模型都是4层模型\n",
    "def sepHier1(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 10,srelu_size = 256,dropout_rate = 0.05):\n",
    "    \n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    featuresInput = layers.Input(shape=(features_size,))\n",
    "    features = layers.BatchNormalization()(featuresInput)\n",
    "    #features=featuresInput\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[featuresInput], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        #build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=160000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "############################################################################\n",
    "############################################################################\n",
    "############################################################################\n",
    "############################################################################\n",
    "print(\"\\n#############################\\n加入新特征SUMO+阶段1的特征,对低概率样本重新训练多级独立kerasNN\")\n",
    "#基于lowprobIndex对原始低概率样本进行了筛选\n",
    "#lowproKerasStage1Input = xlowpra[lowprobIndex]#这里对原始lowprobIndexd进行了筛选\n",
    "#\n",
    "print('lowproKerasStage1Inputx.shape:', lowproKerasStage1Input.shape)\n",
    "x = np.concatenate([lowproKerasStage1Input,x1_sumoOutput,x2_yKerasOutput,x3_outputListNN,x4_outputAvgSpeed,x5_sumoOutList],axis=1)#71%\n",
    "\n",
    "y9Label = y \n",
    "print('x.shape:',x.shape)\n",
    "rN,cN= np.where(np.isnan(x))\n",
    "#print(rN,cN)\n",
    "#print(rN.shape)\n",
    "\n",
    "\n",
    "for i in range(rN.shape[0]):\n",
    "    x[rN[i],cN[i]] = 0\n",
    "    \n",
    "if 1:\n",
    "    #准备字典，用于保存训练后的数据\"\n",
    "    xFloors=  dict()\n",
    "    yFloors =  dict()\n",
    "    xTestFloors =dict()\n",
    "    yTestFloors = dict()\n",
    "    modSaveNameFloors =dict()\n",
    "    encLevels= dict()\n",
    "    yKerasFloors = dict()\n",
    "\n",
    "    y=y9Label.reshape(1,-1)[0]\n",
    "    x=x.astype(np.float32)#GPU 加这个\n",
    "    y=y.astype(np.int64)#GPU 加这个\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    print(y)\n",
    "    \n",
    "    #hierarchy = [2,4,6,8,9]\n",
    "    #hierarchy = [2,3,4,5,6,7,8,9]\n",
    "    yH1,hierarchy = convertY2Hieral(y)\n",
    "    \n",
    "    print(yH1[0:2])\n",
    "    \n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, yH1, test_size=0.7, random_state=0)\n",
    "   \n",
    "    nSamples,nFeatures =  x_train.shape\n",
    "    \n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    numEpochs =10 #1500/60/60*5 = 2houer\n",
    "    \n",
    "    \n",
    "    #for i in range(len(hierarchy)):#因为上层低概率样本少，所以从第6层开始\n",
    "    for i in [4,5,6,7]:\n",
    "        print(\"\\n\\nlevelIndex:\",i,\"\\nnSamples,nFeatures:\",x_train.shape)\n",
    "        levelIndex = i\n",
    "        numLayers = 4\n",
    "        enc = OneHotEncoder()\n",
    "        nSamples,nFeatures =  x_train.shape\n",
    "       \n",
    "            \n",
    "        yCurLayer1 = [t1[i] for t1 in y_train]\n",
    "        \n",
    "        yCurLayer1 = np.array(yCurLayer1)\n",
    "        print(\"yCurLayer1.shape:\",yCurLayer1.shape)\n",
    "        \n",
    "        yCurLayer1= yCurLayer1.reshape(nSamples,-1)\n",
    "        enc.fit(yCurLayer1)\n",
    "        \n",
    "        yOneHot=enc.transform(yCurLayer1).toarray()\n",
    "        print(\"encInfo:\",enc.categories_,enc.get_feature_names())\n",
    "        print(\"enc.get_feature_names().shape,numLabel:\",enc.get_feature_names().shape[0])\n",
    "        print(\"yOneHot[:1]:\",yOneHot[:1])\n",
    "        \n",
    "        \n",
    "        #num_labels = hierarchy[i] #低概率样本下错误\n",
    "        num_labels = enc.get_feature_names().shape[0] #低概率样本下错误\n",
    "        print(\"num_labels:\", num_labels)\n",
    "        saveName = \"../trainedModes/modelSepStage2-9level%d-%dlayer-2slots-gpu1.h5\" %(i,numLayers)\n",
    "        #saveName = \"../trainedModes/modelSep-2level%d-%dlayer-2slots-gpu1.h5\" %(i,numLayers)#基于拥堵定义的2层结构\n",
    "        print(\"saveName:\",saveName)\n",
    "        \n",
    "        sepHier1(x_train,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs)\n",
    "        \n",
    "        encLevels[str(i)] = enc\n",
    "        xFloors[str(i)] = x_train\n",
    "        yFloors[str(i)] = yCurLayer1\n",
    "        \n",
    "        #print(yCurLayer1)\n",
    "        nSamplesTest,nFeaturesT =  x_test.shape\n",
    "        yCurLayerTest = [t1[i] for t1 in y_test]\n",
    "        yCurLayerTest = np.array(yCurLayerTest)\n",
    "        yCurLayerTest= yCurLayerTest.reshape(nSamplesTest,-1)\n",
    "        \n",
    "        xTestFloors[str(i)] = x_test\n",
    "        yTestFloors[str(i)] = yCurLayerTest\n",
    "        modSaveNameFloors[str(i)] = saveName\n",
    "        \n",
    "    #######保存为pickle文件,用于后期的SUMO和数据分析\n",
    "\n",
    "    fpk=open('stage2-LowprobSamples-addingSumoFeatures-Hierachical.pkf','wb+')  \n",
    "    pickle.dump([xFloors,yFloors,modSaveNameFloors,encLevels,xTestFloors, yTestFloors],fpk)  \n",
    "    fpk.close() \n",
    "\n",
    "\n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "print(\"#############################\\n加入新特征SUMO+阶段1的特征,对低概率样本重新训练多级独立kerasNN的进行分析（已经训练完成）\\n\")\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "def getKerasResnetRVLTmp(x,enc,saveName):\n",
    "    \n",
    "    model_name = saveName \n",
    "    model = keras.models.load_model(model_name)\n",
    "    y= model.predict([x], batch_size=2560)\n",
    "    \n",
    "    nSamples = y.shape[0]\n",
    "   \n",
    "    ###需要将预测出的值，转换01整数,并转为数字式\n",
    "    for i in range(y.shape[0]):\n",
    "        tmp = y[i]\n",
    "        index=  np.argmax(tmp)\n",
    "        y[i] = [0]*y.shape[1]\n",
    "        y[i,index]=1\n",
    "   \n",
    "\n",
    "    ###  \n",
    "    y= enc.inverse_transform(y)\n",
    "    y= y.reshape(-1,nSamples)[0]\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "    return y \n",
    "\n",
    "############################################################################\n",
    "print(\"#############################\\n加入新特征SUMO+阶段1的特征,对低概率样本重新训练多级独立kerasNN的进行分析（已经训练完成）\\n\")\n",
    "\n",
    "fpk=open('stage2-LowprobSamples-addingSumoFeatures-Hierachical.pkf','rb')   \n",
    "[xFloors,yFloors,modSaveNameFloors,encLevels,xTestFloors, yTestFloors]=pickle.load(fpk)  \n",
    "fpk.close()  \n",
    "\n",
    "\n",
    "yKerasFloors = dict()\n",
    "hierarchy = [4,5,6,7]\n",
    "#for i in range(len(hierarchy)):\n",
    "for i in [4,5,6,7]:\n",
    "        levelIndex = i\n",
    "        x = xFloors[str(i)]\n",
    "        yCurLayer1 =  yFloors[str(i)]\n",
    "           \n",
    "    \n",
    "        #x =  xTestFloors[str(i)]\n",
    "        #yCurLayer1 =  yTestFloors[str(i)]\n",
    "        \n",
    "        #print(x)\n",
    "        #print(yCurLayer1)\n",
    "        \n",
    "        saveName =  modSaveNameFloors[str(i)] \n",
    "        enc = encLevels[str(i)]\n",
    "        yOneHot=enc.transform(yCurLayer1).toarray()\n",
    "        yPredict=getKerasResnetRVLTmp(x,enc,saveName)\n",
    "       \n",
    "        print(\"加入新特征SUMO+阶段1的特征,对低概率样本重新训练多级独立kerasNN,分离式多层识别结果进行分析:第%d层\\n\" %i)\n",
    "        \n",
    "        mat1num = confusion_matrix(yCurLayer1,yPredict)\n",
    "        print(mat1num)\n",
    "        mat2acc = confusion_matrix(yCurLayer1,yPredict,normalize='pred')  \n",
    "        print(np.around(mat2acc , decimals=3))\n",
    "        \n",
    "        score = accuracy_score(yCurLayer1,yPredict)\n",
    "        print(score) \n",
    "    \n",
    "        yKerasFloors[str(i)] =  yPredict\n",
    "        \n",
    "        df = pd.DataFrame(np.around(mat2acc , decimals=3))\n",
    "        fs = \"低概率样本+SUMO特征后的识别混淆矩阵%d.csv\" %i\n",
    "        df.to_csv(fs,index= False, header= False)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d4ba6e-a510-46a3-a33b-1c7ea5f15ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "print(\"程序4: 在论文中体现，实现验证幽灵堵车规则\")\n",
    "print(\"接程序3:程序3为根据输入的SUMO模拟特征对低概率样本进行重新训练,从而获得识别正确率提升\")\n",
    "\n",
    "\n",
    "print(\" 预测幽灵堵车.原因为本来这条道路机容易出现幽灵堵车，和有车不停变道。主要为D规则 \\\n",
    "\\nA.如果蒙特卡洛预测能高速通过路口，但是预测minSpeed比较低，同时如果设定车辆延迟比较高时，也不能通过路口，认为出现幽灵堵车 \\\n",
    "\\nB.如果初始模型能预测通过路口，但是加入道路特征和设定车辆延迟比较高时也不能通过路口，也不能通过路口，认为出现幽灵堵车 \\\n",
    "\\nC.如果初始模型能预测能通过路口，但是概率比较低，但是加入道路特征和设定车辆延迟比较高时不能通过路口，认为出现幽灵堵车 \\\n",
    "\\nD.如果初始模型和蒙特卡洛模拟预测都能通过路口，但是概率比较低，但是加入道路特征和设定车辆延迟比较高时不能通过路口，认为出现幽灵堵车 \\\n",
    "\\nE.如果初始模型和蒙特卡洛模拟预测都能通过路口，但是概率比较高，但是加入道路特征和设定车辆延迟比较高时不能通过路口，不知道。\\\n",
    "\\nF.如果原始模型和SUMO增强模型都预测能高速通过路口，但是概率比较低而SUMO模拟预测为不能通过路口，那是什么呢？？也许是提前减速，也就是减低最大速度。\")\n",
    "\n",
    "dfSumoData = pd.read_csv('sumoSimData6375.csv', sep=',')\n",
    "\n",
    "headSumoData = ['sampleIndex','outputAvgSpeed','originOutput','sumoOutputSpeedTag','kerasPredictLabel',\\\n",
    "                                               'NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8',\\\n",
    "                                               'smv1','smv2']\n",
    "\n",
    "dfSimVehParams = pd.read_csv('paramsVehAll6375.csv', sep=',')\n",
    "\n",
    "\n",
    "headSimVehParams = ['sampleIndex','vehLen0','maxAcc0','maxDAcc0','maxSpeed0','reacTime0','minGap0','Impat0','speedFactor0',\\\n",
    "                                       'vehLen','maxAcc','maxDAcc','maxSpeed','reacTime','minGap','Impat','speedFactor',\\\n",
    "                                               'minSpeed0']\n",
    "#实际数据只有6366，因为有些数据进行了筛选，'sampleIndex'对应是\n",
    "\n",
    "fpk=open('stage2-LowprobSamples-addingSumoFeatures-Hierachical.pkf','rb')   \n",
    "[xFloors,yFloors,modSaveNameFloors,encLevels,xTestFloors, yTestFloors]=pickle.load(fpk)  \n",
    "fpk.close()  \n",
    "\n",
    "for i in [7]:\n",
    "        levelIndex = i\n",
    "        x = xFloors[str(i)]\n",
    "        yCurLayer1 =  yFloors[str(i)]\n",
    "           \n",
    "        \n",
    "        #x =  xTestFloors[str(i)]\n",
    "        #yCurLayer1 =  yTestFloors[str(i)]\n",
    "        \n",
    "        #print(x)\n",
    "        #print(yCurLayer1)\n",
    "        \n",
    "        saveName =  modSaveNameFloors[str(i)] \n",
    "        enc = encLevels[str(i)]\n",
    "        \n",
    "        model_name = saveName \n",
    "        model = keras.models.load_model(model_name)\n",
    "        y= model.predict([x], batch_size=2560)\n",
    "        \n",
    "        nSamples,nFeatures = x.shape\n",
    "        \n",
    "        for j in range(nSamples):\n",
    "              \n",
    "            dataVP = dfSimVehParams.loc[dfSimVehParams['sampleIndex'] == j]#模拟100次\n",
    "            dataSD = dfSumoData.loc[dfSumoData['sampleIndex'] == j]\n",
    "            yNN =  y[j]\n",
    "            print('yNN:',np.round(yNN,2))\n",
    "          \n",
    "            \n",
    "            #进行分析\n",
    "            headSumoData = ['sampleIndex','outputAvgSpeed','originOutput','sumoOutputSpeedTag','kerasPredictLabel',\\\n",
    "                                               'NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8',\\\n",
    "                                               'smv1','smv2']\n",
    "          #  redLightTime,distToRedLight,speed,laneAvgSpeed,arriveTime1,arriveTime2,laneID,ArrivalDivRedTime,\\\n",
    "        #vehPos_1,vehSpeed_1,vehPos_2,vehSpeed_2,vehPos_3,vehSpeed_3,vehPos_4,vehSpeed_4,vehPos_5,vehSpeed_5,\\\n",
    "        #vehPos_6,vehSpeed_6,vehPos_7,vehSpeed_7,vehPos_8,vehSpeed_8,vehPos_9,vehSpeed_9,vehPos_10,vehSpeed_10,\\\n",
    "        #vehPos_11,vehSpeed_11,vehPos_12,vehSpeed_12,vehPos_13,vehSpeed_13,vehPos_14,vehSpeed_14,vehPos_15,vehSpeed_15,\\\n",
    "        #vehPos_16,vehSpeed_16,vehPos_17,vehSpeed_17,vehPos_18,vehSpeed_18,vehPos_19,vehSpeed_19,vehPos_20,vehSpeed_20 = tmp\n",
    "\n",
    "        #sample_name = 1(ID)+8(keyFeature)+40(otherVehcle)+6(keyFeatures)+40(otherVehs)+1(flag)= 96\n",
    "        #xlowpra:x-name = 8(keyFeature)+40(otherVehcle)+6(keyFeatures)+40(otherVehs)= 94\n",
    "        #x:x-name = \n",
    "        #8(keyFeature)+40(otherVehcle)+6(keyFeatures)+40(otherVehs)= 94,\n",
    "        #x = np.concatenate([lowproKerasStage1Input,x1_sumoOutput,x2_yKerasOutput,x3_outputListNN,x4_outputAvgSpeed,x5_sumoOutList] = 94+2+9+1+2 = x[j].shape = 108\n",
    "        #sumoOutput='sumoOutputSpeedTag'\n",
    "        #yKerasOutput='kerasPredictLabel'\n",
    "        #originOutput ='originOutput'\n",
    "        #sumoOutList = ['smv1','smv2']\n",
    "        #outputListNN = ['NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8']\n",
    "        #outputAvgSpeed = 'outputAvgSpeed'\n",
    "                                               \n",
    "            print(\"j,originOutput:\" ,dataSD['originOutput'].item())#item对于Series\n",
    "            print(\"j,sumoOutputSpeedTag:\" ,dataSD['sumoOutputSpeedTag'].item())\n",
    "            print(\"j,kerasPredictLabel:\" ,dataSD['kerasPredictLabel'].item())\n",
    "            print(\"j,dataSD:\\n\" ,headSumoData,\"\\n\",dataSD.values)#values对应dataframe\n",
    "            \n",
    "            if j==0:\n",
    "                print(dataVP['maxSpeed0']*3.6)\n",
    "                dataVPTmp1= dataVP[dataVP['maxSpeed0']*3.6<20]['minSpeed0']\n",
    "                print(dataVPTmp1)\n",
    "                print(x[j].shape)\n",
    "                tmp = x[j][0:48]\n",
    "                print(np.round(tmp,2))\n",
    "\n",
    "            \n",
    "            pause(3)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee8de9b-8b77-452c-9596-66454a9cd65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "print(\"根据交通拥堵定义，建立的二层模型进行混淆矩阵分析，需要在在论文中体现，但是没有实现\")\n",
    "########################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe44f65-7db7-4ac2-83e6-4d0844f99241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################################################################\n",
    "print(\"效果不好，没有在论文中体现.接程序2: 综合SUMO输出+stage1的特征，对keras输出进行优化。程序输入为程序2的输出\")\n",
    "print(\"优化选择1.NN。 2 回归分析。3 概率分析。\")\n",
    "print(\"程序编号为3.2,用层次模型递推概率计算\")\n",
    "########################################################################################################################\n",
    "\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "print(\"#################################################################################\")\n",
    "print(\"Hierach Keras\")\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z - np.max(z))\n",
    "    return e_z / e_z.sum(axis=0)\n",
    "def simpleProb1(z):\n",
    "    return z / z.sum(axis=0)\n",
    "\n",
    "    \n",
    "fpk=open('stage2-LowprobSamples-addingSumoFeatures-Hierachical.pkf','rb')   \n",
    "[xFloors,yFloors,modSaveNameFloors,encLevels,xTestFloors, yTestFloors]=pickle.load(fpk)  \n",
    "fpk.close()  \n",
    "\n",
    "\n",
    "#fpk=open('lowprobSamples.pkf','rb')   \n",
    "#[xlowpra,ylowpraLabel,ylowPredictLabel,ylowpraPredictNN]=pickle.load(fpk)  #只是第7层的输出\n",
    "#fpk.close()  \n",
    "\n",
    "xFloors = xTestFloors\n",
    "yFloors = yTestFloors\n",
    "\n",
    "yKerasNNout = dict()\n",
    "hierarchy=[4,5,6,7]\n",
    "for i in [4,5,6,7]:\n",
    "    \n",
    "    xTmp  = xFloors[str(i)]\n",
    "    saveName = modSaveNameFloors[str(i)]\n",
    "    #计算当前层次下的每个样本的正确率\n",
    "    model_name = saveName \n",
    "    model2 = keras.models.load_model(model_name)\n",
    "    \n",
    "    yKerasNNout[str(i)]= model2.predict([xTmp], batch_size=2560)\n",
    "    print(yKerasNNout[str(i)].shape)\n",
    "\n",
    "   # hierarchy = [2,3,4,5,6,7,8,9]\n",
    "    #labelDict = {\"0\":[\"01234\",        \"01234\",        \"01234\",   \"01234\",      \"0123\",\"012\",\"01\",\"0\"],\\\n",
    "    #              \"1\":[\"01234\",        \"01234\",        \"01234\",  \"01234\",     \"0123\",\"012\",\"01\",\"1\"],\\\n",
    "    #              \"2\":[\"01234\",          \"01234\",      \"01234\",  \"01234\",     \"0123\",\"012\",\"2\",\"2\"],\\\n",
    "    #              \"3\":[\"01234\",         \"01234\",       \"01234\",  \"01234\",    \"0123\",\"3\",\"3\",\"3\"],\\\n",
    "    #             \"4\":[\"01234\",          \"01234\",       \"01234\",  \"01234\" ,     \"4\", \"4\",\"4\",\"4\"],\\\n",
    "    #             \"5\":[\"5678\",               \"5\",        \"5\" ,      \"5\",       \"5\", \"5\",\"5\",\"5\"],\\\n",
    "    #             \"6\":[\"5678\",            \"678\",        \"6\",        \"6\",       \"6\", \"6\",\"6\",\"6\"],\\\n",
    "    #             \"7\":[\"5678\",             \"678\",       \"78\",       \"7\",       \"7\", \"7\",\"7\",\"7\"],\\\n",
    "    #             \"8\":[\"5678\",             \"678\",       \"78\" ,      \"8\",        \"8\", \"8\",\"8\",\"8\"],\\\n",
    "    \n",
    "    \n",
    "\n",
    "yHierOut = dict()\n",
    "\n",
    "\n",
    "yL1HierOutLabel=[]\n",
    "yL1KeralOutLabel=[]\n",
    "\n",
    "yL2HierOutLabel=[]\n",
    "yL2KeralOutLabel=[]\n",
    "\n",
    "yL3HierOutLabel=[]\n",
    "yL3KeralOutLabel=[]\n",
    "\n",
    "yL4HierOutLabel=[]\n",
    "yL4KeralOutLabel=[]\n",
    "\n",
    "yL5HierOutLabel=[]\n",
    "yL5KeralOutLabel=[]\n",
    "\n",
    "yL6HierOutLabel=[]\n",
    "yL6KeralOutLabel=[]\n",
    "\n",
    "yL7HierOutLabel=[]\n",
    "yL7KeralOutLabel=[]\n",
    "\n",
    "\n",
    "yL7HierOutNN=[]\n",
    "yL7KeralOutNN=[]\n",
    "\n",
    "for level in [7]:\n",
    "    xTmp  = xFloors[str(level)]\n",
    "    saveName = modSaveNameFloors[str(level)] \n",
    "    for i in range(xTmp.shape[0]):\n",
    "\n",
    "\n",
    "        originKerasTrainInput = xTmp[i]\n",
    "\n",
    "        #第0层，输出为2类，['01234','5678']\n",
    "        #nn0 = yKerasNNout['0'][i]\n",
    "        #第1层，输出为3,['01234','5','678']\n",
    "        #nn1 = yKerasNNout['1'][i]\n",
    "        #nn0to1 = np.insert(nn0,1,nn0[1])\n",
    "        #nn1A = simpleProb1(nn0to1*nn1)\n",
    "\n",
    "        #youtTmp1 = np.argmax(nn1,axis=0)\n",
    "        #youtTmp2 = np.argmax(nn1A,axis=0)\n",
    "        #yL1KeralOutLabel.append(youtTmp1)\n",
    "        #yL1HierOutLabel.append(youtTmp2)\n",
    "\n",
    "        #nn1 = nn1A #从上到下链式更新，如果去掉就不是链式更新\n",
    "\n",
    "        #第2层，输出为4,['01234','5','6','78']\n",
    "        #nn2 = yKerasNNout['2'][i]\n",
    "        #nn1to2 = np.insert(nn1,2,nn1[2])\n",
    "        #nn2A = simpleProb1(nn1to2*nn2)\n",
    "\n",
    "        #youtTmp1 = np.argmax(nn2,axis=0)\n",
    "        #youtTmp2 = np.argmax(nn2A,axis=0)\n",
    "        #yL2KeralOutLabel.append(youtTmp1)\n",
    "        #yL2HierOutLabel.append(youtTmp2)\n",
    "\n",
    "\n",
    "        #nn2 = nn2A #从上到下链式更新，如果去掉就不是链式更新\n",
    "\n",
    "        #第3层，输出为5,['01234','5','6','7','8']\n",
    "        #nn3 = yKerasNNout['3'][i]\n",
    "        #nn2to3 = np.insert(nn2,3,nn2[3])\n",
    "        #nn3A = simpleProb1(nn2to3*nn3)\n",
    "\n",
    "        #youtTmp1 = np.argmax(nn3,axis=0)\n",
    "        #youtTmp2 = np.argmax(nn3A,axis=0)\n",
    "        #yL3KeralOutLabel.append(youtTmp1)\n",
    "        #yL3HierOutLabel.append(youtTmp2)\n",
    "\n",
    "        #nn3 = nn3A #从上到下链式更新，如果去掉就不是链式更新\n",
    "\n",
    "\n",
    "        #第4层，输出为6,['0123','4','5','6','7','8']\n",
    "        #nn4 = yKerasNNout['4'][i]\n",
    "        #nn3to4 = np.insert(nn3,0,nn3[0])\n",
    "        #nn4A = simpleProb1(nn3to4*nn4)\n",
    "\n",
    "        #youtTmp1 = np.argmax(nn4,axis=0)\n",
    "        #youtTmp2 = np.argmax(nn4A,axis=0)\n",
    "        #yL4KeralOutLabel.append(youtTmp1)\n",
    "        #yL4HierOutLabel.append(youtTmp2)\n",
    "\n",
    "        #nn4 = nn4A #从上到下链式更新，如果去掉就不是链式更新\n",
    "\n",
    "        #第4层，输出为6,['0123','4','5','6','7','8']\n",
    "        #第5层，输出为7,['012','3','4','5','6','7','8']\n",
    "        #nn4 = yKerasNNout['4'] #如果从第4层开始，而不是第一层开始\n",
    "        \n",
    "        #nn5 = yKerasNNout['5'][i]\n",
    "        #nn4to5 = np.insert(nn4,0,nn4[0])\n",
    "        #nn5A = simpleProb1(nn4to5*nn5)\n",
    "\n",
    "        #youtTmp1 = np.argmax(nn5,axis=0)\n",
    "        #youtTmp2 = np.argmax(nn5A,axis=0)\n",
    "        #yL5KeralOutLabel.append(youtTmp1)\n",
    "        #yL5HierOutLabel.append(youtTmp2)\n",
    "\n",
    "        #nn5 = nn5A #从上到下链式更新，如果去掉就不是链式更新\n",
    "\n",
    "\n",
    "        #第6层，输出为7,['01','2','3','4','5','6','7','8']\n",
    "        #nn5 = yKerasNNout['5'][i] #如果从第6层开始，而不是第一层开始\n",
    "        \n",
    "        #nn6 = yKerasNNout['6'][i]\n",
    "        #nn5to6 = np.insert(nn5,0,nn5[0])\n",
    "        #nn6A = simpleProb1(nn5to6*nn6)\n",
    "        #nn6A = softmax(nn5to6*nn6)\n",
    "\n",
    "        #youtTmp1 = np.argmax(nn6,axis=0)\n",
    "        #youtTmp2 = np.argmax(nn6A,axis=0)\n",
    "        #yL6KeralOutLabel.append(youtTmp1)\n",
    "        #yL6HierOutLabel.append(youtTmp2)\n",
    "\n",
    "        \n",
    "        #nn6 = nn6A #从上到下链式更新，如果去掉就不是链式更新\n",
    "\n",
    "        #最低层7= len([2,3,4,5,6,7,8,9])-1\n",
    "        \n",
    "        nn6 = yKerasNNout['6'][i] #如果从第7层开始，而不是第一层开始\n",
    "        \n",
    "        nn7 = yKerasNNout['7'][i]\n",
    "        nn6to7= np.insert(nn6,0,nn6[0])\n",
    "        nn7A= simpleProb1(nn6to7*nn7)\n",
    "        \n",
    "        #nn7A = softmax(nn6to7*nn7)\n",
    "\n",
    "\n",
    "        youtTmp1 = np.argmax(nn7,axis=0)\n",
    "        youtTmp2 = np.argmax(nn7A,axis=0)\n",
    "\n",
    "        yL7KeralOutLabel.append(youtTmp1)\n",
    "        yL7HierOutLabel.append(youtTmp2)\n",
    "   \n",
    "    \n",
    "    #print(np.round(nn6,3))\n",
    "    #print(np.round(nn7,3))\n",
    "    #print(np.round(nn6to7,3))\n",
    "    #print(np.round(softmax(nn6to7*nn7),3))\n",
    "    #print(np.round(simpleProb1(nn6to7*nn7),3))\n",
    "    \n",
    "#print(yL7HierOutLabel)\n",
    "#print(yL7KeralOutLabel)\n",
    "#print(yL7HierOutNN)\n",
    "#print(yL7KeralOutNN)\n",
    " \n",
    "######################################################\n",
    "print(\"######################################################\")\n",
    "print(\"经过层次推理后，第7层的正确率：（结果和原始没有很大区别）\")\n",
    "y =yFloors['7'].astype(int)#第7层可以直接文字变数字，其他层不行\n",
    "\n",
    "yPredict = np.array(yL7HierOutLabel).reshape(-1,1)\n",
    "\n",
    "tmp1 = classification_report(y,yPredict)\n",
    "mat1num = confusion_matrix(y,yPredict)\n",
    "mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "score = accuracy_score(yPredict, y)\n",
    "print(score)\n",
    "\n",
    "######################################################\n",
    "print(\"######################################################\")\n",
    "print(\"原始的第7层的正确率：\")\n",
    "y =yFloors['7'].astype(int)\n",
    "\n",
    "\n",
    "#xTmp  = xFloors[str(7)]\n",
    "#saveName = modSaveNameFloors[str(7)] \n",
    "#enc = encLevels[str(7)]\n",
    "#yPredict=getKerasResnetRVLTmp(xTmp,enc,saveName).astype(int)\n",
    "\n",
    "yPredict = yL7KeralOutLabel\n",
    "#print(yPredict)\n",
    "tmp1 = classification_report(y,yPredict)\n",
    "mat1num = confusion_matrix(y,yPredict)\n",
    "mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "score = accuracy_score(yPredict, y)\n",
    "print(score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e71f54b-032b-4853-be5a-5bb737975a9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### ########################################################################################################################   \n",
    "import scipy.interpolate as si\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"##############################################################################################################\")\n",
    "\n",
    "print(\"程序编号为3.3，手动权值分配，没有在论文中体现\")\n",
    "\n",
    "df = pd.read_csv('sumoSimData.csv', sep=',')\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "numSamples,numFeatures = df.shape\n",
    "\n",
    "##['sampleIndex','outputAvgSpeed','originOutput','sumoOutputSpeedTag','kerasPredictLabel','smv1','smv2',\\\n",
    " ##                                              'NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8'])\n",
    "sumoOutput='sumoOutputSpeedTag'\n",
    "yKerasOutput='kerasPredictLabel'\n",
    "originOutput ='originOutput'\n",
    "sumoOutList = ['smv1','smv2']\n",
    "outputListNN = ['NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8']\n",
    "\n",
    "\n",
    "\n",
    "df1 = df[originOutput]\n",
    "yo = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "df1 = df[sumoOutput]\n",
    "x1 = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "df1 = df[yKerasOutput]\n",
    "x2 = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "df1 = df[outputListNN]\n",
    "x3 = df1.iloc[0:numSamples].to_numpy()\n",
    "\n",
    "x = np.concatenate([x1,x2,x3],axis=1)\n",
    "print(x)\n",
    "\n",
    "\n",
    "print(\"#############################\\n手动给权值1,2\\n\")\n",
    "\n",
    "manualOut = np.zeros((x3.shape[0],1))\n",
    "manualOut2 = np.zeros((x3.shape[0],1))\n",
    "#for i in range(10):\n",
    "for i in range(x3.shape[0]):\n",
    "    #print(i)\n",
    "    nn1 = x3[i]\n",
    "    sumoOut = min(8,x1[i][0]+2)\n",
    "    kerasOut = x2[i][0]\n",
    "    originOut = yo[i][0]\n",
    "\n",
    "    #手动给权值1,方法1用插值模型，似乎效果不好\n",
    "    xIntp=[0,sumoOut,min(8,sumoOut+1),min(8,sumoOut+2),min(8,sumoOut+3),9]\n",
    "    yIntp =[0,1,0.5,0.4,0.3,0]\n",
    "    \n",
    "    xIntp=[-9,sumoOut-1,sumoOut,sumoOut+1,9]\n",
    "    yIntp =[0.5,0.8,1,0.8,0.5]\n",
    "    f = si.interp1d(xIntp,  yIntp,kind=1)\n",
    "    xi = [0,1,2,3,4,5,6,7,8]\n",
    "    p= f(xi)\n",
    "    yTmp = np.multiply(p,nn1)\n",
    "    finalIndex = np.argmax(yTmp)\n",
    "    manualOut[i] = finalIndex\n",
    "    \n",
    "    #print('sumoOut','kerasOut','originOut','finalIndex')\n",
    "    #print(sumoOut,kerasOut,originOut,finalIndex)\n",
    "    #print(\"nn1:\",nn1)\n",
    "    #print(\"p  :\",np.round(p,decimals=3))\n",
    "    #print(\"y3 :\",np.round(y3,decimals=3))\n",
    "    \n",
    "    #手动给权值2,方法2用集团模型\n",
    "    if sumoOut <= 1:\n",
    "        p = [1,1,1,1,1,1,0,0,0]\n",
    "        yTmp = np.multiply(p,nn1)\n",
    "        finalIndex = np.argmax(yTmp)\n",
    "        manualOut2[i] = finalIndex\n",
    "    \n",
    "    if sumoOut > 1:\n",
    "        p = [0.0,0,0,0,0,0,1,1,1]\n",
    "        yTmp = np.multiply(p,nn1)\n",
    "        finalIndex = np.argmax(yTmp)\n",
    "        manualOut2[i] = finalIndex\n",
    "\n",
    "print(\"#############################\\n手动给权值1结果\\n\")\n",
    "print(yo.shape)\n",
    "print(manualOut.shape)\n",
    "tmp1 = classification_report(yo,manualOut)\n",
    "mat1num = confusion_matrix(yo,manualOut)\n",
    "mat2acc = confusion_matrix(yo,manualOut,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "score = accuracy_score(manualOut, yo)\n",
    "print(score) \n",
    "\n",
    "\n",
    "print(\"#############################\\n手动给权值2结果\\n\")\n",
    "print(yo.shape)\n",
    "print(manualOut2.shape)\n",
    "tmp1 = classification_report(yo,manualOut2)\n",
    "mat1num = confusion_matrix(yo,manualOut2)\n",
    "mat2acc = confusion_matrix(yo,manualOut2,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "score = accuracy_score(manualOut2, yo)\n",
    "print(score) \n",
    "\n",
    "\n",
    "print(\"#############################\\n原生keras\\n\")\n",
    "yPredict = x2\n",
    "tmp1 = classification_report(yo,yPredict)\n",
    "mat1num = confusion_matrix(yo,yPredict)\n",
    "mat2acc = confusion_matrix(yo,yPredict,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "score = accuracy_score(yPredict, yo)\n",
    "print(score) \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "print(\"############################################模拟退火\")\n",
    "print(\"############################################https://www.jb51.net/article/269941.htm\")\n",
    "#https://blog.csdn.net/ljyljyok/article/details/100552618\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import dual_annealing\n",
    "\n",
    "def objective(x,*args):\n",
    "    x1,x2,x3,yo = args\n",
    "    manualOut = np.zeros((x3.shape[0],1))\n",
    "    for i in range(x3.shape[0]):\n",
    "        #print(i)\n",
    "        nn1 = x3[i]\n",
    "        sumoOut = x1[i][0]\n",
    "        kerasOut = x2[i][0]\n",
    "        originOut = yo[i][0]\n",
    "\n",
    "        \n",
    "\n",
    "        xIntp=[-9,sumoOut-x[0],sumoOut,sumoOut+x[1],9]\n",
    "        yIntp =[0.5,x[2],1,x[2],0.5]\n",
    "        f = si.interp1d(xIntp,  yIntp,kind=1)\n",
    "        xi = [0,1,2,3,4,5,6,7,8]\n",
    "        p= f(xi)\n",
    "        yTmp = np.multiply(p,nn1)\n",
    "        finalIndex = np.argmax(yTmp)\n",
    "        manualOut[i] = finalIndex\n",
    "\n",
    "    score = accuracy_score(manualOut, yo)\n",
    "    \n",
    "    return -score\n",
    "\n",
    "\n",
    "\n",
    "#args1 = (x1,x2,x3,yo)\n",
    "#x0 = [1,1,0.3]\n",
    "bounds1 = ((0, 4), (0, 4),(0.01, 0.9))\n",
    "\n",
    "#constraints = {'type': 'ineq', 'fun': cons}\n",
    "\n",
    "#res = minimize(objective, x0, args=args1,method='SLSQP',bounds=bounds1)\n",
    "#print(res.fun)\n",
    "#print(res.success)\n",
    "#print(res.x)\n",
    "#https://vimsky.com/zh-tw/examples/usage/python-scipy.optimize.dual_annealing.html\n",
    "args1 = (x1,x2,x3,yo)\n",
    "x0 = [1,1,0.3]\n",
    "bounds1 = [[0, 4], [0, 4],[0.01, 0.9]]\n",
    "res = dual_annealing(objective,bounds1,x0=x0, args=args1)\n",
    "print(res.fun)\n",
    "print(res.success)\n",
    "print(res.x)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add02d9c-af03-4a00-8629-42b8a751033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "print(\"辅助程序 对模拟后的数据进行分析，计算正确率\")\n",
    "########################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"对于低概率样本的识别结果，采用keras和MCS的识别结果对比\")\n",
    "file1 = \"./data-Copy1.csv\"\n",
    "xyDataTmp = pd.read_csv(file1,index_col=0)\n",
    "\n",
    "print(xyDataTmp.head(3))\n",
    "print(xyDataTmp.info())\n",
    "\n",
    "file1 = \"。./trainData/france_0_allSamples1.csv\"\n",
    "xyOrigin = pd.read_csv(file1,index_col=0)\n",
    "\n",
    "originlabel =  xyDataTmp.iloc[:,1].to_numpy()\n",
    "keraslabel =   xyDataTmp.iloc[:,2].to_numpy()      \n",
    "mcslabel =     xyDataTmp.iloc[:,3].to_numpy()\n",
    "\n",
    "#\n",
    "    \n",
    "print('\\norigin_mcs')\n",
    "mat1num = confusion_matrix(originlabel, mcslabel)\n",
    "mat2acc = confusion_matrix(originlabel, mcslabel,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "      \n",
    "print('\\nmcs_keras')\n",
    "mat1num = confusion_matrix(mcslabel, keraslabel)\n",
    "mat2acc = confusion_matrix(mcslabel, keraslabel,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "\n",
    "print('\\norgin_keras')\n",
    "mat1num = confusion_matrix(originlabel, keraslabel)\n",
    "mat2acc = confusion_matrix(originlabel ,keraslabel,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))      \n",
    "\n",
    "##用于分析实际标记类别大于预测标记类别\n",
    "def analyzing1(tmp, xyDataTmp,xyOrigin): \n",
    "    dfTmp1 = xyDataTmp[tmp]\n",
    "    #print(dfTmp1.head(5))\n",
    "    \n",
    "    \n",
    "    \n",
    "    df2 =  xyOrigin.iloc[dfTmp1.originIndex,:]   \n",
    "    plt.show()\n",
    "    df2[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    #print(df2.info())\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    \n",
    "    df2.to_csv(\"tmpForAnalyzing.csv\")\n",
    "    \n",
    "    tmp1 = df2['redLightTime'] - df2['arriveTime2'] >1.5 #红灯时间大于到达时间\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 = df2[df2['redLightTime'] - df2['arriveTime2'] >1.5] #红灯时间大于到达时间 ,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing3.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    \n",
    "   \n",
    "    tmp1 = df2['speed'] < 5/3.6 #本身速度就小于5/3.6\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df2['speed'] > 5/3.6 #本身速度就小于5/3.6,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    df3.to_csv(\"tmpForAnalyzing4.csv\")\n",
    "    \n",
    "    tmp1 = df2['redLightTime'] - df2['arriveTime2'] >1.5  #红灯时间大于到达时间\n",
    "    tmp1 = tmp1 | (df2['speed'] < 5/3.6) #本身速度就小于5/3.6\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"红灯时间大于到达时间  or 本身速度就小于5/3.6,df3 shape:\",df3.shape,\"占输入样本比例为:\",df3.shape[0]/df2.shape[0])\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    df3.to_csv(\"tmpForAnalyzing5.csv\")\n",
    "    \n",
    "    \n",
    "    tmp1 = df2['arriveTime2'] - df2['redLightTime'] >0 #到达时间大于红灯时间\n",
    "    tmp1 = tmp1 & (df2['speed'] > 5/3.6) #本身速度就大于于5/3.6\n",
    "    tmp1 = tmp1 & (df2['vehPos_2'] > 0) #\n",
    "    tmp1 = tmp1 & (df2['vehSpeed_2'] < 5/3.6) #\n",
    "    tmp1 = tmp1 & (df2['vehPos_3'] >0) #\n",
    "    tmp1 = tmp1 & (df2['vehSpeed_3'] <5/3.6) #\n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"到达时间大于红灯时间  and 本身速度就大于5/3.6,df3 shape:\",df3.shape,\"占输入样本比例为:\",df3.shape[0]/df2.shape[0])\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    df3.to_csv(\"tmpForAnalyzing6.csv\")\n",
    "\n",
    "def extractStillVeh2(df):\n",
    "    df=df.rename(columns={'ArrTimeDivRedTime':'numStillVeh'})\n",
    "    df=df.rename(columns={'lanAvgSpeed':'predictStats'})\n",
    "    df['numStillVeh'] = 0\n",
    "    df['predictStats'] = \"unknown\"\n",
    "    for i in range(df.shape[0]):\n",
    "        numStillVeh = 0\n",
    "        tmp = df.iloc[i]\n",
    "        redTime = tmp.iloc[0]\n",
    "        vPosObj = tmp.iloc[1]\n",
    "        predictStats = -1\n",
    "\n",
    "        for j in range(20):\n",
    "           \n",
    "            vehPos = tmp.iloc[2*j+8]\n",
    "            vehVeh = tmp.iloc[2*j+1+8]\n",
    "            \n",
    "            if vehPos >0 and vehVeh<5/3.6:#经验数据,参数\n",
    "                numStillVeh = numStillVeh + 1\n",
    "            elif vehPos >0 and  vPosObj > vehPos:\n",
    "                timeTmp1 =(vehPos-j*6.5)/(vehVeh+0.001)#经验公式，到固定位置后，启动需要的时间\n",
    "                if timeTmp1  < redTime +numStillVeh*1.5:\n",
    "                    numStillVeh = numStillVeh + 1\n",
    "\n",
    "            if vehPos >0 and vPosObj == vehPos and vehVeh<5/3.6:\n",
    "                predictStats = \"stop\"#目标车要听停止\n",
    "               \n",
    "\n",
    "            if vehPos >0 and vPosObj == vehPos and vehVeh>5/3.6 :    \n",
    "                timeTmp1 =(vehPos-j*6.5)/(vehVeh+0.001)#经验公式，到固定位置后需要的时间\n",
    "                if timeTmp1  <= redTime +numStillVeh*1.5+1.5:#小于虚拟红灯结束时间\n",
    "                     predictStats = \"stop\" #目标车要听停止\n",
    "                else:        \n",
    "                     predictStats = \"no stop\"  #目标车要不要停止\n",
    "                \n",
    "     \n",
    "        df['numStillVeh'][i] = numStillVeh\n",
    "        df['predictStats'][i] = predictStats\n",
    "\n",
    "    return df\n",
    "##用于分析实际标记类别小于预测标记类别， xyDataTmp[\"predicted Labels By MCS\"] - xyDataTmp[\"origin speedFlag\"]>0\n",
    "def analyzing2(tmp, xyDataTmp,xyOrigin): \n",
    "    dfTmp1 = xyDataTmp[tmp]\n",
    "    #print(dfTmp1.head(5))\n",
    "    \n",
    "    \n",
    "    #1\n",
    "    df2 =  xyOrigin.iloc[dfTmp1.originIndex,:]   \n",
    "    plt.show()\n",
    "    df2[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    #print(df2.info())\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df2 = extractStillVeh2( df2)\n",
    "    df2.to_csv(\"tmpForAnalyzing.csv\")\n",
    "    \n",
    "    \n",
    "    ######################################\n",
    "    tmp1 = (df2['speedFlag'] == 0)  & (df2['predictStats'] == \"stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing1.csv\")\n",
    "    \n",
    "    tmp1 = (df2['speedFlag'] > 0)  & (df2['predictStats'] == \"no stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing2.csv\")\n",
    "    \n",
    "    \n",
    "    tmp1 = (df2['speedFlag'] == 0)  & (df2['predictStats'] == \"no stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing3.csv\")\n",
    "    \n",
    "    tmp1 = (df2['speedFlag'] > 0)  & (df2['predictStats'] == \"stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing4.csv\")\n",
    "    return\n",
    "    \n",
    "    '''\n",
    "    #2\n",
    "    tmp1 = df2['redLightTime'] - df2['arriveTime2'] >0 #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 = df2[df2['redLightTime'] - df2['arriveTime2'] >0] #红灯时间大于到达时间 ,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing2.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    \n",
    "    #3\n",
    "    tmp1 =df2['arriveTime2'] - df2['redLightTime'] >3 #红灯时间小于到达时间3，\n",
    "    tmp1 = tmp1 & (df2['speedFlag'] == 0) \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"#红灯时间小于于到达时间 ,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing3.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))  \n",
    "    \n",
    "   \n",
    "    #4据静止汽车数目，分析在df2['speedFlag'] > 0情况下，虚拟红灯时间小于于到达时间情况，也就是目标车可能不需要停下来\n",
    "    \n",
    "    tmp1 =(df2['speedFlag'] == 0) \n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 < df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 = tmp1 & tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing4.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))  \n",
    "    \n",
    "    #5 据静止汽车数目，分析在df2['speedFlag'] > 0情况下，虚拟红灯时间大于到达时间情况，也就是目标车可能需要停下来\n",
    "    tmp1 =(df2['speedFlag'] > 0) \n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 >= df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 = tmp1 & tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing5.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))  \n",
    "    \n",
    "    \n",
    "    #6 据静止汽车数目，分析虚拟红灯时间大于到达时间情况，也就是目标车可能需要停下来\n",
    "  \n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 >= df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 =  tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing6.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    \n",
    "    \n",
    "    #7 根据静止汽车数目，分析虚拟红灯时间小于到达时间情况，也就是目标车可能不需要停下来\n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']+1.5\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 < df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 =  tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing7.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4)) \n",
    "   '''\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "tmp = (xyDataTmp[\"origin speedFlag\"] - xyDataTmp[\"predicted Labels By MCS\"] >0) \n",
    "\n",
    "#analyzing1(tmp, xyDataTmp,xyOrigin)\n",
    "\n",
    "tmp = xyDataTmp[\"origin speedFlag\"] - xyDataTmp[\"predicted Labels By MCS\"]  >=3 \n",
    "#analyzing(tmp, xyDataTmp,xyOrigin)\n",
    "\n",
    "tmp = xyDataTmp[\"predicted Labels By MCS\"] - xyDataTmp[\"origin speedFlag\"]>0\n",
    "analyzing2(tmp, xyDataTmp,xyOrigin)\n",
    "      \n",
    "tmp = xyDataTmp[\"predicted Labels By MCS\"] - xyDataTmp[\"origin speedFlag\"]>=3\n",
    "#analyzing2(tmp, xyDataTmp,xyOrigin)\n",
    "\n",
    "#手动修改\n",
    "\n",
    "    \n",
    "      \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46c252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T10:10:55.559866Z",
     "start_time": "2023-01-27T10:10:53.398090Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c877a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T09:04:50.440620Z",
     "start_time": "2023-01-28T09:04:50.239643Z"
    }
   },
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b3f88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-29T03:05:13.857103Z",
     "start_time": "2023-01-29T03:05:13.853125Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "timestr= datetime.now()\n",
    "print(timestr)\n",
    "\n",
    "!conda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281fc84-e6c1-4671-9323-70c1fdbb7210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf tmp*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75a25a-12fd-4f04-85e2-c2a78f766298",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[11, 3, 4 ,5],[6, 7, 8, 9]])\n",
    "print(np.where(arr < 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0697b-8759-40f4-8556-8ec1bfa76aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.interpolate as si\n",
    "\n",
    "%matplotlib inline\n",
    "from numpy import polyfit, poly1d\n",
    "x=[0,3,8]\n",
    "y =[0,1,0]\n",
    "coeff = polyfit(x, y, 2)\n",
    "print(coeff)\n",
    " \n",
    "p = plt.plot(x, y, 'rx')\n",
    "\n",
    "x=[0,3,8]\n",
    "y =[0.5,1,0.5]\n",
    "\n",
    "x1 = np.linspace(0, 8, 100)\n",
    "y1 = np.polyval(coeff, x1)\n",
    "p = plt.plot(x1,y1, 'k-')\n",
    "\n",
    "\n",
    "f = si.interp1d(x, y,kind=1)\n",
    "y2= f(x1)  #调用经由interp1d返回的函数\n",
    "p = plt.plot(x1,y2, 'b-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8290fe-e417-434f-b337-6a5cb390c97a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5])\n",
    "b = np.array([5,5,6,7,8])\n",
    "c = a*b\n",
    "c\n",
    "\n",
    "a = [2,2,3,4,1]\n",
    "b = [5,5,6,7,8]\n",
    "d = np.multiply(a,b)\n",
    "d\n",
    "\n",
    "a =np.array(['1','2'])\n",
    "b = a.astype(int)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b28932-e69e-4875-bd40-c9f6ea1a46eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1,2,3],[4,5,6]]\n",
    "b = [[7,8,9],[11,12,13]]\n",
    "c = a+b\n",
    "print(c)\n",
    "\n",
    "a.extend(b)\n",
    "print(a)\n",
    "\n",
    "#a.append(b)\n",
    "#print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab0426-0753-429d-bf14-73128670f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,-5]\n",
    "b= min(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b98d3d1-78e7-4ab7-b7c9-1d71b2ab5b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d07d66-daff-4b8b-affa-24ae8575ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. 以何种方式过红灯,如何优化\n",
    "A.红灯状态下，如果预测minSpeed为高速而且概率较高，那样可以认为车辆能以高速或者一般速度通过前方路口，可以进行模拟,看几个速度可以通过\n",
    "如果我已经知道我会以较高速度通过路口，不做模拟，直接给出值\n",
    "B.红灯状态下，如果预测minSpeed为低速而且概率较高，那样可以认为车辆一般无法不停车通过路口\n",
    "如果我已经知道我会以较低度通过路口，不做模拟，直接给出值\n",
    "C.中间状态呢，红灯状态下如果预测minSpeed为高速而且概率较低，可以进行模拟,看几个速度可以通过\n",
    "D.中间状态呢，红灯状态下如果预测minSpeed为低速而且概率较低，可以进行模拟,看几个速度可以通过\n",
    "\n",
    "2.预测幽灵堵车\n",
    "原因式本来这条道路机容易出现幽灵堵车，和有车不停变道\n",
    "\n",
    "A.如果蒙特卡洛预测能高速通过路口，但是预测minSpeed比较低，同时如果设定车辆延迟比较高时，也不能通过路口，认为出现幽灵堵车\n",
    "B.如果初始模型能预测通过路口，但是加入道路特征和设定车辆延迟比较高时也不能通过路口，也不能通过路口，认为出现幽灵堵车\n",
    "C.如果初始模型能预测能通过路口，但是概率比较低，但是加入道路特征和设定车辆延迟比较高时不能通过路口，认为出现幽灵堵车\n",
    "D.如果初始模型和蒙特卡洛模拟预测都能通过路口，但是概率比较低，但是加入道路特征和设定车辆延迟比较高时不能通过路口，认为出现幽灵堵车\n",
    "E.如果初始模型和蒙特卡洛模拟预测都能通过路口，但是概率比较高，但是加入道路特征和设定车辆延迟比较高时不能通过路口，不知道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87838dd9-2153-46b2-8141-ca5c75415891",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "A=pd.DataFrame({'a':[1,2,3],'b':[2,3,4],'c':[2,4,5]})\n",
    "A=pd.DataFrame([[1,2,3],[2,3,4],[2,4,5]])\n",
    "print(A)\n",
    "print(A.shape)\n",
    "print(A.iloc[:,-1])\n",
    "\n",
    "A= A.drop(labels=[0],axis=0)\n",
    "print(A)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8083560-9008-4ba9-b319-89bb067371b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor23py36gpu",
   "language": "python",
   "name": "tensor23py36gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b27f224da048d073ae2b306b979c73d2559eaa860bf21b792b51024f42769a7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
