{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f6c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T03:55:17.555121Z",
     "start_time": "2023-01-26T03:55:17.435109Z"
    }
   },
   "outputs": [],
   "source": [
    "#一些常用的命令\n",
    "!nvidia-smi\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#一些常用的命令\n",
    "!git status\n",
    "!git add .\n",
    "!git commit -m \"correct and optimize some code\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0809da8b-07fe-447a-b1fd-d7520e79d721",
   "metadata": {},
   "outputs": [],
   "source": [
    "############print(\"程序0.000 全局共享函数，运行主程序前需要运行\")\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import pickle  \n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "#######开始为功能函数\n",
    "print(\"全局共享函数，运行主程序前需要运行\")\n",
    "def dtFitAndSave(x,y,saveName):\n",
    "    str1=\"dtFitAndSave,用于决策树拟合和识别\"\n",
    "    \n",
    "    dt = tree.DecisionTreeClassifier(max_depth=7,min_samples_leaf=100)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    #data=tree.export_graphviz(dt, out_file=None,class_names=None,filled=True) \n",
    "    #graph = graphviz.Source(data)\n",
    "    #graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(\"纯决策树的识别\\n\",tmp1)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    #text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    return dt,yPredict\n",
    "\n",
    "########################################################################################################################\n",
    "###简单模型3，resnet_like\n",
    "def local_model(num_labels, dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(num_labels, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def global_model(dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='sigmoid',name=\"global\"))\n",
    "    return model\n",
    "\n",
    "def softmax_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='softmax',name=\"global\"))\n",
    "    return model\n",
    "'''\n",
    "############################################################################\n",
    "############################################################################\n",
    "#单层模型\n",
    "def kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels,saveName):\n",
    "    str1=\"kerasFitAndSaveSimple3LikeResnet,用于resnet_like的神经网络拟合和识别\"\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 512\n",
    "    dropout_rate = 0.05\n",
    "    hierarchy = [1,1,1,1]#四层，对于当前数据集已经足够了\n",
    "    global_models = []\n",
    "   \n",
    "    \n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(len(hierarchy)):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "\n",
    "   \n",
    "    \n",
    "    p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "       build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file='KerasSimple3_likeResnet_4lay512nodes.jpg', show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=1500, batch_size=40000*1)#GPU用这个\n",
    "    #saveName = \"KerasSimple3_likeResnet.h5\"\n",
    "    build_model.save(saveName)\n",
    "    plot_model(build_model, to_file='KerasSimple3_likeResnet_4lay512nodes.jpg', show_shapes=True)\n",
    "    return build_model\n",
    "\n",
    "############################################################################\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，\n",
    "def kerasFitAndSaveHierSimple4LikeResnet(x,yOneHot,num_labels,saveName):\n",
    "    str1=\"kerasFitAndSaveHierSimple4LikeResnet,用于resnet_like的 神经网络拟合和识别\"\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    beta = 0.5\n",
    "    hierarchy = [2,4,6,8,9]#5层，对于当前数据集已经足够了\n",
    "    global_models = []\n",
    "    local_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(len(hierarchy)):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "    \n",
    "    for i in range(len(hierarchy)):\n",
    "        local_models.append(local_model(hierarchy[i], dropout_rate, relu_size)(global_models[i]))\n",
    "        \n",
    "        \n",
    "    #显示只有局部局模型的情况(部分全局)\n",
    "    p_loc = layers.concatenate(local_models)\n",
    "    #modelTmp2 = tf.keras.Model(inputs=[features], outputs=[p_loc])\n",
    "    #modelTmp2.summary()#\n",
    "    #plot_model(modelTmp2, to_file='Flatten2.png', show_shapes=True)\n",
    "    p_glob1 = layers.Lambda(lambda x: x*beta,name=\"global\")(p_glob)\n",
    "    p_loc1 = layers.Lambda(lambda x: x*(1-beta),name=\"local\")(p_loc)\n",
    "\n",
    "    labels = layers.add([p_glob1, p_loc1])\n",
    "\n",
    "\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[labels])\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file='hmcnf1.jpg', show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=3500, batch_size=40000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "'''\n",
    "############################################################################\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，所以采用分离式\n",
    "###每一层的识别模型都是4层模型\n",
    "def g_sepHier1(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 10,srelu_size = 256,dropout_rate = 0.05):\n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.01\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    if 0:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=40000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "def getKerasResnetRVL(x,enc,saveName):\n",
    "    model_name = saveName \n",
    "    model = keras.models.load_model(model_name)\n",
    "    y= model.predict([x], batch_size=2560)\n",
    "    nSamples = y.shape[0]\n",
    "    ###需要将预测出的值，转换01整数,并转为数字式\n",
    "    for i in range(y.shape[0]):\n",
    "        tmp = y[i]\n",
    "        index=  np.argmax(tmp)\n",
    "        y[i] = [0]*y.shape[1]\n",
    "        y[i,index]=1\n",
    "   \n",
    "\n",
    "    ###  \n",
    "    y= enc.inverse_transform(y)\n",
    "    y= y.reshape(-1,nSamples)[0]\n",
    "    \n",
    "    \n",
    "    return y\n",
    "\n",
    "def string2int(inputString):\n",
    "     #print(inputString)\n",
    "     tmp = 0\n",
    "     try:\n",
    "         strTmp=[str(ord(x)) for x in inputString]\n",
    "         tmp=tmp.join(strTmp)\n",
    "         tmp = float(tmp)/(len(inputString)*128)\n",
    "     except:\n",
    "         #print(inputString)\n",
    "         strTmp = inputString\n",
    "         tmp= \"0\"\n",
    "         tmp = 0\n",
    "     return tmp\n",
    " ## 根据经验以及最佳正确率的合并方法\n",
    " #第一次合并为0,1的合并\n",
    "\n",
    "########################################################################################################################\n",
    "##手工确定层次结构，以前测试时候为5层，根据论文为9层\n",
    "def convertY2Hieral(y):\n",
    "    #mat2acc\n",
    "    # [[0.914 0.009 0.017 0.007 0.032 0.    0.    0.    0.   ]\n",
    "    # [0.027 0.984 0.006 0.007 0.018 0.    0.    0.    0.   ]\n",
    "    # [0.02  0.006 0.972 0.    0.011 0.    0.    0.    0.   ]\n",
    "    # [0.036 0.002 0.    0.986 0.014 0.    0.002 0.    0.   ]\n",
    "    # [0.003 0.    0.    0.    0.925 0.    0.    0.    0.   ]\n",
    "    # [0.    0.    0.    0.    0.    1.    0.005 0.    0.   ]\n",
    "    # [0.    0.    0.    0.    0.    0.    0.993 0.    0.004]\n",
    "    # [0.    0.    0.    0.    0.    0.    0.    0.996 0.   ]\n",
    "    # [0.    0.    0.004 0.    0.    0.    0.    0.004 0.996]]\n",
    "    \n",
    "    \n",
    "    #hierarchy = [2,4,6,8,9]\n",
    "   # labelDict = {\"0\":[\"01234\",\"0123\",\"012\",\"01\",\"0\"],\\\n",
    "   #               \"1\":[\"01234\",\"0123\",\"012\",\"01\",\"1\"],\\\n",
    "   #               \"2\":[\"01234\",\"0123\",\"012\",\"2\",\"2\"],\\\n",
    "   #               \"3\":[\"01234\",\"0123\",\"3\",\"3\",\"3\"],\\\n",
    "   #              \"4\":[\"01234\",\"4\",    \"4\",\"4\",\"4\"],\\\n",
    "   #              \"5\":[\"5678\",\"5\",     \"5\",\"5\",\"5\"],\\\n",
    "   #              \"6\":[\"5678\",\"678\",   \"67\",\"6\",\"6\"],\\\n",
    "   #              \"7\":[\"5678\",\"678\",   \"67\",\"7\",\"7\"],\\\n",
    "   #              \"8\":[\"5678\",\"678\",   \"8\",\"8\",\"8\"],\\\n",
    "   #               }\n",
    "    \n",
    "    hierarchy = [2,3,4,5,6,7,8,9]\n",
    "    labelDict = {\"0\":[\"01234\",        \"01234\",        \"01234\",   \"01234\",      \"0123\",\"012\",\"01\",\"0\"],\\\n",
    "                  \"1\":[\"01234\",        \"01234\",        \"01234\",  \"01234\",     \"0123\",\"012\",\"01\",\"1\"],\\\n",
    "                  \"2\":[\"01234\",          \"01234\",      \"01234\",  \"01234\",     \"0123\",\"012\",\"2\",\"2\"],\\\n",
    "                  \"3\":[\"01234\",         \"01234\",       \"01234\",  \"01234\",    \"0123\",\"3\",\"3\",\"3\"],\\\n",
    "                 \"4\":[\"01234\",          \"01234\",       \"01234\",  \"01234\" ,     \"4\", \"4\",\"4\",\"4\"],\\\n",
    "                 \"5\":[\"5678\",               \"5\",        \"5\" ,      \"5\",       \"5\", \"5\",\"5\",\"5\"],\\\n",
    "                 \"6\":[\"5678\",            \"678\",        \"6\",        \"6\",       \"6\", \"6\",\"6\",\"6\"],\\\n",
    "                 \"7\":[\"5678\",             \"678\",       \"78\",       \"7\",       \"7\", \"7\",\"7\",\"7\"],\\\n",
    "                 \"8\":[\"5678\",             \"678\",       \"78\" ,      \"8\",        \"8\", \"8\",\"8\",\"8\"],\\\n",
    "                  }\n",
    "    '''\n",
    "    hierarchy = [5,9]\n",
    "    labelDict = {\"0\":[\"01\",\"0\"],\\\n",
    "                  \"1\":[\"01\",\"1\"],\\\n",
    "                  \"2\":[\"2\",\"2\"],\\\n",
    "                  \"3\":[\"34\",\"3\"],\\\n",
    "                \"4\":[\"34\",\"4\"],\\\n",
    "                 \"5\":[\"56\",\"5\"],\\\n",
    "                 \"6\":[\"56\",\"6\"],\\\n",
    "                 \"7\":[\"78\",\"7\"],\\\n",
    "                 \"8\":[\"78\",\"8\"],\\\n",
    "                 }\n",
    "    '''\n",
    "\n",
    "    y1 = [list(labelDict[str(x)]) for x in y]\n",
    "   \n",
    "    #print(\"!!!y1.type:\", type(y1))\n",
    "    #print(y1[:2])\n",
    "    #y2 = [t1[0] for t1 in y1]\n",
    "    #print(len(y2))\n",
    "  \n",
    "\n",
    "    return y1,hierarchy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dbf652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T14:33:49.232503Z",
     "start_time": "2023-02-13T14:31:39.188869Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##主程序开始######################################################################################################################\n",
    "print(\"0.主程序开始，建立多层嵌套决策树模型，3080ti的GPU是AMD2400CPU 运算速度100倍\")\n",
    "print(\"0.这是简化程序，原始带有更多测试和原始模型的程序在mainTestCSVMLP3(hmcnf_keras).ipynb\")\n",
    "print(\"程序编号为0\")\n",
    "########################################################################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import pickle  \n",
    "\n",
    "\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，所以采用分离式\n",
    "###每一层的识别模型都是4层模型\n",
    "def sepHier1(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 10,srelu_size = 256,dropout_rate = 0.05):\n",
    "    \n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    featuresInput = layers.Input(shape=(features_size,))\n",
    "    features = layers.BatchNormalization()(featuresInput)\n",
    "    #features=featuresInput\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[featuresInput], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        #build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=160000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "print(\"0.主程序开始, 建立多层嵌套决策树模型,3080ti的GPU是AMD2400CPU 运算速度100倍\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "#from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "#policy = mixed_precision.Policy('mixed_float16')\n",
    "#mixed_precision.set_policy(policy)\n",
    "\n",
    " \n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "print(\"读取France数据并且把数据进行onehot处理\")\n",
    "\n",
    "#file1 = \"../trainData/france_0_allSamples1.csv\"\n",
    "file1 = \"../trainData/france_0_allSamples1_2slot.csv\"\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "h,w = xyData.shape\n",
    "#x = xyData[:,1:23]#简单处理与SUMO数据库一致\n",
    "x0rigin = xyData[:,1:w-1]#用所有的数据,第0列为vehID,不要\n",
    "y0rigin  = xyData[:,w-1]\n",
    "\n",
    "x0rigin[:,6] = [string2int(inputString) for inputString in x0rigin[:,6] ]#字符串vehLaneID 变为整数\n",
    "\n",
    "x0rigin =x0rigin.astype(np.float32)#GPU 加这个\n",
    "y0rigin =y0rigin.astype(np.int64)#GPU 加这个\n",
    "\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "x0,y0= ros.fit_resample(x0rigin , y0rigin)#对数据不平衡进行处理，保证样本数一致\n",
    "\n",
    "x0=x0.astype(np.float32)#GPU 加这个\n",
    "y0=y0.astype(np.int64)#GPU 加这个\n",
    "yl5 = y0\n",
    "print(\"x0.shape:\",x0.shape,\"y0.shape:\",y0.shape,\"y0.type:\", type(y0) )\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "###现在暂时不训练多层模型，只训练9label模型\n",
    "if 0:\n",
    "    print(\"训练4层, 9 label 模型\")\n",
    "    x=x0\n",
    "    y=yl5\n",
    "    x=x.astype(np.float32)#GPU 加这个\n",
    "    y=y.astype(np.int64)#GPU 加这个\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    \n",
    "    num_labels = 9 \n",
    "    nSamples,nFeatures =  x.shape\n",
    "    enc = OneHotEncoder()\n",
    "    y= y.reshape(nSamples,-1)\n",
    "    \n",
    "    print(\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    enc.fit(y)\n",
    "    yOneHot=enc.transform(y).toarray()\n",
    "    saveName = \"../trainedModes/model-9label-4lays-512nodes-2slots-gpu1.h5\"\n",
    "    if 0:\n",
    "        kerasModel3_5label = kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels,saveName)     \n",
    "    yKeras_5label=getKerasResnetRVL(x,enc,saveName)\n",
    "    \n",
    "    print('keras\\n')\n",
    "    mat1num = confusion_matrix(y, yKeras_5label)\n",
    "    mat2acc = confusion_matrix(y, yKeras_5label,normalize='pred')\n",
    "    print('mat1num\\n',mat1num)\n",
    "    print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "    \n",
    "    \n",
    "########################################################################################################################    \n",
    "########################################################################################################################   \n",
    "########################################################################################################################    \n",
    "########################################################################################################################    \n",
    "print(\"##############################################################################################################\")\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"1.接编号为0的主程序,根据基于正确率的聚类程序或者经验将底层类别归结到上一层的类别\")\n",
    "print(\"2.程序编号为0+\")  \n",
    " \n",
    "'''\n",
    "if 0:# 训练统合样式的HMCN-F多级模型\n",
    "    print(\"训练5hieral, 4层, 9 label 模型\")\n",
    "    x=x0\n",
    "    y=yl5\n",
    "    x=x.astype(np.float32)#GPU 加这个\n",
    "    y=y.astype(np.int64)#GPU 加这个\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    yH1 = convertY2Hieral(y)\n",
    "    hierarchy = [2,4,6,8,9]#5层，对于当前数据集已经足够了\n",
    "    \n",
    "    yH1= np.array(yH1)\n",
    "    \n",
    "    nSamples,nFeatures =  x.shape\n",
    "    enc = OneHotEncoder()\n",
    "    yH1= yH1.reshape(nSamples,-1)\n",
    "    #print(yH1[:3])\n",
    "    print(\"yH1.shape:\",yH1 .shape,\"yH1.type:\", type(yH1) )\n",
    "    enc.fit(yH1)\n",
    "    #print(enc.categories_,enc.get_feature_names())\n",
    "    yOneHot=enc.transform(yH1).toarray()\n",
    "    #print(yOneHot[:3])\n",
    "    \n",
    "    num_labels = yOneHot.shape[1] \n",
    "    print(num_labels)\n",
    "    saveName = \"../trainedModes/model-5hier-9label-5lays-128nodes-2slots-gpu1.h5\"\n",
    "    kerasModel4_5hier_9label = kerasFitAndSaveHierSimple4LikeResnet(x,yOneHot,num_labels,saveName)   \n",
    "'''    \n",
    "    \n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if 1:# 训练多级模型\n",
    "    print(\"训练分离式多级模型\")\n",
    "    \n",
    "    #准备字典，用于保存训练后的数据\"\n",
    "    xFloors=  dict()\n",
    "    yFloors =  dict()\n",
    "    xTestFloors =dict()\n",
    "    yTestFloors = dict()\n",
    "    modSaveNameFloors =dict()\n",
    "    encLevels= dict()\n",
    "    yKerasFloors = dict()\n",
    "    x=x0\n",
    "    y=yl5\n",
    "    x=x.astype(np.float32)#GPU 加这个\n",
    "    y=y.astype(np.int64)#GPU 加这个\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    print(y)\n",
    "    \n",
    "    #hierarchy = [2,4,6,8,9]\n",
    "    #hierarchy = [2,3,4,5,6,7,8,9]\n",
    "    yH1,hierarchy = convertY2Hieral(y)\n",
    "    \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, yH1, test_size=0.5, random_state=0)\n",
    "   \n",
    "    nSamples,nFeatures =  x_train.shape\n",
    "    \n",
    "    \n",
    "    numEpochs =30 #1500/60/60*5 = 2houer\n",
    "    \n",
    "    \n",
    "    for i in range(len(hierarchy)):\n",
    "        print(\"\\n\\n levelIndex\",i,\"nSamples,nFeatures\",x_train.shape)\n",
    "        levelIndex = i\n",
    "        numLayers = 4\n",
    "        enc = OneHotEncoder()\n",
    "        nSamples,nFeatures =  x_train.shape\n",
    "       \n",
    "            \n",
    "        yCurLayer1 = [t1[i] for t1 in y_train]\n",
    "        \n",
    "        yCurLayer1 = np.array(yCurLayer1)\n",
    "        print(\"yCurLayer1.shape:\",yCurLayer1.shape)\n",
    "        \n",
    "        yCurLayer1= yCurLayer1.reshape(nSamples,-1)\n",
    "        enc.fit(yCurLayer1)\n",
    "        \n",
    "        yOneHot=enc.transform(yCurLayer1).toarray()\n",
    "        print(enc.categories_,enc.get_feature_names())\n",
    "        print(yOneHot[:1])\n",
    "        \n",
    "        \n",
    "        num_labels = hierarchy[i] \n",
    "        print(\"num_labels:\", num_labels)\n",
    "        saveName = \"../trainedModes/modelSep-9level%d-%dlayer-2slots-gpu1.h5\" %(i,numLayers)\n",
    "        #saveName = \"../trainedModes/modelSep-2level%d-%dlayer-2slots-gpu1.h5\" %(i,numLayers)#基于拥堵定义的2层结构\n",
    "        print(saveName)\n",
    "        sepHier1(x_train,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs)\n",
    "        \n",
    "        encLevels[str(i)] = enc\n",
    "        xFloors[str(i)] = x_train\n",
    "        yFloors[str(i)] = yCurLayer1\n",
    "        \n",
    "        \n",
    "        nSamplesTest,nFeaturesT =  x_test.shape\n",
    "        yCurLayerTest = [t1[i] for t1 in y_test]\n",
    "        yCurLayerTest = np.array(yCurLayerTest)\n",
    "        yCurLayerTest= yCurLayerTest.reshape(nSamplesTest,-1)\n",
    "        \n",
    "        xTestFloors[str(i)] = x_test\n",
    "        yTestFloors[str(i)] = yCurLayerTest\n",
    "        modSaveNameFloors[str(i)] = saveName\n",
    "        \n",
    "    #######保存为pickle文件,用于后期的SUMO和数据分析\n",
    "\n",
    "    fpk=open('samples1.pkf','wb+')  \n",
    "    pickle.dump([xFloors,yFloors,modSaveNameFloors,encLevels,xTestFloors, yTestFloors],fpk)  \n",
    "    fpk.close() \n",
    "\n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "#####用现有训练模型进行预测\n",
    "\n",
    "fpk=open('samples1.pkf','rb')   \n",
    "[xFloors,yFloors,modSaveNameFloors,encLevels,xTestFloors, yTestFloors]=pickle.load(fpk)  \n",
    "fpk.close()  \n",
    "\n",
    "\n",
    "yKerasFloors = dict()\n",
    "\n",
    "for i in range(len(hierarchy)):\n",
    "        levelIndex = i\n",
    "        #x = xFloors[str(i)]\n",
    "        #yCurLayer1 =  yFloors[str(i)]\n",
    "        \n",
    "        x = xTestFloors[str(i)]\n",
    "        yCurLayer1 =  yTestFloors[str(i)]\n",
    "        \n",
    "        saveName =  modSaveNameFloors[str(i)] \n",
    "        enc = encLevels[str(i)]\n",
    "        yOneHot=enc.transform(yCurLayer1).toarray()\n",
    "        yPredict=getKerasResnetRVL(x,enc,saveName)\n",
    "        print(\"分离式多层识别结果:第%d层\\n\" %i)\n",
    "        mat1num = confusion_matrix(yCurLayer1,yPredict)\n",
    "        print(mat1num)\n",
    "        mat2acc = confusion_matrix(yCurLayer1,yPredict,normalize='pred')  \n",
    "        print(np.around(mat2acc , decimals=3))\n",
    "        yKerasFloors[str(i)] =  yPredict\n",
    "        \n",
    "        df = pd.DataFrame(np.around(mat2acc , decimals=3))\n",
    "        fs = \"test_mat2acc%d.csv\" %i\n",
    "        df.to_csv(fs,index= False, header= False)\n",
    "        \n",
    "fpk=open('samples2.pkf','wb+')  \n",
    "pickle.dump([xFloors,yFloors,modSaveNameFloors,encLevels,yKerasFloors,xTestFloors,yTestFloors],fpk)  \n",
    "fpk.close() \n",
    "\n",
    " \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8674b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T14:36:52.438549Z",
     "start_time": "2023-02-13T14:36:16.048928Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "########################################################################################################################\n",
    "print(\"1.接编号为0的主程序,先找出低概率样本，\")\n",
    "print(\"2.对较低概率的样本进行蒙特卡洛模拟分析，原始对应程序为mainSimSumoFranceDatra\")\n",
    "print(\"3.最终进行分析，程序编号为1\")\n",
    "print(\"3.最终进行分析，程序编号为1\")\n",
    "########################################################################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "#import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "      \n",
    "\n",
    "\n",
    "    \n",
    "########################################################################################################################\n",
    "print(\"1.1 主程序开始\")\n",
    "########################################################################################################################\n",
    "\n",
    "########################################################################################################################\n",
    "#####用现有训练模型进行预测\n",
    "\n",
    "fpk=open('samples2.pkf','rb')   \n",
    "[xFloors,yFloors,modSaveNameFloors,encLevels,yKerasFloors,xTestFloors,yTestFloors]=pickle.load(fpk)  \n",
    "fpk.close()  \n",
    "\n",
    "hierarchy=[2,3,4,5,6,7,8,9]\n",
    "for i in [7]:\n",
    "#for i in range(len(hierarchy)):\n",
    "        levelIndex = i \n",
    "        x = xTestFloors[str(i)]\n",
    "        yCurLayer1 =  yTestFloors[str(i)]\n",
    "        #yP = yKerasFloors[str(i)]\n",
    "        \n",
    "        modeSaveName = \"../trainedModes/modelSep-9level7-4layer-2slots-gpu1.h5\"\n",
    "        model = keras.models.load_model(modeSaveName)\n",
    "        yPredictOut= model.predict([x], batch_size=2560)\n",
    "        yPredictOut = np.around(yPredictOut , decimals=3)\n",
    "        #print(yPredictOut)\n",
    "        ymax1=np.max(yPredictOut,axis=1)\n",
    "        ymax2=np.argmax(yPredictOut,axis=1)\n",
    "        \n",
    "        index = np.where(ymax1<0.5)[0]#提取最大值小于0.95的例子\n",
    "        \n",
    "        ylowpraPredictNN=yPredictOut[index]#对较低概率的样本\n",
    "        xlowpra=x[index]\n",
    "        ylowpraLabel = yCurLayer1[index]\n",
    "        ylowPredictLabel = ymax2[index].reshape(-1,1)\n",
    "        \n",
    "        \n",
    "        print(\"xlowpra.shape\",xlowpra.shape)\n",
    "        \n",
    "        \n",
    "        fpk=open('lowprobSamples.pkf','wb+')  \n",
    "        pickle.dump([xlowpra,ylowpraLabel,ylowPredictLabel,ylowpraPredictNN],fpk)  \n",
    "        fpk.close() \n",
    "        \n",
    "        df = pd.DataFrame(xlowpra)\n",
    "        fs = \"lowprobSamplesX.csv\"\n",
    "        df.to_csv(fs,index= False, header= False)\n",
    "       \n",
    "        ylowPredictLabel = ymax2[index].reshape(-1,1)\n",
    "        \n",
    "        df = pd.DataFrame(np.concatenate([ylowpraLabel,ylowPredictLabel,ylowpraPredictNN],axis=1))\n",
    "        fs = \"lowprobSamplesY.csv\"\n",
    "        df.to_csv(fs,index= False, header=['ylowpraLabel','ylowPredictLabel','0','1','2','3','4','5','6','7','8'])\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c074866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T09:51:42.931165Z",
     "start_time": "2023-01-28T09:51:42.771943Z"
    }
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "print(\"接程序1: 对较低概率的样本进行蒙特卡洛模拟分析,原始对应程序为mainSimSumoFranceDatra\")\n",
    "print(\"因为配置失误，采用将低概率的样本进行保存为文件，然后再root用户下命令行模式用SUMO模拟（不使用conda）\")\n",
    "print(\"输出为sumoSimData？？？.csv,里面有每个样本的sumo输出，kerasNN输出以及原始的输入输出\")\n",
    "print(\"程序编号为2\")\n",
    "print(\"程序编号为2\")\n",
    "########################################################################################################################\n",
    "!python3 sumoSimByFrance.py#运行runSumoSimFun.py 中test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "becda4ae-fe95-4636-9b74-0f775aec3d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "接程序2: 综合SUMO输出，对keras输出进行优化。程序输入为程序2的输出\n",
      "优化选择1.NN。 2 回归分析。3 概率分析。\n",
      "程序编号为3\n",
      "##############################################################################################################\n",
      "程序编号为3.1，主程序开始运行\n",
      "(10591, 16)\n",
      "Index(['sampleIndex', 'outputAvgSpeed', 'originOutput', 'sumoOutputSpeedTag',\n",
      "       'kerasPredictLabel', 'NN0', 'NN1', 'NN2', 'NN3', 'NN4', 'NN5', 'NN6',\n",
      "       'NN7', 'NN8', 'smv1', 'smv2'],\n",
      "      dtype='object')\n",
      "(10591, 11)\n",
      "(10591, 1)\n",
      "#############################\n",
      "数据预处理\n",
      "\n",
      "#############################\n",
      "原生keras\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.17      0.30      3029\n",
      "           1       0.30      0.21      0.24      2437\n",
      "           2       0.00      0.00      0.00       298\n",
      "           3       0.44      0.60      0.51      2752\n",
      "           4       0.37      0.81      0.51      2075\n",
      "           5       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.41     10591\n",
      "   macro avg       0.30      0.26      0.22     10591\n",
      "weighted avg       0.54      0.41      0.37     10591\n",
      "\n",
      "[[ 530  394   64 1092  944    4    1]\n",
      " [   0  502    0 1001  934    0    0]\n",
      " [   0  165    0    0  133    0    0]\n",
      " [   0  229    0 1648  875    0    0]\n",
      " [   0  395    0    0 1680    0    0]\n",
      " [   0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0]]\n",
      "[[1.    0.234 1.    0.292 0.207 1.    1.   ]\n",
      " [0.    0.298 0.    0.268 0.205 0.    0.   ]\n",
      " [0.    0.098 0.    0.    0.029 0.    0.   ]\n",
      " [0.    0.136 0.    0.441 0.192 0.    0.   ]\n",
      " [0.    0.234 0.    0.    0.368 0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.   ]]\n",
      "0.4116702860919649\n",
      "#############################\n",
      "决策树\n",
      "\n",
      "纯决策树的识别\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.51      0.66      3029\n",
      "           1       0.64      0.21      0.32      2437\n",
      "           2       0.00      0.00      0.00       298\n",
      "           3       0.54      0.60      0.57      2752\n",
      "           4       0.41      1.00      0.59      2075\n",
      "\n",
      "    accuracy                           0.55     10591\n",
      "   macro avg       0.50      0.47      0.43     10591\n",
      "weighted avg       0.63      0.55      0.52     10591\n",
      "\n",
      "[[1559  293    0  429  748]\n",
      " [ 139  516    0 1001  781]\n",
      " [   0    0    0    0  298]\n",
      " [   0    0    0 1648 1104]\n",
      " [   0    0    0    0 2075]]\n",
      "[[0.918 0.362 0.    0.139 0.149]\n",
      " [0.082 0.638 0.    0.325 0.156]\n",
      " [0.    0.    0.    0.    0.06 ]\n",
      " [0.    0.    0.    0.535 0.221]\n",
      " [0.    0.    0.    0.    0.415]]\n",
      "#############################\n",
      "逻辑回归\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.54      0.55      3029\n",
      "           1       0.38      0.20      0.26      2437\n",
      "           2       0.00      0.00      0.00       298\n",
      "           3       0.49      0.43      0.46      2752\n",
      "           4       0.43      0.82      0.56      2075\n",
      "\n",
      "    accuracy                           0.47     10591\n",
      "   macro avg       0.37      0.40      0.37     10591\n",
      "weighted avg       0.46      0.47      0.45     10591\n",
      "\n",
      "[[1634  386    3  442  564]\n",
      " [ 582  482    0  781  592]\n",
      " [  84    0    0    0  214]\n",
      " [ 458  219    0 1184  891]\n",
      " [ 202  176    0    0 1697]]\n",
      "[[0.552 0.306 1.    0.184 0.142]\n",
      " [0.197 0.382 0.    0.324 0.15 ]\n",
      " [0.028 0.    0.    0.    0.054]\n",
      " [0.155 0.173 0.    0.492 0.225]\n",
      " [0.068 0.139 0.    0.    0.429]]\n",
      "0.4718156925691625\n",
      "#############################\n",
      "贝叶斯高斯回归\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.04      0.07      3029\n",
      "           1       0.23      0.11      0.15      2437\n",
      "           2       0.20      0.55      0.30       298\n",
      "           3       0.39      0.60      0.48      2752\n",
      "           4       0.39      0.82      0.53      2075\n",
      "\n",
      "    accuracy                           0.37     10591\n",
      "   macro avg       0.44      0.42      0.30     10591\n",
      "weighted avg       0.52      0.37      0.29     10591\n",
      "\n",
      "[[ 108  653  258 1304  706]\n",
      " [   0  260   79 1226  872]\n",
      " [   0   54  163    0   81]\n",
      " [   0   56   46 1648 1002]\n",
      " [   0  122  259    0 1694]]\n",
      "[[1.    0.57  0.32  0.312 0.162]\n",
      " [0.    0.227 0.098 0.293 0.2  ]\n",
      " [0.    0.047 0.202 0.    0.019]\n",
      " [0.    0.049 0.057 0.394 0.23 ]\n",
      " [0.    0.107 0.322 0.    0.389]]\n",
      "0.36568784817297706\n",
      "#############################\n",
      "kerasNN\n",
      "\n",
      "(10591, 11)\n",
      "(10591, 5)\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9924 - accuracy: 0.5319\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9906 - accuracy: 0.5367\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9895 - accuracy: 0.5361\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9877 - accuracy: 0.5401\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9826 - accuracy: 0.5446\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9836 - accuracy: 0.5443\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "print(\"接程序2: 综合SUMO输出，对keras输出进行优化。程序输入为程序2的输出\")\n",
    "print(\"优化选择1.NN。 2 回归分析。3 概率分析。\")\n",
    "print(\"程序编号为3\")\n",
    "########################################################################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import pickle \n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "############################################################################\n",
    "############################################################################\n",
    "####建立NN模型，与sepHier2一样\n",
    "def sepHier2(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 10,srelu_size = 256,dropout_rate = 0.05):\n",
    "    \n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    featuresInput = layers.Input(shape=(features_size,))\n",
    "    features = layers.BatchNormalization()(featuresInput)\n",
    "    #features=featuresInput\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[featuresInput], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        #build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=20000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "\n",
    "def dtFit(x,y):\n",
    "    str1=\"dtFitAndSave,用于决策树拟合和识别\"\n",
    "    \n",
    "    dt = tree.DecisionTreeClassifier(max_depth=7,min_samples_leaf=100)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    #data=tree.export_graphviz(dt, out_file=None,class_names=None,filled=True) \n",
    "    #graph = graphviz.Source(data)\n",
    "    #graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(\"纯决策树的识别\\n\",tmp1)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    #text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    return dt,yPredict\n",
    "########################################################################################################################   \n",
    "########################################################################################################################    \n",
    "print(\"##############################################################################################################\")\n",
    "\n",
    "print(\"程序编号为3.1，主程序开始运行\")\n",
    "\n",
    "   \n",
    "\n",
    "df = pd.read_csv('sumoSimData.csv', sep=',')\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "numSamples,numFeatures = df.shape\n",
    "\n",
    "##['sampleIndex','outputAvgSpeed','originOutput','sumoOutputSpeedTag','kerasPredictLabel','smv1','smv2',\\\n",
    " ##                                              'NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8'])\n",
    "sumoOutput='sumoOutputSpeedTag'\n",
    "yKerasOutput='kerasPredictLabel'\n",
    "originOutput ='originOutput'\n",
    "sumoOutList = ['smv1','smv2']\n",
    "outputListNN = ['NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8']\n",
    "\n",
    "df1 = df[sumoOutput]\n",
    "x1 = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "df1 = df[yKerasOutput]\n",
    "x2 = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "df1 = df[outputListNN]\n",
    "x3 = df1.iloc[0:numSamples].to_numpy()\n",
    "\n",
    "\n",
    "df1 = df[originOutput]\n",
    "y = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y)\n",
    "yOneHot=enc.transform(y).toarray()\n",
    "\n",
    "x = np.concatenate([x1,x2,x3],axis=1)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "#print(y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"#############################\\n数据预处理\\n\")\n",
    "#数据预处理\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, yOneHot, test_size=0.5, random_state=0)\n",
    "   \n",
    "\n",
    "rN,cN= np.where(np.isnan(x))\n",
    "#print(rN,cN)\n",
    "#print(rN.shape)\n",
    "\n",
    "for i in range(rN.shape[0]):\n",
    "    x[rN[i],cN[i]] = 0\n",
    " \n",
    "\n",
    "print(\"#############################\\n原生keras\\n\")\n",
    "yPredict = x2\n",
    "tmp1 = classification_report(y,yPredict)\n",
    "mat1num = confusion_matrix(y,yPredict)\n",
    "mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "\n",
    "score = accuracy_score(yPredict, y)\n",
    "print(score) \n",
    "\n",
    "print(\"#############################\\n决策树\\n\")\n",
    "dtFit(x,y)\n",
    "   \n",
    "print(\"#############################\\n逻辑回归\\n\")    \n",
    "model = LogisticRegression()\n",
    "model.fit(x,y)\n",
    "yPredict = model.predict(x)\n",
    "\n",
    "tmp1 = classification_report(y,yPredict)\n",
    "mat1num = confusion_matrix(y,yPredict)\n",
    "mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "score = accuracy_score(yPredict, y)\n",
    "print(score) \n",
    "print(\"#############################\\n贝叶斯高斯回归\\n\")\n",
    "nb_cls = naive_bayes.GaussianNB().fit(x,y)\n",
    "yPredict = nb_cls.predict(x) \n",
    "\n",
    "tmp1 = classification_report(y,yPredict)\n",
    "mat1num = confusion_matrix(y,yPredict)\n",
    "mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "score = accuracy_score(yPredict, y)\n",
    "print(score) \n",
    "print(\"#############################\\nkerasNN\\n\")\n",
    "print(x.shape)\n",
    "print(yOneHot.shape)\n",
    "num_labels = yOneHot.shape[1]\n",
    "numLayers = 4\n",
    "numEpochs = 5\n",
    "saveName =\"../trainedModes/stage2_1.h5\";\n",
    "levelIndex = 7\n",
    "\n",
    "sepHier2(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fpk=open('samples2-stage2.pkf','wb')  \n",
    "pickle.dump([df,x,y,yOneHot,x_train, x_test, y_train, y_test,enc,saveName],fpk)  \n",
    "fpk.close() \n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################   \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a367eac-c740-4be5-8f8a-5bdba5969e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################################################################\n",
      "程序编号为3.2，手动权值分配\n",
      "(10591, 16)\n",
      "Index(['sampleIndex', 'outputAvgSpeed', 'originOutput', 'sumoOutputSpeedTag',\n",
      "       'kerasPredictLabel', 'NN0', 'NN1', 'NN2', 'NN3', 'NN4', 'NN5', 'NN6',\n",
      "       'NN7', 'NN8', 'smv1', 'smv2'],\n",
      "      dtype='object')\n",
      "[[0.    4.    0.14  ... 0.    0.    0.   ]\n",
      " [0.    4.    0.178 ... 0.    0.    0.   ]\n",
      " [0.    4.    0.185 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    1.    0.184 ... 0.    0.    0.   ]\n",
      " [0.    1.    0.184 ... 0.    0.    0.   ]\n",
      " [0.    3.    0.342 ... 0.    0.    0.   ]]\n",
      "(10591, 1)\n",
      "(10591, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.57      0.53      3029\n",
      "           1       0.26      0.12      0.16      2437\n",
      "           2       0.00      0.00      0.00       298\n",
      "           3       0.51      0.35      0.42      2752\n",
      "           4       0.42      0.81      0.55      2075\n",
      "\n",
      "    accuracy                           0.44     10591\n",
      "   macro avg       0.34      0.37      0.33     10591\n",
      "weighted avg       0.42      0.44      0.41     10591\n",
      "\n",
      "[[1737  257   49  291  695]\n",
      " [ 838  290    0  648  661]\n",
      " [  54  111    0    0  133]\n",
      " [ 734  173    0  970  875]\n",
      " [ 122  273    0    0 1680]]\n",
      "[[0.498 0.233 1.    0.152 0.172]\n",
      " [0.24  0.263 0.    0.339 0.163]\n",
      " [0.015 0.101 0.    0.    0.033]\n",
      " [0.211 0.157 0.    0.508 0.216]\n",
      " [0.035 0.247 0.    0.    0.415]]\n",
      "0.4416013596449816\n",
      "#############################\n",
      "原生keras\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.17      0.30      3029\n",
      "           1       0.30      0.21      0.24      2437\n",
      "           2       0.00      0.00      0.00       298\n",
      "           3       0.44      0.60      0.51      2752\n",
      "           4       0.37      0.81      0.51      2075\n",
      "           5       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.41     10591\n",
      "   macro avg       0.30      0.26      0.22     10591\n",
      "weighted avg       0.54      0.41      0.37     10591\n",
      "\n",
      "[[ 530  394   64 1092  944    4    1]\n",
      " [   0  502    0 1001  934    0    0]\n",
      " [   0  165    0    0  133    0    0]\n",
      " [   0  229    0 1648  875    0    0]\n",
      " [   0  395    0    0 1680    0    0]\n",
      " [   0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0]]\n",
      "[[1.    0.234 1.    0.292 0.207 1.    1.   ]\n",
      " [0.    0.298 0.    0.268 0.205 0.    0.   ]\n",
      " [0.    0.098 0.    0.    0.029 0.    0.   ]\n",
      " [0.    0.136 0.    0.441 0.192 0.    0.   ]\n",
      " [0.    0.234 0.    0.    0.368 0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.   ]]\n",
      "0.4116702860919649\n",
      "############################################https://www.jb51.net/article/269941.htm\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################   \n",
    "########################################################################################################################   \n",
    "import scipy.interpolate as si\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"##############################################################################################################\")\n",
    "\n",
    "print(\"程序编号为3.2，手动权值分配\")\n",
    "\n",
    "df = pd.read_csv('sumoSimData.csv', sep=',')\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "numSamples,numFeatures = df.shape\n",
    "\n",
    "##['sampleIndex','outputAvgSpeed','originOutput','sumoOutputSpeedTag','kerasPredictLabel','smv1','smv2',\\\n",
    " ##                                              'NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8'])\n",
    "sumoOutput='sumoOutputSpeedTag'\n",
    "yKerasOutput='kerasPredictLabel'\n",
    "originOutput ='originOutput'\n",
    "sumoOutList = ['smv1','smv2']\n",
    "outputListNN = ['NN0','NN1','NN2','NN3','NN4','NN5','NN6','NN7','NN8']\n",
    "\n",
    "\n",
    "\n",
    "df1 = df[originOutput]\n",
    "yo = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "df1 = df[sumoOutput]\n",
    "x1 = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "df1 = df[yKerasOutput]\n",
    "x2 = df1.iloc[0:numSamples].to_numpy().reshape(-1,1)\n",
    "\n",
    "df1 = df[outputListNN]\n",
    "x3 = df1.iloc[0:numSamples].to_numpy()\n",
    "\n",
    "x = np.concatenate([x1,x2,x3],axis=1)\n",
    "print(x)\n",
    "\n",
    "manualOut = np.zeros((x3.shape[0],1))\n",
    "\n",
    "\n",
    "#for i in range(10):\n",
    "for i in range(x3.shape[0]):\n",
    "    #print(i)\n",
    "    nn1 = x3[i]\n",
    "    sumoOut = x1[i][0]\n",
    "    kerasOut = x2[i][0]\n",
    "    originOut = yo[i][0]\n",
    "\n",
    "    xIntp=[0,sumoOut,min(8,sumoOut+1),min(8,sumoOut+2),min(8,sumoOut+3),9]\n",
    "    yIntp =[0,1,0.5,0.4,0.3,0]\n",
    "    \n",
    "    xIntp=[-9,sumoOut-1,sumoOut,sumoOut+1,9]\n",
    "    yIntp =[0.5,0.7,1,0.7,0.5]\n",
    "    f = si.interp1d(xIntp,  yIntp,kind=1)\n",
    "    xi = [0,1,2,3,4,5,6,7,8]\n",
    "    p= f(xi)\n",
    "    yTmp = np.multiply(p,nn1)\n",
    "    finalIndex = np.argmax(yTmp)\n",
    "    manualOut[i] = finalIndex\n",
    "    \n",
    "    #print('sumoOut','kerasOut','originOut','finalIndex')\n",
    "    #print(sumoOut,kerasOut,originOut,finalIndex)\n",
    "    #print(\"nn1:\",nn1)\n",
    "    #print(\"p  :\",np.round(p,decimals=3))\n",
    "    #print(\"y3 :\",np.round(y3,decimals=3))\n",
    "\n",
    "  \n",
    "\n",
    "print(yo.shape)\n",
    "print(manualOut.shape)\n",
    "tmp1 = classification_report(yo,manualOut)\n",
    "mat1num = confusion_matrix(yo,manualOut)\n",
    "mat2acc = confusion_matrix(yo,manualOut,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "score = accuracy_score(manualOut, yo)\n",
    "print(score) \n",
    "\n",
    "print(\"#############################\\n原生keras\\n\")\n",
    "yPredict = x2\n",
    "tmp1 = classification_report(yo,yPredict)\n",
    "mat1num = confusion_matrix(yo,yPredict)\n",
    "mat2acc = confusion_matrix(yo,yPredict,normalize='pred')\n",
    "print(tmp1)\n",
    "print(mat1num)\n",
    "print(np.around(mat2acc , decimals=3))\n",
    "\n",
    "score = accuracy_score(yPredict, y)\n",
    "print(score) \n",
    "\n",
    "\n",
    "print(\"############################################https://www.jb51.net/article/269941.htm\")\n",
    "#https://blog.csdn.net/ljyljyok/article/details/100552618\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import dual_annealing\n",
    "\n",
    "def objective(x,*args):\n",
    "    x1,x2,x3,yo = args\n",
    "    manualOut = np.zeros((x3.shape[0],1))\n",
    "    for i in range(x3.shape[0]):\n",
    "        #print(i)\n",
    "        nn1 = x3[i]\n",
    "        sumoOut = x1[i][0]\n",
    "        kerasOut = x2[i][0]\n",
    "        originOut = yo[i][0]\n",
    "\n",
    "        \n",
    "\n",
    "        xIntp=[-9,sumoOut-x[0],sumoOut,sumoOut+x[1],9]\n",
    "        yIntp =[0.5,x[2],1,x[2],0.5]\n",
    "        f = si.interp1d(xIntp,  yIntp,kind=1)\n",
    "        xi = [0,1,2,3,4,5,6,7,8]\n",
    "        p= f(xi)\n",
    "        yTmp = np.multiply(p,nn1)\n",
    "        finalIndex = np.argmax(yTmp)\n",
    "        manualOut[i] = finalIndex\n",
    "\n",
    "    score = accuracy_score(manualOut, yo)\n",
    "    \n",
    "    return -score\n",
    "\n",
    "\n",
    "\n",
    "#args1 = (x1,x2,x3,yo)\n",
    "#x0 = [1,1,0.3]\n",
    "bounds1 = ((0, 4), (0, 4),(0.01, 0.9))\n",
    "\n",
    "#constraints = {'type': 'ineq', 'fun': cons}\n",
    "\n",
    "#res = minimize(objective, x0, args=args1,method='SLSQP',bounds=bounds1)\n",
    "#print(res.fun)\n",
    "#print(res.success)\n",
    "#print(res.x)\n",
    "#https://vimsky.com/zh-tw/examples/usage/python-scipy.optimize.dual_annealing.html\n",
    "args1 = (x1,x2,x3,yo)\n",
    "x0 = [1,1,0.3]\n",
    "bounds1 = [[0, 4], [0, 4],[0.01, 0.9]]\n",
    "res = dual_annealing(objective,bounds1,x0=x0, args=args1)\n",
    "print(res.fun)\n",
    "print(res.success)\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add02d9c-af03-4a00-8629-42b8a751033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "print(\"辅助程序 对模拟后的数据进行分析，计算正确率\")\n",
    "########################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"对于低概率样本的识别结果，采用keras和MCS的识别结果对比\")\n",
    "file1 = \"./data-Copy1.csv\"\n",
    "xyDataTmp = pd.read_csv(file1,index_col=0)\n",
    "\n",
    "print(xyDataTmp.head(3))\n",
    "print(xyDataTmp.info())\n",
    "\n",
    "file1 = \"。./trainData/france_0_allSamples1.csv\"\n",
    "xyOrigin = pd.read_csv(file1,index_col=0)\n",
    "\n",
    "originlabel =  xyDataTmp.iloc[:,1].to_numpy()\n",
    "keraslabel =   xyDataTmp.iloc[:,2].to_numpy()      \n",
    "mcslabel =     xyDataTmp.iloc[:,3].to_numpy()\n",
    "\n",
    "#\n",
    "    \n",
    "print('\\norigin_mcs')\n",
    "mat1num = confusion_matrix(originlabel, mcslabel)\n",
    "mat2acc = confusion_matrix(originlabel, mcslabel,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "      \n",
    "print('\\nmcs_keras')\n",
    "mat1num = confusion_matrix(mcslabel, keraslabel)\n",
    "mat2acc = confusion_matrix(mcslabel, keraslabel,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "\n",
    "print('\\norgin_keras')\n",
    "mat1num = confusion_matrix(originlabel, keraslabel)\n",
    "mat2acc = confusion_matrix(originlabel ,keraslabel,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))      \n",
    "\n",
    "##用于分析实际标记类别大于预测标记类别\n",
    "def analyzing1(tmp, xyDataTmp,xyOrigin): \n",
    "    dfTmp1 = xyDataTmp[tmp]\n",
    "    #print(dfTmp1.head(5))\n",
    "    \n",
    "    \n",
    "    \n",
    "    df2 =  xyOrigin.iloc[dfTmp1.originIndex,:]   \n",
    "    plt.show()\n",
    "    df2[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    #print(df2.info())\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    \n",
    "    df2.to_csv(\"tmpForAnalyzing.csv\")\n",
    "    \n",
    "    tmp1 = df2['redLightTime'] - df2['arriveTime2'] >1.5 #红灯时间大于到达时间\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 = df2[df2['redLightTime'] - df2['arriveTime2'] >1.5] #红灯时间大于到达时间 ,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing3.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    \n",
    "   \n",
    "    tmp1 = df2['speed'] < 5/3.6 #本身速度就小于5/3.6\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df2['speed'] > 5/3.6 #本身速度就小于5/3.6,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    df3.to_csv(\"tmpForAnalyzing4.csv\")\n",
    "    \n",
    "    tmp1 = df2['redLightTime'] - df2['arriveTime2'] >1.5  #红灯时间大于到达时间\n",
    "    tmp1 = tmp1 | (df2['speed'] < 5/3.6) #本身速度就小于5/3.6\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"红灯时间大于到达时间  or 本身速度就小于5/3.6,df3 shape:\",df3.shape,\"占输入样本比例为:\",df3.shape[0]/df2.shape[0])\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    df3.to_csv(\"tmpForAnalyzing5.csv\")\n",
    "    \n",
    "    \n",
    "    tmp1 = df2['arriveTime2'] - df2['redLightTime'] >0 #到达时间大于红灯时间\n",
    "    tmp1 = tmp1 & (df2['speed'] > 5/3.6) #本身速度就大于于5/3.6\n",
    "    tmp1 = tmp1 & (df2['vehPos_2'] > 0) #\n",
    "    tmp1 = tmp1 & (df2['vehSpeed_2'] < 5/3.6) #\n",
    "    tmp1 = tmp1 & (df2['vehPos_3'] >0) #\n",
    "    tmp1 = tmp1 & (df2['vehSpeed_3'] <5/3.6) #\n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"到达时间大于红灯时间  and 本身速度就大于5/3.6,df3 shape:\",df3.shape,\"占输入样本比例为:\",df3.shape[0]/df2.shape[0])\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    df3.to_csv(\"tmpForAnalyzing6.csv\")\n",
    "\n",
    "def extractStillVeh2(df):\n",
    "    df=df.rename(columns={'ArrTimeDivRedTime':'numStillVeh'})\n",
    "    df=df.rename(columns={'lanAvgSpeed':'predictStats'})\n",
    "    df['numStillVeh'] = 0\n",
    "    df['predictStats'] = \"unknown\"\n",
    "    for i in range(df.shape[0]):\n",
    "        numStillVeh = 0\n",
    "        tmp = df.iloc[i]\n",
    "        redTime = tmp.iloc[0]\n",
    "        vPosObj = tmp.iloc[1]\n",
    "        predictStats = -1\n",
    "\n",
    "        for j in range(20):\n",
    "           \n",
    "            vehPos = tmp.iloc[2*j+8]\n",
    "            vehVeh = tmp.iloc[2*j+1+8]\n",
    "            \n",
    "            if vehPos >0 and vehVeh<5/3.6:#经验数据,参数\n",
    "                numStillVeh = numStillVeh + 1\n",
    "            elif vehPos >0 and  vPosObj > vehPos:\n",
    "                timeTmp1 =(vehPos-j*6.5)/(vehVeh+0.001)#经验公式，到固定位置后，启动需要的时间\n",
    "                if timeTmp1  < redTime +numStillVeh*1.5:\n",
    "                    numStillVeh = numStillVeh + 1\n",
    "\n",
    "            if vehPos >0 and vPosObj == vehPos and vehVeh<5/3.6:\n",
    "                predictStats = \"stop\"#目标车要听停止\n",
    "               \n",
    "\n",
    "            if vehPos >0 and vPosObj == vehPos and vehVeh>5/3.6 :    \n",
    "                timeTmp1 =(vehPos-j*6.5)/(vehVeh+0.001)#经验公式，到固定位置后需要的时间\n",
    "                if timeTmp1  <= redTime +numStillVeh*1.5+1.5:#小于虚拟红灯结束时间\n",
    "                     predictStats = \"stop\" #目标车要听停止\n",
    "                else:        \n",
    "                     predictStats = \"no stop\"  #目标车要不要停止\n",
    "                \n",
    "     \n",
    "        df['numStillVeh'][i] = numStillVeh\n",
    "        df['predictStats'][i] = predictStats\n",
    "\n",
    "    return df\n",
    "##用于分析实际标记类别小于预测标记类别， xyDataTmp[\"predicted Labels By MCS\"] - xyDataTmp[\"origin speedFlag\"]>0\n",
    "def analyzing2(tmp, xyDataTmp,xyOrigin): \n",
    "    dfTmp1 = xyDataTmp[tmp]\n",
    "    #print(dfTmp1.head(5))\n",
    "    \n",
    "    \n",
    "    #1\n",
    "    df2 =  xyOrigin.iloc[dfTmp1.originIndex,:]   \n",
    "    plt.show()\n",
    "    df2[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    #print(df2.info())\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df2 = extractStillVeh2( df2)\n",
    "    df2.to_csv(\"tmpForAnalyzing.csv\")\n",
    "    \n",
    "    \n",
    "    ######################################\n",
    "    tmp1 = (df2['speedFlag'] == 0)  & (df2['predictStats'] == \"stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing1.csv\")\n",
    "    \n",
    "    tmp1 = (df2['speedFlag'] > 0)  & (df2['predictStats'] == \"no stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing2.csv\")\n",
    "    \n",
    "    \n",
    "    tmp1 = (df2['speedFlag'] == 0)  & (df2['predictStats'] == \"no stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing3.csv\")\n",
    "    \n",
    "    tmp1 = (df2['speedFlag'] > 0)  & (df2['predictStats'] == \"stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing4.csv\")\n",
    "    return\n",
    "    \n",
    "    '''\n",
    "    #2\n",
    "    tmp1 = df2['redLightTime'] - df2['arriveTime2'] >0 #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 = df2[df2['redLightTime'] - df2['arriveTime2'] >0] #红灯时间大于到达时间 ,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing2.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    \n",
    "    #3\n",
    "    tmp1 =df2['arriveTime2'] - df2['redLightTime'] >3 #红灯时间小于到达时间3，\n",
    "    tmp1 = tmp1 & (df2['speedFlag'] == 0) \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"#红灯时间小于于到达时间 ,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing3.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))  \n",
    "    \n",
    "   \n",
    "    #4据静止汽车数目，分析在df2['speedFlag'] > 0情况下，虚拟红灯时间小于于到达时间情况，也就是目标车可能不需要停下来\n",
    "    \n",
    "    tmp1 =(df2['speedFlag'] == 0) \n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 < df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 = tmp1 & tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing4.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))  \n",
    "    \n",
    "    #5 据静止汽车数目，分析在df2['speedFlag'] > 0情况下，虚拟红灯时间大于到达时间情况，也就是目标车可能需要停下来\n",
    "    tmp1 =(df2['speedFlag'] > 0) \n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 >= df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 = tmp1 & tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing5.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))  \n",
    "    \n",
    "    \n",
    "    #6 据静止汽车数目，分析虚拟红灯时间大于到达时间情况，也就是目标车可能需要停下来\n",
    "  \n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 >= df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 =  tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing6.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    \n",
    "    \n",
    "    #7 根据静止汽车数目，分析虚拟红灯时间小于到达时间情况，也就是目标车可能不需要停下来\n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']+1.5\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 < df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 =  tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing7.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4)) \n",
    "   '''\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "tmp = (xyDataTmp[\"origin speedFlag\"] - xyDataTmp[\"predicted Labels By MCS\"] >0) \n",
    "\n",
    "#analyzing1(tmp, xyDataTmp,xyOrigin)\n",
    "\n",
    "tmp = xyDataTmp[\"origin speedFlag\"] - xyDataTmp[\"predicted Labels By MCS\"]  >=3 \n",
    "#analyzing(tmp, xyDataTmp,xyOrigin)\n",
    "\n",
    "tmp = xyDataTmp[\"predicted Labels By MCS\"] - xyDataTmp[\"origin speedFlag\"]>0\n",
    "analyzing2(tmp, xyDataTmp,xyOrigin)\n",
    "      \n",
    "tmp = xyDataTmp[\"predicted Labels By MCS\"] - xyDataTmp[\"origin speedFlag\"]>=3\n",
    "#analyzing2(tmp, xyDataTmp,xyOrigin)\n",
    "\n",
    "#手动修改\n",
    "\n",
    "    \n",
    "      \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46c252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T10:10:55.559866Z",
     "start_time": "2023-01-27T10:10:53.398090Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c877a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T09:04:50.440620Z",
     "start_time": "2023-01-28T09:04:50.239643Z"
    }
   },
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b3f88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-29T03:05:13.857103Z",
     "start_time": "2023-01-29T03:05:13.853125Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "timestr= datetime.now()\n",
    "print(timestr)\n",
    "\n",
    "!conda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281fc84-e6c1-4671-9323-70c1fdbb7210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf tmp*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75a25a-12fd-4f04-85e2-c2a78f766298",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[11, 3, 4 ,5],[6, 7, 8, 9]])\n",
    "print(np.where(arr < 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0697b-8759-40f4-8556-8ec1bfa76aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.interpolate as si\n",
    "\n",
    "%matplotlib inline\n",
    "from numpy import polyfit, poly1d\n",
    "x=[0,3,8]\n",
    "y =[0,1,0]\n",
    "coeff = polyfit(x, y, 2)\n",
    "print(coeff)\n",
    " \n",
    "p = plt.plot(x, y, 'rx')\n",
    "\n",
    "x=[0,3,8]\n",
    "y =[0.5,1,0.5]\n",
    "\n",
    "x1 = np.linspace(0, 8, 100)\n",
    "y1 = np.polyval(coeff, x1)\n",
    "p = plt.plot(x1,y1, 'k-')\n",
    "\n",
    "\n",
    "f = si.interp1d(x, y,kind=1)\n",
    "y2= f(x1)  #调用经由interp1d返回的函数\n",
    "p = plt.plot(x1,y2, 'b-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8290fe-e417-434f-b337-6a5cb390c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5])\n",
    "b = np.array([5,5,6,7,8])\n",
    "c = a*b\n",
    "c\n",
    "\n",
    "a = [2,2,3,4,1]\n",
    "b = [5,5,6,7,8]\n",
    "d = np.multiply(a,b)\n",
    "d\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor23py36gpu",
   "language": "python",
   "name": "tensor23py36gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b27f224da048d073ae2b306b979c73d2559eaa860bf21b792b51024f42769a7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
