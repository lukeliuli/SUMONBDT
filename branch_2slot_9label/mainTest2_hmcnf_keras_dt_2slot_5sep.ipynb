{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f6c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T03:55:17.555121Z",
     "start_time": "2023-01-26T03:55:17.435109Z"
    }
   },
   "outputs": [],
   "source": [
    "#一些常用的命令\n",
    "!nvidia-smi\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#一些常用的命令\n",
    "!git status\n",
    "!git add .\n",
    "!git commit -m \"correct and optimize some code\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0809da8b-07fe-447a-b1fd-d7520e79d721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全局共享函数，运行主程序前需要运行\n"
     ]
    }
   ],
   "source": [
    "###定义的公用函数\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import pickle  \n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "#######开始为功能函数\n",
    "print(\"全局共享函数，运行主程序前需要运行\")\n",
    "def dtFitAndSave(x,y,saveName):\n",
    "    str1=\"dtFitAndSave,用于决策树拟合和识别\"\n",
    "    \n",
    "    dt = tree.DecisionTreeClassifier(max_depth=7,min_samples_leaf=100)\n",
    "    dt = dt.fit(x, y)\n",
    "    tree.plot_tree(dt)\n",
    "    #data=tree.export_graphviz(dt, out_file=None,class_names=None,filled=True) \n",
    "    #graph = graphviz.Source(data)\n",
    "    #graph.render(saveName)\n",
    "    \n",
    "    yPredict = dt.predict(x)\n",
    "    tmp1 = classification_report(y,yPredict)\n",
    "    print(\"纯决策树的识别\\n\",tmp1)\n",
    "    mat1num = confusion_matrix(y,yPredict)\n",
    "    mat2acc = confusion_matrix(y,yPredict,normalize='pred')\n",
    "    print(mat1num)\n",
    "    print(np.around(mat2acc , decimals=3))\n",
    "    #text_representation = tree.export_text(dt)\n",
    "    #print(text_representation)\n",
    "    #yPredict = dt.predict_proba(x)\n",
    "    #index = np.where((yPredict[:,1]<0.98)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.90)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.80)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    #index = np.where((yPredict[:,1]<0.70)&(yPredict[:,1]>0.5))\n",
    "    #print(index[0].shape,index)\n",
    "    return dt,yPredict\n",
    "\n",
    "########################################################################################################################\n",
    "###简单模型3，resnet_like\n",
    "def local_model(num_labels, dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(num_labels, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def global_model(dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='sigmoid',name=\"global\"))\n",
    "    return model\n",
    "\n",
    "def softmax_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='softmax',name=\"global\"))\n",
    "    return model\n",
    "'''\n",
    "############################################################################\n",
    "############################################################################\n",
    "#单层模型\n",
    "def kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels,saveName):\n",
    "    str1=\"kerasFitAndSaveSimple3LikeResnet,用于resnet_like的神经网络拟合和识别\"\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 512\n",
    "    dropout_rate = 0.05\n",
    "    hierarchy = [1,1,1,1]#四层，对于当前数据集已经足够了\n",
    "    global_models = []\n",
    "   \n",
    "    \n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(len(hierarchy)):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "\n",
    "   \n",
    "    \n",
    "    p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "    #model = tf.keras.Model(inputs=[features], outputs=[build_model])\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(y)  \n",
    "    #yOnehot=enc.transform(y).toarray()\n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "       build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file='KerasSimple3_likeResnet_4lay512nodes.jpg', show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=1500, batch_size=40000*1)#GPU用这个\n",
    "    #saveName = \"KerasSimple3_likeResnet.h5\"\n",
    "    build_model.save(saveName)\n",
    "    plot_model(build_model, to_file='KerasSimple3_likeResnet_4lay512nodes.jpg', show_shapes=True)\n",
    "    return build_model\n",
    "\n",
    "############################################################################\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，\n",
    "def kerasFitAndSaveHierSimple4LikeResnet(x,yOneHot,num_labels,saveName):\n",
    "    str1=\"kerasFitAndSaveHierSimple4LikeResnet,用于resnet_like的 神经网络拟合和识别\"\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    beta = 0.5\n",
    "    hierarchy = [2,4,6,8,9]#5层，对于当前数据集已经足够了\n",
    "    global_models = []\n",
    "    local_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(len(hierarchy)):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "    \n",
    "    for i in range(len(hierarchy)):\n",
    "        local_models.append(local_model(hierarchy[i], dropout_rate, relu_size)(global_models[i]))\n",
    "        \n",
    "        \n",
    "    #显示只有局部局模型的情况(部分全局)\n",
    "    p_loc = layers.concatenate(local_models)\n",
    "    #modelTmp2 = tf.keras.Model(inputs=[features], outputs=[p_loc])\n",
    "    #modelTmp2.summary()#\n",
    "    #plot_model(modelTmp2, to_file='Flatten2.png', show_shapes=True)\n",
    "    p_glob1 = layers.Lambda(lambda x: x*beta,name=\"global\")(p_glob)\n",
    "    p_loc1 = layers.Lambda(lambda x: x*(1-beta),name=\"local\")(p_loc)\n",
    "\n",
    "    labels = layers.add([p_glob1, p_loc1])\n",
    "\n",
    "\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[labels])\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file='hmcnf1.jpg', show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=3500, batch_size=40000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "'''\n",
    "############################################################################\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，所以采用分离式\n",
    "###每一层的识别模型都是4层模型\n",
    "def sepHier1(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 10,srelu_size = 256,dropout_rate = 0.05):\n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    if 0:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=40000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "def getKerasResnetRVL(x,enc,saveName):\n",
    "    model_name = saveName \n",
    "    model = keras.models.load_model(model_name)\n",
    "    y= model.predict([x], batch_size=2560)\n",
    "    nSamples = y.shape[0]\n",
    "    ###需要将预测出的值，转换01整数,并转为数字式\n",
    "    for i in range(y.shape[0]):\n",
    "        tmp = y[i]\n",
    "        index=  np.argmax(tmp)\n",
    "        y[i] = [0]*y.shape[1]\n",
    "        y[i,index]=1\n",
    "   \n",
    "\n",
    "    ###  \n",
    "    y= enc.inverse_transform(y)\n",
    "    y= y.reshape(-1,nSamples)[0]\n",
    "    \n",
    "    \n",
    "    return y\n",
    "\n",
    "def string2int(inputString):\n",
    "     #print(inputString)\n",
    "     tmp = 0\n",
    "     try:\n",
    "         strTmp=[str(ord(x)) for x in inputString]\n",
    "         tmp=tmp.join(strTmp)\n",
    "         tmp = float(tmp)/(len(inputString)*128)\n",
    "     except:\n",
    "         #print(inputString)\n",
    "         strTmp = inputString\n",
    "         tmp= \"0\"\n",
    "         tmp = 0\n",
    "     return tmp\n",
    " ## 根据经验以及最佳正确率的合并方法\n",
    " #第一次合并为0,1的合并\n",
    "\n",
    "########################################################################################################################\n",
    "##手工确定层次结构，以前测试时候为5层，根据论文为9层\n",
    "def convertY2Hieral(y):\n",
    "    #mat2acc\n",
    "    # [[0.914 0.009 0.017 0.007 0.032 0.    0.    0.    0.   ]\n",
    "    # [0.027 0.984 0.006 0.007 0.018 0.    0.    0.    0.   ]\n",
    "    # [0.02  0.006 0.972 0.    0.011 0.    0.    0.    0.   ]\n",
    "    # [0.036 0.002 0.    0.986 0.014 0.    0.002 0.    0.   ]\n",
    "    # [0.003 0.    0.    0.    0.925 0.    0.    0.    0.   ]\n",
    "    # [0.    0.    0.    0.    0.    1.    0.005 0.    0.   ]\n",
    "    # [0.    0.    0.    0.    0.    0.    0.993 0.    0.004]\n",
    "    # [0.    0.    0.    0.    0.    0.    0.    0.996 0.   ]\n",
    "    # [0.    0.    0.004 0.    0.    0.    0.    0.004 0.996]]\n",
    "    \n",
    "   # labelDict = {\"0\":[\"01234\",\"0123\",\"012\",\"01\",\"0\"],\\\n",
    "   #               \"1\":[\"01234\",\"0123\",\"012\",\"01\",\"1\"],\\\n",
    "   #               \"2\":[\"01234\",\"0123\",\"012\",\"2\",\"2\"],\\\n",
    "   #               \"3\":[\"01234\",\"0123\",\"3\",\"3\",\"3\"],\\\n",
    "   #              \"4\":[\"01234\",\"4\",    \"4\",\"4\",\"4\"],\\\n",
    "   #              \"5\":[\"5678\",\"5\",     \"5\",\"5\",\"5\"],\\\n",
    "   #              \"6\":[\"5678\",\"678\",   \"67\",\"6\",\"6\"],\\\n",
    "   #              \"7\":[\"5678\",\"678\",   \"67\",\"7\",\"7\"],\\\n",
    "   #              \"8\":[\"5678\",\"678\",   \"8\",\"8\",\"8\"],\\\n",
    "   #               }\n",
    "    labelDict = {\"0\":[\"01234\",        \"01234\",        \"01234\",  \"0123\",   \"0123\",\"012\",\"01\",\"0\"],\\\n",
    "                  \"1\":[\"01234\",        \"01234\",        \"01234\",  \"0123\",   \"0123\",\"012\",\"01\",\"1\"],\\\n",
    "                  \"2\":[\"01234\",          \"01234\",      \"01234\",  \"0123\",   \"0123\",\"012\",\"2\",\"2\"],\\\n",
    "                  \"3\":[\"01234\",         \"01234\",       \"01234\",  \"0123\",  \"0123\",\"3\",\"3\",\"3\"],\\\n",
    "                 \"4\":[\"01234\",          \"01234\",       \"01234\",    \"4\"       \"4\", \"4\",\"4\",\"4\"],\\\n",
    "                 \"5\":[\"5678\",               \"5\",        \"5\" ,      \"5\",     \"5\", \"5\",\"5\",\"5\"],\\\n",
    "                 \"6\":[\"5678\",            \"678\",        \"6\",        \"6\",     \"6\", \"6\",\"6\",\"6\"],\\\n",
    "                 \"7\":[\"5678\",             \"678\",       \"78\",       \"7\",     \"7\", \"7\",\"7\",\"7\"],\\\n",
    "                 \"8\":[\"5678\",             \"678\",       \"78\" ,      \"8\",     \"8\", \"8\",\"8\",\"8\"],\\\n",
    "                  }\n",
    "\n",
    "\n",
    "    y1 = [list(labelDict[str(x)]) for x in y]\n",
    "   \n",
    "    #print(\"!!!y1.type:\", type(y1))\n",
    "    #print(y1[:2])\n",
    "    #y2 = [t1[0] for t1 in y1]\n",
    "    #print(len(y2))\n",
    "    return y1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dbf652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T14:33:49.232503Z",
     "start_time": "2023-02-13T14:31:39.188869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.主程序开始，建立多层嵌套决策树模型，3080ti的GPU是AMD2400CPU 运算速度100倍\n",
      "0.这是简化程序，原始带有更多测试和原始模型的程序在mainTestCSVMLP3(hmcnf_keras).ipynb\n",
      "0.这是简化程序，只训练和测试9label模型,编号为0\n",
      "0.主程序开始, 建立多层嵌套决策树模型,3080ti的GPU是AMD2400CPU 运算速度100倍\n",
      "读取France数据并且把数据进行onehot处理\n",
      "x0.shape: (1177983, 94) y0.shape: (1177983,) y0.type: <class 'numpy.ndarray'>\n",
      "##############################################################################################################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.接编号为0的主程序,根据基于正确率的聚类程序或者经验将底层类别归结到上一层的类别\n",
      "2.程序编号为0+\n",
      "训练分离式多级模型\n",
      "x.shape: (1177983, 94) y.shape: (1177983,) y.type: <class 'numpy.ndarray'>\n",
      "(1177983,)\n",
      "../trainedModes/modelSep-level0-4layer-2slots-gpu1.h5\n",
      "Model: \"functional_24\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           [(None, 94)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 94)           376         input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_65 (Sequential)      (None, 256)          24320       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 350)          0           sequential_65[0][0]              \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_66 (Sequential)      (None, 256)          89856       concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 350)          0           sequential_66[0][0]              \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_67 (Sequential)      (None, 256)          89856       concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 350)          0           sequential_67[0][0]              \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_68 (Sequential)      (None, 256)          89856       concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sequential_69 (Sequential)      (None, 2)            514         sequential_68[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 294,778\n",
      "Trainable params: 294,590\n",
      "Non-trainable params: 188\n",
      "__________________________________________________________________________________________________\n",
      "118/118 [==============================] - 1s 13ms/step - loss: 0.0082 - accuracy: 0.9975\n",
      "Epoch 1/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 2/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 6.7777e-04 - accuracy: 0.9998\n",
      "Epoch 3/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 5.4473e-04 - accuracy: 0.9998\n",
      "Epoch 4/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 6.3773e-04 - accuracy: 0.9998\n",
      "Epoch 5/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.5397e-04 - accuracy: 0.9998\n",
      "Epoch 6/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.0922e-04 - accuracy: 0.9999\n",
      "Epoch 7/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 5.0791e-04 - accuracy: 0.9999\n",
      "Epoch 8/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 9.5331e-04 - accuracy: 0.9997\n",
      "Epoch 9/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 5.3744e-04 - accuracy: 0.9998\n",
      "Epoch 10/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 3.6444e-04 - accuracy: 0.9999\n",
      "Epoch 11/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 3.7010e-04 - accuracy: 0.9999\n",
      "Epoch 12/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 1.8429e-04 - accuracy: 1.0000\n",
      "Epoch 13/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 3.3182e-04 - accuracy: 0.9999\n",
      "Epoch 14/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 5.4081e-04 - accuracy: 0.9998\n",
      "Epoch 15/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.0141e-04 - accuracy: 0.9999\n",
      "Epoch 16/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 2.6178e-04 - accuracy: 0.9999\n",
      "Epoch 17/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.2891e-04 - accuracy: 0.9999\n",
      "Epoch 18/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 3.2492e-04 - accuracy: 0.9999\n",
      "Epoch 19/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.6771e-04 - accuracy: 0.9999\n",
      "Epoch 20/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 9.5474e-04 - accuracy: 0.9997\n",
      "Epoch 21/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.3452e-04 - accuracy: 0.9999\n",
      "Epoch 22/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 2.9777e-04 - accuracy: 0.9999\n",
      "Epoch 23/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.8092e-04 - accuracy: 0.9998\n",
      "Epoch 24/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 8.4169e-04 - accuracy: 0.9997\n",
      "Epoch 25/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.2355e-04 - accuracy: 0.9999\n",
      "Epoch 26/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 5.0791e-04 - accuracy: 0.9998\n",
      "Epoch 27/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 5.4698e-04 - accuracy: 0.9998\n",
      "Epoch 28/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 3.3454e-04 - accuracy: 0.9999\n",
      "Epoch 29/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 5.2812e-04 - accuracy: 0.9998\n",
      "Epoch 30/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.9431e-04 - accuracy: 0.9998\n",
      "Epoch 31/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 2.9117e-04 - accuracy: 0.9999\n",
      "Epoch 32/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.0010 - accuracy: 0.9996\n",
      "Epoch 33/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.7308e-04 - accuracy: 0.9998\n",
      "Epoch 34/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.0011 - accuracy: 0.9996\n",
      "Epoch 35/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 7.7686e-04 - accuracy: 0.9998\n",
      "Epoch 36/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.7346e-04 - accuracy: 0.9998\n",
      "Epoch 37/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 5.5139e-04 - accuracy: 0.9998\n",
      "Epoch 38/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 7.3000e-04 - accuracy: 0.9998\n",
      "Epoch 39/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 2.5449e-04 - accuracy: 0.9999\n",
      "Epoch 40/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 2.5429e-04 - accuracy: 0.9999\n",
      "Epoch 41/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.3325e-04 - accuracy: 0.9998\n",
      "Epoch 42/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.9043e-04 - accuracy: 0.9998\n",
      "Epoch 43/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 2.8620e-04 - accuracy: 0.9999\n",
      "Epoch 44/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 8.3828e-04 - accuracy: 0.9997\n",
      "Epoch 45/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 3.5885e-04 - accuracy: 0.9999\n",
      "Epoch 46/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 8.7454e-04 - accuracy: 0.9996\n",
      "Epoch 47/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 5.8530e-04 - accuracy: 0.9998\n",
      "Epoch 48/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 9.3937e-04 - accuracy: 0.9997\n",
      "Epoch 49/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 3.2157e-04 - accuracy: 0.9999\n",
      "Epoch 50/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.5289e-04 - accuracy: 0.9999\n",
      "Epoch 51/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 6.0124e-04 - accuracy: 0.9998\n",
      "Epoch 52/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 6.1026e-04 - accuracy: 0.9998\n",
      "Epoch 53/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 6.1190e-04 - accuracy: 0.9998\n",
      "Epoch 54/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.6741e-04 - accuracy: 0.9998\n",
      "Epoch 55/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 3.5435e-04 - accuracy: 0.9999\n",
      "Epoch 56/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 1.9951e-04 - accuracy: 0.9999\n",
      "Epoch 57/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 1.4836e-04 - accuracy: 1.0000\n",
      "Epoch 58/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 5.3303e-04 - accuracy: 0.9998\n",
      "Epoch 59/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 5.9631e-04 - accuracy: 0.9998\n",
      "Epoch 60/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 5.1651e-04 - accuracy: 0.9998\n",
      "Epoch 61/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 6.6288e-04 - accuracy: 0.9998\n",
      "Epoch 62/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 7.1180e-04 - accuracy: 0.9998\n",
      "Epoch 63/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 6.2314e-04 - accuracy: 0.9997\n",
      "Epoch 64/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 9.9528e-04 - accuracy: 0.9996\n",
      "Epoch 65/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 3.0603e-04 - accuracy: 0.9999\n",
      "Epoch 66/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 5.1268e-04 - accuracy: 0.9998\n",
      "Epoch 67/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 5.0307e-04 - accuracy: 0.9998\n",
      "Epoch 68/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 3.2232e-04 - accuracy: 0.9999\n",
      "Epoch 69/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.2884e-04 - accuracy: 0.9998\n",
      "Epoch 70/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 5.6015e-04 - accuracy: 0.9998\n",
      "Epoch 71/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 2.5731e-04 - accuracy: 0.9999\n",
      "Epoch 72/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 2.2408e-04 - accuracy: 0.9999\n",
      "Epoch 73/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 2.2040e-04 - accuracy: 0.9999\n",
      "Epoch 74/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 6.0808e-04 - accuracy: 0.9998\n",
      "Epoch 75/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 8.9482e-04 - accuracy: 0.9998\n",
      "Epoch 76/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.4179e-04 - accuracy: 0.9999\n",
      "Epoch 77/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.0011 - accuracy: 0.9996\n",
      "Epoch 78/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 7.0695e-04 - accuracy: 0.9998\n",
      "Epoch 79/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.4697e-04 - accuracy: 0.9999\n",
      "Epoch 80/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 2.1906e-04 - accuracy: 1.0000\n",
      "Epoch 81/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 2.1201e-04 - accuracy: 0.9999\n",
      "Epoch 82/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 3.6343e-04 - accuracy: 0.9999\n",
      "Epoch 83/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 2.7231e-04 - accuracy: 0.9999\n",
      "Epoch 84/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 6.9111e-04 - accuracy: 0.9998\n",
      "Epoch 85/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 3.4220e-04 - accuracy: 0.9999\n",
      "Epoch 86/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.7130e-04 - accuracy: 0.9998\n",
      "Epoch 87/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 6.3733e-04 - accuracy: 0.9998\n",
      "Epoch 88/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 6.7844e-04 - accuracy: 0.9997\n",
      "Epoch 89/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 3.6959e-04 - accuracy: 0.9999\n",
      "Epoch 90/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 5.5826e-04 - accuracy: 0.9998\n",
      "Epoch 91/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 9.1787e-04 - accuracy: 0.9998\n",
      "Epoch 92/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 5.1077e-04 - accuracy: 0.9998\n",
      "Epoch 93/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 4.6250e-04 - accuracy: 0.9998\n",
      "Epoch 94/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 2.1017e-04 - accuracy: 0.9999\n",
      "Epoch 95/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 7.6373e-04 - accuracy: 0.9998\n",
      "Epoch 96/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 3.9599e-04 - accuracy: 0.9999\n",
      "Epoch 97/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 2.6161e-04 - accuracy: 0.9999\n",
      "Epoch 98/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 2.9449e-04 - accuracy: 0.9999\n",
      "Epoch 99/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 8.2380e-04 - accuracy: 0.9997\n",
      "Epoch 100/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 5.5875e-04 - accuracy: 0.9998\n",
      "Epoch 101/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 2.8330e-04 - accuracy: 0.9999\n",
      "Epoch 102/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 4.6280e-04 - accuracy: 0.9999"
     ]
    }
   ],
   "source": [
    "##主程序开始######################################################################################################################\n",
    "print(\"0.主程序开始，建立多层嵌套决策树模型，3080ti的GPU是AMD2400CPU 运算速度100倍\")\n",
    "print(\"0.这是简化程序，原始带有更多测试和原始模型的程序在mainTestCSVMLP3(hmcnf_keras).ipynb\")\n",
    "print(\"0.这是简化程序，只训练和测试9label模型,编号为0\")\n",
    "########################################################################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import pickle  \n",
    "\n",
    "\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，所以采用分离式\n",
    "###每一层的识别模型都是4层模型\n",
    "def sepHier1(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 10,srelu_size = 256,dropout_rate = 0.05):\n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    featuresInput = layers.Input(shape=(features_size,))\n",
    "    features = layers.BatchNormalization()(featuresInput)\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[featuresInput], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=40000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "print(\"0.主程序开始, 建立多层嵌套决策树模型,3080ti的GPU是AMD2400CPU 运算速度100倍\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "#from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "#policy = mixed_precision.Policy('mixed_float16')\n",
    "#mixed_precision.set_policy(policy)\n",
    "\n",
    " \n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "print(\"读取France数据并且把数据进行onehot处理\")\n",
    "\n",
    "#file1 = \"../trainData/france_0_allSamples1.csv\"\n",
    "file1 = \"../trainData/france_0_allSamples1_2slot.csv\"\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "h,w = xyData.shape\n",
    "#x = xyData[:,1:23]#简单处理与SUMO数据库一致\n",
    "x0rigin = xyData[:,1:w-1]#用所有的数据,第0列为vehID,不要\n",
    "y0rigin  = xyData[:,w-1]\n",
    "\n",
    "x0rigin[:,6] = [string2int(inputString) for inputString in x0rigin[:,6] ]#字符串vehLaneID 变为整数\n",
    "\n",
    "x0rigin =x0rigin.astype(np.float32)#GPU 加这个\n",
    "y0rigin =y0rigin.astype(np.int64)#GPU 加这个\n",
    "\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "x0,y0= ros.fit_resample(x0rigin , y0rigin)#对数据不平衡进行处理，保证样本数一致\n",
    "\n",
    "x0=x0.astype(np.float32)#GPU 加这个\n",
    "y0=y0.astype(np.int64)#GPU 加这个\n",
    "yl5 = y0\n",
    "print(\"x0.shape:\",x0.shape,\"y0.shape:\",y0.shape,\"y0.type:\", type(y0) )\n",
    "del xyDataTmp #节省内存\n",
    "del xyData #节省内存\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "###现在暂时不训练多层模型，只训练9label模型\n",
    "if 0:\n",
    "    print(\"训练4层, 9 label 模型\")\n",
    "    x=x0\n",
    "    y=yl5\n",
    "    x=x.astype(np.float32)#GPU 加这个\n",
    "    y=y.astype(np.int64)#GPU 加这个\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    \n",
    "    num_labels = 9 \n",
    "    nSamples,nFeatures =  x.shape\n",
    "    enc = OneHotEncoder()\n",
    "    y= y.reshape(nSamples,-1)\n",
    "    \n",
    "    print(\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    enc.fit(y)\n",
    "    yOneHot=enc.transform(y).toarray()\n",
    "    saveName = \"../trainedModes/model-9label-4lays-512nodes-2slots-gpu1.h5\"\n",
    "    if 0:\n",
    "        kerasModel3_5label = kerasFitAndSaveSimple3LikeResnet(x,yOneHot,num_labels,saveName)     \n",
    "    yKeras_5label=getKerasResnetRVL(x,enc,saveName)\n",
    "    \n",
    "    print('keras\\n')\n",
    "    mat1num = confusion_matrix(y, yKeras_5label)\n",
    "    mat2acc = confusion_matrix(y, yKeras_5label,normalize='pred')\n",
    "    print('mat1num\\n',mat1num)\n",
    "    print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "    \n",
    "    \n",
    "########################################################################################################################    \n",
    "########################################################################################################################   \n",
    "########################################################################################################################    \n",
    "########################################################################################################################    \n",
    "print(\"##############################################################################################################\")\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"1.接编号为0的主程序,根据基于正确率的聚类程序或者经验将底层类别归结到上一层的类别\")\n",
    "print(\"2.程序编号为0+\")  \n",
    " \n",
    "'''\n",
    "if 0:# 训练统合样式的HMCN-F多级模型\n",
    "    print(\"训练5hieral, 4层, 9 label 模型\")\n",
    "    x=x0\n",
    "    y=yl5\n",
    "    x=x.astype(np.float32)#GPU 加这个\n",
    "    y=y.astype(np.int64)#GPU 加这个\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    yH1 = convertY2Hieral(y)\n",
    "    hierarchy = [2,4,6,8,9]#5层，对于当前数据集已经足够了\n",
    "    \n",
    "    yH1= np.array(yH1)\n",
    "    \n",
    "    nSamples,nFeatures =  x.shape\n",
    "    enc = OneHotEncoder()\n",
    "    yH1= yH1.reshape(nSamples,-1)\n",
    "    #print(yH1[:3])\n",
    "    print(\"yH1.shape:\",yH1 .shape,\"yH1.type:\", type(yH1) )\n",
    "    enc.fit(yH1)\n",
    "    #print(enc.categories_,enc.get_feature_names())\n",
    "    yOneHot=enc.transform(yH1).toarray()\n",
    "    #print(yOneHot[:3])\n",
    "    \n",
    "    num_labels = yOneHot.shape[1] \n",
    "    print(num_labels)\n",
    "    saveName = \"../trainedModes/model-5hier-9label-5lays-128nodes-2slots-gpu1.h5\"\n",
    "    kerasModel4_5hier_9label = kerasFitAndSaveHierSimple4LikeResnet(x,yOneHot,num_labels,saveName)   \n",
    "'''    \n",
    "    \n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 1:# 训练多级模型\n",
    "    print(\"训练分离式多级模型\")\n",
    "    \n",
    "    #准备字典，用于保存训练后的数据\"\n",
    "    xFloors=  dict()\n",
    "    yFloors =  dict()\n",
    "    modSaveNameFloors =dict()\n",
    "    encLevels= dict()\n",
    "    yKerasFloors = dict()\n",
    "    x=x0\n",
    "    y=yl5\n",
    "    x=x.astype(np.float32)#GPU 加这个\n",
    "    y=y.astype(np.int64)#GPU 加这个\n",
    "    print(\"x.shape:\",x .shape,\"y.shape:\",y .shape,\"y.type:\", type(y) )\n",
    "    yH1 = convertY2Hieral(y)\n",
    "   \n",
    "   \n",
    "    nSamples,nFeatures =  x.shape\n",
    "    #hierarchy = [2,4,6,8,9]\n",
    "    hierarchy = [2,3,4,5,6,7,8,9]\n",
    "    numEpochs = 500 #1500/60/60*5 = 2houer\n",
    "    for i in range(len(hierarchy)):\n",
    "        levelIndex = i\n",
    "        numLayers = 4\n",
    "        enc = OneHotEncoder()\n",
    "        yCurLayer1 = [t1[i] for t1 in yH1]\n",
    "        \n",
    "        yCurLayer1 = np.array(yCurLayer1)\n",
    "        print(yCurLayer1.shape)\n",
    "        \n",
    "        yCurLayer1= yCurLayer1.reshape(nSamples,-1)\n",
    "        enc.fit(yCurLayer1)\n",
    "        \n",
    "        yOneHot=enc.transform(yCurLayer1).toarray()\n",
    "        #print(enc.categories_,enc.get_feature_names())\n",
    "        #print(yOneHot[:1])\n",
    "        \n",
    "        \n",
    "        num_labels = yOneHot.shape[1] \n",
    "        saveName = \"../trainedModes/modelSep-level%d-%dlayer-2slots-gpu1.h5\" %(i,numLayers)\n",
    "        print(saveName)\n",
    "        sepHier1(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs)\n",
    "        \n",
    "        encLevels[str(i)] = enc\n",
    "        xFloors[str(i)] = x\n",
    "        yFloors[str(i)] = yCurLayer1\n",
    "        modSaveNameFloors[str(i)] = saveName\n",
    "        \n",
    "    #######保存为pickle文件,用于后期的SUMO和数据分析\n",
    "\n",
    "    fpk=open('samples1.pkf','wb+')  \n",
    "    pickle.dump([xFloors,yFloors,modSaveNameFloors,encLevels],fpk)  \n",
    "    fpk.close() \n",
    "\n",
    "########################################################################################################################    \n",
    "########################################################################################################################\n",
    "#####用现有训练模型进行预测\n",
    "fpk=open('samples1.pkf','rb')   \n",
    "[xFloors,yFloors,modSaveNameFloors,encLevels]=pickle.load(fpk)  \n",
    "fpk.close()  \n",
    "\n",
    "#hierarchy = [2,4,6,8,9]\n",
    "hierarchy = [2,3,4,5,6,7,8,9]\n",
    "yKerasFloors = dict()\n",
    "\n",
    "for i in range(len(hierarchy)):\n",
    "        levelIndex = i\n",
    "        x = xFloors[str(i)]\n",
    "        yCurLayer1 =  yFloors[str(i)]\n",
    "        saveName =  modSaveNameFloors[str(i)] \n",
    "        enc = encLevels[str(i)]\n",
    "        yOneHot=enc.transform(yCurLayer1).toarray()\n",
    "        yPredict=getKerasResnetRVL(x,enc,saveName)\n",
    "        print(\"分离式多层识别结果:第%d层\\n\" %i)\n",
    "        mat1num = confusion_matrix(yCurLayer1,yPredict)\n",
    "        print(mat1num)\n",
    "        mat2acc = confusion_matrix(yCurLayer1,yPredict,normalize='pred')  \n",
    "        print(np.around(mat2acc , decimals=3))\n",
    "        yKerasFloors[str(i)] =  yPredict\n",
    "        \n",
    "fpk=open('samples2.pkf','wb')  \n",
    "pickle.dump([xFloors,yFloors,modSaveNameFloors,encLevels,yKerasFloors],fpk)  \n",
    "fpk.close() \n",
    "        \n",
    "########################################################################################################################   \n",
    "########################################################################################################################    \n",
    "print(\"##############################################################################################################\")\n",
    "print(\"基于SUMO和蒙特卡洛模拟分析生成预测当前样本的最小速度，原始对应程序为mainSimSumoFranceData，需要在docker的原始环境下运行，没有conda\")\n",
    "print(\"2.程序编号为0.1\")\n",
    "!python3 sumoSimByFrance.py\n",
    "\n",
    " \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6315ad19-ec9e-4603-a84a-22182c39dcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################################################################\n",
      "2.程序编号为0.2\n",
      "##############################################################################################################\n",
      "2.程序编号为0.2，主程序开始运行\n",
      "(10000, 1)\n",
      "Model: \"functional_48\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_29 (InputLayer)           [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_125 (Sequential)     (None, 256)          1024        input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_75 (Concatenate)    (None, 259)          0           sequential_125[0][0]             \n",
      "                                                                 input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_126 (Sequential)     (None, 256)          66560       concatenate_75[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_76 (Concatenate)    (None, 259)          0           sequential_126[0][0]             \n",
      "                                                                 input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_127 (Sequential)     (None, 256)          66560       concatenate_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_77 (Concatenate)    (None, 259)          0           sequential_127[0][0]             \n",
      "                                                                 input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_128 (Sequential)     (None, 256)          66560       concatenate_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sequential_129 (Sequential)     (None, 2)            514         sequential_128[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 201,218\n",
      "Trainable params: 201,218\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 0.9994\n",
      "Epoch 1/15\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0078 - accuracy: 0.9993\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0078 - accuracy: 0.9994\n",
      "Epoch 3/15\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0078 - accuracy: 0.9995\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0078 - accuracy: 0.9995\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0078 - accuracy: 0.9995\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0078 - accuracy: 0.9995\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0078 - accuracy: 0.9994\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0078 - accuracy: 0.9995\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0078 - accuracy: 0.9995\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0078 - accuracy: 0.9995\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0078 - accuracy: 0.9994\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0078 - accuracy: 0.9994\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0078 - accuracy: 0.9995\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0078 - accuracy: 0.9994\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0078 - accuracy: 0.9994\n",
      "(10000, 1)\n",
      "Model: \"functional_50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_30 (InputLayer)           [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_130 (Sequential)     (None, 256)          1792        input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_78 (Concatenate)    (None, 262)          0           sequential_130[0][0]             \n",
      "                                                                 input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_131 (Sequential)     (None, 256)          67328       concatenate_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_79 (Concatenate)    (None, 262)          0           sequential_131[0][0]             \n",
      "                                                                 input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_132 (Sequential)     (None, 256)          67328       concatenate_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_80 (Concatenate)    (None, 262)          0           sequential_132[0][0]             \n",
      "                                                                 input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_133 (Sequential)     (None, 256)          67328       concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sequential_134 (Sequential)     (None, 4)            1028        sequential_133[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 204,804\n",
      "Trainable params: 204,804\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 0.9995\n",
      "Epoch 1/15\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0038 - accuracy: 0.9995\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 0.9995\n",
      "Epoch 3/15\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 0.9995\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0039 - accuracy: 0.9995\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0039 - accuracy: 0.9995\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0039 - accuracy: 0.9995\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0039 - accuracy: 0.9995\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 0.9995\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0038 - accuracy: 0.9995\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 0.9995\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 0.9995\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0039 - accuracy: 0.9995\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 0.9995\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 0.9995\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0038 - accuracy: 0.9995\n",
      "(10000, 1)\n",
      "Model: \"functional_52\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_31 (InputLayer)           [(None, 9)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_135 (Sequential)     (None, 256)          2560        input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_81 (Concatenate)    (None, 265)          0           sequential_135[0][0]             \n",
      "                                                                 input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_136 (Sequential)     (None, 256)          68096       concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_82 (Concatenate)    (None, 265)          0           sequential_136[0][0]             \n",
      "                                                                 input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_137 (Sequential)     (None, 256)          68096       concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_83 (Concatenate)    (None, 265)          0           sequential_137[0][0]             \n",
      "                                                                 input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_138 (Sequential)     (None, 256)          68096       concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sequential_139 (Sequential)     (None, 6)            1542        sequential_138[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 208,390\n",
      "Trainable params: 208,390\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0021 - accuracy: 0.9980\n",
      "Epoch 1/15\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0020 - accuracy: 0.9981\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0021 - accuracy: 0.9985\n",
      "Epoch 3/15\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 0.9981\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0020 - accuracy: 0.9984\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0022 - accuracy: 0.9984\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.9982\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0020 - accuracy: 0.9988\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0020 - accuracy: 0.9985\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.9989\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0020 - accuracy: 0.9982\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0019 - accuracy: 0.9988\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0019 - accuracy: 0.9990\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0020 - accuracy: 0.9987\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.9985\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0021 - accuracy: 0.9987\n",
      "(10000, 1)\n",
      "Model: \"functional_54\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_32 (InputLayer)           [(None, 12)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_140 (Sequential)     (None, 256)          3328        input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_84 (Concatenate)    (None, 268)          0           sequential_140[0][0]             \n",
      "                                                                 input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_141 (Sequential)     (None, 256)          68864       concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_85 (Concatenate)    (None, 268)          0           sequential_141[0][0]             \n",
      "                                                                 input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_142 (Sequential)     (None, 256)          68864       concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_86 (Concatenate)    (None, 268)          0           sequential_142[0][0]             \n",
      "                                                                 input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_143 (Sequential)     (None, 256)          68864       concatenate_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sequential_144 (Sequential)     (None, 7)            1799        sequential_143[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 211,719\n",
      "Trainable params: 211,719\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:1605 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4829 binary_crossentropy\n        bce = target * math_ops.log(output + epsilon())\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1140 binary_op_wrapper\n        raise e\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1124 binary_op_wrapper\n        return func(x, y, name=name)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1456 _mul_dispatch\n        return multiply(x, y, name=name)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:508 multiply\n        return gen_math_ops.mul(x, y, name)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py:6176 mul\n        \"Mul\", x=x, y=y, name=name)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal\n        compute_device)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:3485 _create_op_internal\n        op_def=op_def)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1975 __init__\n        control_input_ops, op_def)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1815 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 8 and 7 for '{{node binary_crossentropy/mul}} = Mul[T=DT_FLOAT](IteratorGetNext:1, binary_crossentropy/Log)' with input shapes: [10000,8], [10000,7].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ed9fc98aaac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mnumEpochs\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput1OneHot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0msepHier2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput1OneHot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaveName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlevelIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumLayers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumEpochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mmodSaveNameFloors_S2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaveName\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-ed9fc98aaac0>\u001b[0m in \u001b[0;36msepHier2\u001b[0;34m(x, yOneHot, num_labels, saveName, levelIndex, numLayers, numEpochs, srelu_size, dropout_rate)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m#build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mbuild_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mbuild_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myOneHot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:1605 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4829 binary_crossentropy\n        bce = target * math_ops.log(output + epsilon())\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1140 binary_op_wrapper\n        raise e\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1124 binary_op_wrapper\n        return func(x, y, name=name)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1456 _mul_dispatch\n        return multiply(x, y, name=name)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:508 multiply\n        return gen_math_ops.mul(x, y, name)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py:6176 mul\n        \"Mul\", x=x, y=y, name=name)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal\n        compute_device)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:3485 _create_op_internal\n        op_def=op_def)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1975 __init__\n        control_input_ops, op_def)\n    /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1815 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 8 and 7 for '{{node binary_crossentropy/mul}} = Mul[T=DT_FLOAT](IteratorGetNext:1, binary_crossentropy/Log)' with input shapes: [10000,8], [10000,7].\n"
     ]
    }
   ],
   "source": [
    "print(\"##############################################################################################################\")\n",
    "\n",
    "print(\"2.程序编号为0.2\")\n",
    "\n",
    "########################################################################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import pickle \n",
    "\n",
    "############################################################################\n",
    "############################################################################\n",
    "####HMCM-F ,层次模型，发现hmcn-f训练效果很差，所以采用分离式\n",
    "###每一层的识别模型都是4层模型\n",
    "def sepHier2(x,yOneHot,num_labels,saveName,levelIndex,numLayers,numEpochs = 10,srelu_size = 256,dropout_rate = 0.05):\n",
    "    str1=\"layIndex-\"+str(levelIndex)\n",
    "    \n",
    "    nSamples,features_size = x.shape\n",
    "    relu_size = 256\n",
    "    dropout_rate = 0.05\n",
    "    global_models = []\n",
    "    \n",
    "    label_size = num_labels\n",
    "    features = layers.Input(shape=(features_size,))\n",
    "    #features = layers.BatchNormalization(features,axis=1) #相对于sepHier1，加入batch_normalization\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "        else:\n",
    "            global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "    \n",
    "    p_glob = softmax_model(label_size)(global_models[-1])\n",
    "    build_model = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "\n",
    "    \n",
    "    build_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    if 1:\n",
    "        build_model = keras.models.load_model(saveName)\n",
    "    if 1:#用于画图\n",
    "        #build_model.fit([x],[yOneHot],epochs=1, batch_size=10000*1)\n",
    "        build_model.summary()\n",
    "        build_model.fit(x,yOneHot,epochs=1, batch_size=10000*1)\n",
    "        plot_model(build_model, to_file=str1+\".jpg\", show_shapes=True)\n",
    "    \n",
    "  \n",
    "    build_model.fit(x,yOneHot,epochs=numEpochs,batch_size=40000*1)#GPU用这个\n",
    "    build_model.save(saveName)\n",
    "    return build_model\n",
    "\n",
    "\n",
    "########################################################################################################################   \n",
    "########################################################################################################################    \n",
    "print(\"##############################################################################################################\")\n",
    "\n",
    "print(\"2.程序编号为0.2，主程序开始运行\")\n",
    "\n",
    "xFloors_S2=  dict()\n",
    "yFloors_S2 =  dict()\n",
    "modSaveNameFloors_S2 =dict()\n",
    "encLevels_S2= dict()\n",
    "yKerasFloors_S2 = dict()\n",
    "df_S2 = dict()    \n",
    "\n",
    "df = pd.read_csv('sumoSimData15000.csv', sep=',')\n",
    "hierarchy = [2,4,6,8,9]\n",
    "\n",
    "numSamples =10000\n",
    "for i in range(len(hierarchy)):\n",
    "        \n",
    "        levelIndex = i\n",
    "        df1 = df.loc[df['floor'] ==  i]\n",
    "        df1 = df1.iloc[0:numSamples]\n",
    "        \n",
    "        df1[\"yKerasOutput\"] =df1[\"yKerasOutput\"]/df[\"yKerasOutput\"].max() #看有效果吗\n",
    "        \n",
    "        input1 = df1[[\"sumoOutput\",\"yKerasOutput\",\"outputAvgSpeed\"]].to_numpy()\n",
    "        output1 = df1[\"outputY\"].to_numpy()\n",
    "        output1 = output1.reshape(-1,1)\n",
    "        print(output1.shape)\n",
    "        \n",
    "        \n",
    "        if i==0:\n",
    "            input1 = input1\n",
    "        else:\n",
    "            input1 = np.concatenate((inputLast,input1),axis=1)\n",
    "            \n",
    "        inputLast = input1\n",
    "        \n",
    "        enc = OneHotEncoder()\n",
    "        enc.fit(output1)\n",
    "        output1OneHot=enc.transform(output1).toarray()\n",
    "       \n",
    "        \n",
    "        saveName =\"../trainedModes/stage2_layerIndex=%d.h5\" %levelIndex;\n",
    "        numLayers = 4\n",
    "        numEpochs =  15\n",
    "        num_labels = output1OneHot.shape[1] \n",
    "        sepHier2(input1,output1OneHot,num_labels,saveName,levelIndex,numLayers,numEpochs)\n",
    "        \n",
    "        modSaveNameFloors_S2[str(i)] = saveName\n",
    "        xFloors_S2[str(i)]  = input1 \n",
    "        yFloors_S2[str(i)]  = output1 \n",
    "        encLevels_S2[str(i)] = enc\n",
    "        df_S2[str(i)] = df1\n",
    "        \n",
    "fpk=open('samples2-stage2.pkf','wb')  \n",
    "pickle.dump([xFloors_S2,yFloors_S2,df_S2,modSaveNameFloors_S2,encLevels_S2],fpk)  \n",
    "fpk.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8674b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T14:36:52.438549Z",
     "start_time": "2023-02-13T14:36:16.048928Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "########################################################################################################################\n",
    "print(\"1.接编号为0的主程序,对于输入的一个样本识别与各个类别的距离，并转换为概率\")\n",
    "print(\"2.对较低概率的样本进行蒙特卡洛模拟分析，原始对应程序为mainSimSumoFranceDatra\")\n",
    "print(\"3.程序编号为1\")\n",
    "########################################################################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "#import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "import copy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "########################################################################################################################\n",
    "###自定义log,用于记录\n",
    "def log(info,logtype=0):\n",
    "    \n",
    "    #*args\n",
    "    if logtype == -1:\n",
    "        return\n",
    "    if logtype == 0:\n",
    "        print(info)        \n",
    "\n",
    "\n",
    "    \n",
    "########################################################################################################################\n",
    "print(\"1.1 主程序开始,接编号为0的主程序,对于输入的一个样本识别与各个类别的距离，并转换为概率\")\n",
    "########################################################################################################################\n",
    "logtype=0\n",
    "\n",
    "xi=x0\n",
    "yi=yl5\n",
    "modeSaveName = \"../trainedModes/model-9label-4lays-512nodes-cpu1.h5\"\n",
    "model = keras.models.load_model(modeSaveName)\n",
    "yout= model.predict([xi], batch_size=2560)\n",
    "#log(xi,logtype)\n",
    "#log(yi,logtype)\n",
    "#log(yout,logtype)\n",
    "ymax1=np.max(yout,axis=1)\n",
    "index = np.where(ymax1<0.95)[0]#提取最大值小于0.95的例子\n",
    "ylowpraPredictNN=yout[index]#对较低概率的样本\n",
    "xlowpra=xi[index]\n",
    "ylowpraLabel = yi[index]\n",
    "#log(xi,logtype)\n",
    "#log(yi,logtype)\n",
    "#log(ylowpra)\n",
    "#log(xlowpra)\n",
    "#log(index)\n",
    "\n",
    "    \n",
    "#######保存为pickle文件\n",
    "\n",
    "##保存低概率的样本\n",
    "fpk=open('../trainData/lowprobSamples.pkf','wb+')  \n",
    "pickle.dump([index,xlowpra,ylowpraLabel,ylowpraPredictNN],fpk)  \n",
    "fpk.close() \n",
    "del xlowpra,ylowpraLabel\n",
    "fpk=open('../trainData/lowprobSamples.pkf','rb')   \n",
    "[index1,xlowprob1,ylowprobLabel1,ylowprobPredictNN1]=pickle.load(fpk)  \n",
    "fpk.close()  \n",
    "\n",
    "\n",
    "log(index1)\n",
    "#log(ylowprobLabel1)\n",
    "log(np.round(ylowprobPredictNN1,2))\n",
    "\n",
    "##保存所有的的样本\n",
    "fpk=open('../trainData/allSamples9label.pkf','wb+')  \n",
    "pickle.dump([x0,yl5,yout],fpk)  \n",
    "fpk.close() \n",
    "\n",
    "fpk=open('../trainData/allSamples9label.pkf','rb')   \n",
    "[x0tmp,yl5tmp,ykerasNNtmp]=pickle.load(fpk)  \n",
    "fpk.close()  \n",
    "\n",
    "#log(np.round(ykerasNNtmp,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c074866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T09:51:42.931165Z",
     "start_time": "2023-01-28T09:51:42.771943Z"
    }
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "print(\"接程序1.1: 1.2 对较低概率的样本进行蒙特卡洛模拟分析,原始对应程序为mainSimSumoFranceDatra\")\n",
    "print(\"因为配置失误，采用将低概率的样本进行保存为文件，然后再root用户下命令行模式用SUMO模拟（不使用conda）\")\n",
    "########################################################################################################################\n",
    "!python runSumoSimFun.py#运行runSumoSimFun.py 中test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f393658",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T14:37:52.867395Z",
     "start_time": "2023-02-13T14:37:49.763146Z"
    }
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "print(\"接程序1.2: 1.3 基于pickle样本对模型进行重训练\")\n",
    "\n",
    "########################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add02d9c-af03-4a00-8629-42b8a751033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "print(\"接程序1.3: 1.4 对模拟后的数据进行分析，计算正确率\")\n",
    "########################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"对于低概率样本的识别结果，采用keras和MCS的识别结果对比\")\n",
    "file1 = \"./data-Copy1.csv\"\n",
    "xyDataTmp = pd.read_csv(file1,index_col=0)\n",
    "\n",
    "print(xyDataTmp.head(3))\n",
    "print(xyDataTmp.info())\n",
    "\n",
    "file1 = \"。./trainData/france_0_allSamples1.csv\"\n",
    "xyOrigin = pd.read_csv(file1,index_col=0)\n",
    "\n",
    "originlabel =  xyDataTmp.iloc[:,1].to_numpy()\n",
    "keraslabel =   xyDataTmp.iloc[:,2].to_numpy()      \n",
    "mcslabel =     xyDataTmp.iloc[:,3].to_numpy()\n",
    "\n",
    "#\n",
    "    \n",
    "print('\\norigin_mcs')\n",
    "mat1num = confusion_matrix(originlabel, mcslabel)\n",
    "mat2acc = confusion_matrix(originlabel, mcslabel,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "      \n",
    "print('\\nmcs_keras')\n",
    "mat1num = confusion_matrix(mcslabel, keraslabel)\n",
    "mat2acc = confusion_matrix(mcslabel, keraslabel,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))\n",
    "\n",
    "print('\\norgin_keras')\n",
    "mat1num = confusion_matrix(originlabel, keraslabel)\n",
    "mat2acc = confusion_matrix(originlabel ,keraslabel,normalize='pred')\n",
    "print('mat1num\\n',mat1num)\n",
    "print('mat2acc\\n',np.around(mat2acc , decimals=3))      \n",
    "\n",
    "##用于分析实际标记类别大于预测标记类别\n",
    "def analyzing1(tmp, xyDataTmp,xyOrigin): \n",
    "    dfTmp1 = xyDataTmp[tmp]\n",
    "    #print(dfTmp1.head(5))\n",
    "    \n",
    "    \n",
    "    \n",
    "    df2 =  xyOrigin.iloc[dfTmp1.originIndex,:]   \n",
    "    plt.show()\n",
    "    df2[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    #print(df2.info())\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    \n",
    "    df2.to_csv(\"tmpForAnalyzing.csv\")\n",
    "    \n",
    "    tmp1 = df2['redLightTime'] - df2['arriveTime2'] >1.5 #红灯时间大于到达时间\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 = df2[df2['redLightTime'] - df2['arriveTime2'] >1.5] #红灯时间大于到达时间 ,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing3.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    \n",
    "   \n",
    "    tmp1 = df2['speed'] < 5/3.6 #本身速度就小于5/3.6\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df2['speed'] > 5/3.6 #本身速度就小于5/3.6,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    df3.to_csv(\"tmpForAnalyzing4.csv\")\n",
    "    \n",
    "    tmp1 = df2['redLightTime'] - df2['arriveTime2'] >1.5  #红灯时间大于到达时间\n",
    "    tmp1 = tmp1 | (df2['speed'] < 5/3.6) #本身速度就小于5/3.6\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"红灯时间大于到达时间  or 本身速度就小于5/3.6,df3 shape:\",df3.shape,\"占输入样本比例为:\",df3.shape[0]/df2.shape[0])\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    df3.to_csv(\"tmpForAnalyzing5.csv\")\n",
    "    \n",
    "    \n",
    "    tmp1 = df2['arriveTime2'] - df2['redLightTime'] >0 #到达时间大于红灯时间\n",
    "    tmp1 = tmp1 & (df2['speed'] > 5/3.6) #本身速度就大于于5/3.6\n",
    "    tmp1 = tmp1 & (df2['vehPos_2'] > 0) #\n",
    "    tmp1 = tmp1 & (df2['vehSpeed_2'] < 5/3.6) #\n",
    "    tmp1 = tmp1 & (df2['vehPos_3'] >0) #\n",
    "    tmp1 = tmp1 & (df2['vehSpeed_3'] <5/3.6) #\n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"到达时间大于红灯时间  and 本身速度就大于5/3.6,df3 shape:\",df3.shape,\"占输入样本比例为:\",df3.shape[0]/df2.shape[0])\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    df3.to_csv(\"tmpForAnalyzing6.csv\")\n",
    "\n",
    "def extractStillVeh2(df):\n",
    "    df=df.rename(columns={'ArrTimeDivRedTime':'numStillVeh'})\n",
    "    df=df.rename(columns={'lanAvgSpeed':'predictStats'})\n",
    "    df['numStillVeh'] = 0\n",
    "    df['predictStats'] = \"unknown\"\n",
    "    for i in range(df.shape[0]):\n",
    "        numStillVeh = 0\n",
    "        tmp = df.iloc[i]\n",
    "        redTime = tmp.iloc[0]\n",
    "        vPosObj = tmp.iloc[1]\n",
    "        predictStats = -1\n",
    "\n",
    "        for j in range(20):\n",
    "           \n",
    "            vehPos = tmp.iloc[2*j+8]\n",
    "            vehVeh = tmp.iloc[2*j+1+8]\n",
    "            \n",
    "            if vehPos >0 and vehVeh<5/3.6:#经验数据,参数\n",
    "                numStillVeh = numStillVeh + 1\n",
    "            elif vehPos >0 and  vPosObj > vehPos:\n",
    "                timeTmp1 =(vehPos-j*6.5)/(vehVeh+0.001)#经验公式，到固定位置后，启动需要的时间\n",
    "                if timeTmp1  < redTime +numStillVeh*1.5:\n",
    "                    numStillVeh = numStillVeh + 1\n",
    "\n",
    "            if vehPos >0 and vPosObj == vehPos and vehVeh<5/3.6:\n",
    "                predictStats = \"stop\"#目标车要听停止\n",
    "               \n",
    "\n",
    "            if vehPos >0 and vPosObj == vehPos and vehVeh>5/3.6 :    \n",
    "                timeTmp1 =(vehPos-j*6.5)/(vehVeh+0.001)#经验公式，到固定位置后需要的时间\n",
    "                if timeTmp1  <= redTime +numStillVeh*1.5+1.5:#小于虚拟红灯结束时间\n",
    "                     predictStats = \"stop\" #目标车要听停止\n",
    "                else:        \n",
    "                     predictStats = \"no stop\"  #目标车要不要停止\n",
    "                \n",
    "     \n",
    "        df['numStillVeh'][i] = numStillVeh\n",
    "        df['predictStats'][i] = predictStats\n",
    "\n",
    "    return df\n",
    "##用于分析实际标记类别小于预测标记类别， xyDataTmp[\"predicted Labels By MCS\"] - xyDataTmp[\"origin speedFlag\"]>0\n",
    "def analyzing2(tmp, xyDataTmp,xyOrigin): \n",
    "    dfTmp1 = xyDataTmp[tmp]\n",
    "    #print(dfTmp1.head(5))\n",
    "    \n",
    "    \n",
    "    #1\n",
    "    df2 =  xyOrigin.iloc[dfTmp1.originIndex,:]   \n",
    "    plt.show()\n",
    "    df2[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    #print(df2.info())\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df2 = extractStillVeh2( df2)\n",
    "    df2.to_csv(\"tmpForAnalyzing.csv\")\n",
    "    \n",
    "    \n",
    "    ######################################\n",
    "    tmp1 = (df2['speedFlag'] == 0)  & (df2['predictStats'] == \"stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing1.csv\")\n",
    "    \n",
    "    tmp1 = (df2['speedFlag'] > 0)  & (df2['predictStats'] == \"no stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing2.csv\")\n",
    "    \n",
    "    \n",
    "    tmp1 = (df2['speedFlag'] == 0)  & (df2['predictStats'] == \"no stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing3.csv\")\n",
    "    \n",
    "    tmp1 = (df2['speedFlag'] > 0)  & (df2['predictStats'] == \"stop\") #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    df3.to_csv(\"tmpForAnalyzing4.csv\")\n",
    "    return\n",
    "    \n",
    "    '''\n",
    "    #2\n",
    "    tmp1 = df2['redLightTime'] - df2['arriveTime2'] >0 #红灯时间大于到达时间，这个结果难以理解\n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 = df2[df2['redLightTime'] - df2['arriveTime2'] >0] #红灯时间大于到达时间 ,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing2.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))   \n",
    "    \n",
    "    #3\n",
    "    tmp1 =df2['arriveTime2'] - df2['redLightTime'] >3 #红灯时间小于到达时间3，\n",
    "    tmp1 = tmp1 & (df2['speedFlag'] == 0) \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"#红灯时间小于于到达时间 ,df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing3.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))  \n",
    "    \n",
    "   \n",
    "    #4据静止汽车数目，分析在df2['speedFlag'] > 0情况下，虚拟红灯时间小于于到达时间情况，也就是目标车可能不需要停下来\n",
    "    \n",
    "    tmp1 =(df2['speedFlag'] == 0) \n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 < df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 = tmp1 & tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing4.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))  \n",
    "    \n",
    "    #5 据静止汽车数目，分析在df2['speedFlag'] > 0情况下，虚拟红灯时间大于到达时间情况，也就是目标车可能需要停下来\n",
    "    tmp1 =(df2['speedFlag'] > 0) \n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 >= df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 = tmp1 & tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing5.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))  \n",
    "    \n",
    "    \n",
    "    #6 据静止汽车数目，分析虚拟红灯时间大于到达时间情况，也就是目标车可能需要停下来\n",
    "  \n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 >= df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 =  tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing6.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4))\n",
    "    \n",
    "    \n",
    "    #7 根据静止汽车数目，分析虚拟红灯时间小于到达时间情况，也就是目标车可能不需要停下来\n",
    "    tmp11 = df2['numStillVeh']*1.5+df2['redLightTime']+1.5\n",
    "    #print(tmp11)\n",
    "    #print(df2['arriveTime2'])\n",
    "    tmp11 = tmp11 < df2['arriveTime2']\n",
    "    #print(tmp11)\n",
    "    tmp1 =  tmp11  \n",
    "    \n",
    "    df3 = df2[tmp1]\n",
    "    print(\"df3 shape:\",df3.shape)\n",
    "    print(\"df2 origin Shape:\",df2.shape)\n",
    "    df3.to_csv(\"tmpForAnalyzing7.csv\")\n",
    "    plt.show()\n",
    "    df3[\"vehLaneID\"].hist(figsize=(15, 4)) \n",
    "   '''\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "tmp = (xyDataTmp[\"origin speedFlag\"] - xyDataTmp[\"predicted Labels By MCS\"] >0) \n",
    "\n",
    "#analyzing1(tmp, xyDataTmp,xyOrigin)\n",
    "\n",
    "tmp = xyDataTmp[\"origin speedFlag\"] - xyDataTmp[\"predicted Labels By MCS\"]  >=3 \n",
    "#analyzing(tmp, xyDataTmp,xyOrigin)\n",
    "\n",
    "tmp = xyDataTmp[\"predicted Labels By MCS\"] - xyDataTmp[\"origin speedFlag\"]>0\n",
    "analyzing2(tmp, xyDataTmp,xyOrigin)\n",
    "      \n",
    "tmp = xyDataTmp[\"predicted Labels By MCS\"] - xyDataTmp[\"origin speedFlag\"]>=3\n",
    "#analyzing2(tmp, xyDataTmp,xyOrigin)\n",
    "\n",
    "#手动修改\n",
    "\n",
    "    \n",
    "      \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46c252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T10:10:55.559866Z",
     "start_time": "2023-01-27T10:10:53.398090Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c877a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T09:04:50.440620Z",
     "start_time": "2023-01-28T09:04:50.239643Z"
    }
   },
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b3f88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-29T03:05:13.857103Z",
     "start_time": "2023-01-29T03:05:13.853125Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "timestr= datetime.now()\n",
    "print(timestr)\n",
    "\n",
    "!conda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281fc84-e6c1-4671-9323-70c1fdbb7210",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf tmp*.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor23py36gpu",
   "language": "python",
   "name": "tensor23py36gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b27f224da048d073ae2b306b979c73d2559eaa860bf21b792b51024f42769a7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
