{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1si0P8tFT072L1JA6f9OU0NxRgm0PdYdm",
      "authorship_tag": "ABX9TyOtWfOD82+5tWcgrSPQwSJR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukeliuli/SUMONBDT/blob/main/mainNBDT1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd  /content/drive/MyDrive/SUMONBDT\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torchvision.models as models\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "\n",
        "import time\n",
        "from collections import namedtuple\n",
        "import resnet\n",
        "import os\n",
        "\n",
        "\n",
        "outputNow =torch.Tensor()\n",
        "inputNow =torch.Tensor()\n",
        "fc_weight =torch.Tensor()\n",
        "fc_bias =torch.Tensor()\n",
        "\n",
        "\n",
        "def hook2(module, input, output):\n",
        "    '''把这层的输出拷贝到features中'''\n",
        "    global inputNow\n",
        "    global outputNow\n",
        "    global fc_weight\n",
        "    global fc_bias\n",
        "    \n",
        "    #print(input[0].shape)\n",
        "    inputNow=input[0].clone()\n",
        "    outputNow=output.clone()\n",
        "    fc_weight=module.weight.clone()\n",
        "    fc_bias=module.bias.clone()\n",
        "    #inputNow =  input\n",
        "    \n",
        "\n",
        "    \n",
        "    #print(output.data)\n",
        "    #print(input[0].data)\n",
        "    #print(input[0].data.size())\n",
        "    #print(module.weight)\n",
        "    #print(module.bias)\n",
        "    #print(inputNow)\n",
        "    #print(outputNow)\n",
        "\n",
        "    #computeLoss()\n",
        "\n",
        "\n",
        "\n",
        "def computeLoss(labels):\n",
        "\n",
        "  global inputNow\n",
        "  global outputNow\n",
        "  global fc_weight\n",
        "  global fc_bias\n",
        "  global myLoss\n",
        "\n",
        "  ###简单的分级识别\n",
        "  #GOD: 1. plane,bird\n",
        "  #     2. deer,dog,horse,cat,frog\n",
        "  #     3. car,truck,ship\n",
        "  #classes = ('plane' 0, 'car' 1, 'bird' 2, 'cat' 3,\n",
        "  #           'deer' 4, 'dog' 5, 'frog' 6, 'horse' 7 , 'ship' 8,  'truck' 9)\n",
        "  #layerA         0 \n",
        "  #layerB      0 (likebird)       1 (likecat)         2(likecar)\n",
        "  #layerC  plane 0,bird2     deer4,dog5,horse7,cat3,frog6        car 1,truck9,ship8\n",
        "  myDict =torch.LongTensor([0,2,0,1,1,1,1,1,2,2])\n",
        "  #print(myDict)\n",
        "  w0 = fc_weight  # 10x512\n",
        "  bias = fc_bias  # 10x512\n",
        "  wA_B = {}\n",
        "  bA_B = {}\n",
        "\n",
        "\n",
        "  wA_B['0_1_likebird'] =  (w0[0]+w0[2])/2\n",
        "  bA_B['0_1_likebird'] = (bias[0]+bias[2])/2\n",
        "\n",
        "\n",
        "  wA_B['0_2_likecat'] = (w0[3]+w0[4]+w0[5]+w0[6]+w0[7])/6\n",
        "  bA_B['0_2_likecat'] = (bias[3]+bias[4]+bias[5]+bias[6]+bias[7])/6\n",
        "\n",
        "  wA_B['0_3_likecar'] = (w0[1]+w0[8]+w0[9])/3\n",
        "  bA_B['0_3_likecar'] = (bias[1]+bias[8]+bias[9])/3\n",
        "  wTmp = wA_B['0_1_likebird'].unsqueeze(1)\n",
        "  s1 =torch.mm(inputNow,wTmp)+bA_B['0_1_likebird']\n",
        "  wTmp = wA_B['0_2_likecat'].unsqueeze(1)\n",
        "  s2 =torch.mm(inputNow,wTmp)+bA_B['0_2_likecat']\n",
        "  wTmp = wA_B['0_3_likecar'].unsqueeze(1)\n",
        "  s3 =torch.mm(inputNow,wTmp)+bA_B['0_3_likecar']\n",
        "\n",
        "  s4 = torch.cat((s1,s2,s3),1)\n",
        "  \n",
        "  #_, predicted = s4.max(1)\n",
        " \n",
        "  #print(\"s1:\",s1)\n",
        "  #print(\"s2:\",s2)\n",
        "  #print(\"s3:\",s3)\n",
        "  #print(\"s4:\",s4)\n",
        "  #print(\"myPredicted:\",predicted)\n",
        "\n",
        "  myLabels = labels.clone().detach()\n",
        "  #print(\"mylabels1:\",myLabels)\n",
        "  for i,data in enumerate(labels):\n",
        "    #tmp = data.numpy()\n",
        "    #print(data)\n",
        "    myLabels[i] = myDict[data]\n",
        "  #print(\"mylabels2:\",myLabels)\n",
        "\n",
        "  myLoss = criterion(s4,myLabels)\n",
        "  #print(\"myLoss:\",myLoss)\n",
        "  return myLoss,myLabels\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "#########################################################################################\n",
        "\n",
        "transform1 = transforms.Compose(\n",
        "    [transforms.RandomCrop(32, padding=4),  # 先四周填充0，在吧图像随机裁剪成32*32\n",
        "     transforms.RandomHorizontalFlip(),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])  # R,G,B每层的归一化用到的均值和方差\n",
        "\n",
        "\n",
        "#transform1 = transforms.Compose(\n",
        "#    [transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])  # R,G,B每层的归一化用到的均值和方差\n",
        "\n",
        "batch_sizeV = 512\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform1)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=batch_sizeV, shuffle=True, num_workers=2)\n",
        "\n",
        "batch_sizeV = 512\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform1)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=batch_sizeV, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "startEpoch = 0\n",
        "\n",
        "#resnet18 = models.resnet18(pretrained=False)#采用torchvision的模型，无法达到94%的正确率，最多88%\n",
        "resnet18 = resnet.resnet18(num_classes=10)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device =\"cpu\"\n",
        "if device == 'cuda':\n",
        "  resnet18 = torch.nn.DataParallel(resnet18)\n",
        "  cudnn.benchmark = True\n",
        "print(device)\n",
        "resnet18 = resnet18.to(device)\n",
        "\n",
        "\n",
        "modelPathName = \"/content/drive/MyDrive/SUMONBDT/trainedModes/resnet18End_accuray95.modeparams\"\n",
        "modelPath = \"/content/drive/MyDrive/SUMONBDT/trainedModes/\"\n",
        "params = torch.load(modelPathName, map_location=device)\n",
        "resnet18.load_state_dict(params[\"net\"])\n",
        "startEpoch = params[\"epoch\"]\n",
        "#print(params)\n",
        "\n",
        "\n",
        "#######################################################################\n",
        "##基于hook和name_parameters.看输出数据类型和值,网络权值\n",
        "for name, parameters in resnet18.named_parameters():\n",
        "    #print(name, ':', parameters.size())\n",
        "    params[name] = parameters.detach()\n",
        "#print(params[\"fc.bias\"])\n",
        "handle = resnet18.fc.register_forward_hook(hook2)\n",
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next()\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "outputs = resnet18(images)\n",
        "_, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "loss = criterion(outputs,labels)\n",
        "myLoss,myLabels = computeLoss(labels)\n",
        "#print(outputs.data.numpy())\n",
        "print(\"inputNow:\",inputNow)\n",
        "print(\"outputNow:\",outputNow)\n",
        "print(\"predicted:\",predicted)\n",
        "print(\"labels:\",labels)\n",
        "print(\"myLabels:\",myLabels)\n",
        "print(\"loss:\",loss)\n",
        "print(\"myLoss:\",myLoss)\n",
        "\n",
        "handle.remove()\n",
        "\n",
        "##########################################################################################\n",
        "###开始训练\n",
        "print(\"##开始训练################################################################################\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device =\"cpu\"\n",
        "if device == 'cuda':\n",
        "  resnet18 = torch.nn.DataParallel(resnet18)\n",
        "  cudnn.benchmark = True\n",
        "print(device)\n",
        "resnet18 = resnet18.to(device)\n",
        "\n",
        "# trainloader\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(resnet18.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "startEpoc = 0\n",
        "epochs = 1000\n",
        "resnet18.train()\n",
        "print(\"start training\")\n",
        "\n",
        "handle = resnet18.fc.register_forward_hook(hook2)\n",
        "\n",
        "for epoch in range(startEpoch, epochs):  # 多批次循环\n",
        "\n",
        "     resnet18.train()\n",
        "     running_loss = 0.0\n",
        "     time_start = time.time()\n",
        "     #learning rate 不变\n",
        "#     #adjust_learning_rate(optimizer, epoch, epochs, trainloader, batch_sizeV)\n",
        "     total = 0\n",
        "     correct = 0\n",
        "     for i, data in enumerate(trainloader, 0):\n",
        "         # 获取输入\n",
        "         #print(epoch,i)\n",
        "         inputs, labels = data\n",
        "\n",
        "         inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "         # 梯度置0\n",
        "         optimizer.zero_grad()\n",
        "\n",
        "         # 正向传播，反向传播，优化a\n",
        "         outputs = resnet18(inputs)\n",
        "\n",
        "         loss = criterion(outputs, labels)\n",
        "\n",
        "         ###############\n",
        "         myLoss,myLabels = computeLoss(labels)\n",
        "         loss = loss+0.1*myLoss\n",
        "         loss.backward()\n",
        "         optimizer.step()\n",
        "\n",
        "         _, predicted = outputs.max(1)\n",
        "         total += labels.size(0)\n",
        "         correct += predicted.eq(labels).sum().item()\n",
        "         #print(\"myLoss:\",myLoss)\n",
        "         #print(\"myLabels:\",myLabels)\n",
        "         #print(\"Loss:\",loss)\n",
        "         #print(\"labels:\",labels)\n",
        "         #print(\"i,epoch:\",i,epoch)\n",
        "         # 打印状态信息\n",
        "         running_loss += loss.item()\n",
        "         if i % 20 == 19:  # 每200批次打印一次\n",
        "             time_end = time.time()\n",
        "             print('one 20 batch totally time cost %.3f' %(time_end-time_start))\n",
        "             print(\"batchIndex %d |trainLen %d | Loss: %.3f | Acc: %.3f | correct, total: (%d,%d)\" % (\n",
        "                 i, len(trainloader), running_loss/(i+1), 100.*correct/total, correct, total))\n",
        "\n",
        "     time_end = time.time()\n",
        "     print('epoch %d totally time cost %.3f' % (epoch, time_end-time_start))\n",
        "     state = {\"net\": resnet18.state_dict(), \"optimizer\": optimizer.state_dict(), \"epoch\": epoch}\n",
        "     modelPath = \"/content/drive/MyDrive/SUMONBDT/trainedModes\"\n",
        "     modelPathNameTmp = modelPath+\"/nbdt_resnet18_\"+str(epoch)+\".modeparams\"\n",
        "     torch.save(state, modelPathNameTmp)\n",
        "     params = torch.load(modelPathNameTmp)\n",
        "     resnet18.load_state_dict(params[\"net\"])\n",
        "     optimizer.load_state_dict(params[\"optimizer\"])\n",
        "     #evalCifar(testloader, resnet18, classes)\n",
        "handle.remove()\n",
        "print('Finished Training')\n",
        "\n",
        "#有两种方法，一种只保存参数，一种全保存，后者简单但存储量大，我用的是前者\n",
        "# # 保存和加载整个模型\n",
        "state = {\"net\": resnet18.state_dict(), \"optimizer\": optimizer.state_dict(), \"epoch\": epoch}\n",
        "modelPath = \"/content/drive/MyDrive/SUMONBDT/trainedModes\"\n",
        "modelPathName = modelPath+\"/nbdt_resnet18_End\"+\".modeparams\"\n",
        "torch.save(state, modelPathName)\n",
        "params = torch.load(modelPathName)\n",
        "resnet18.load_state_dict(params['net'])\n",
        "optimizer.load_state_dict(params['optimizer'])\n",
        "\n",
        "# # 仅保存和加载模型参数(推荐使用)\n",
        "#torch.save(resnet18.state_dict(), './trainedModes/resnet18params.pkl')\n",
        "#resnet18.load_state_dict(torch.load('./trainedModes/resnet18params.pkl'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWSjWG0Qmpdm",
        "outputId": "0cba85dd-47db-4428-ddc7-9b6825348064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/SUMONBDT\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "cuda\n",
            "inputNow: tensor([[0.0000, 0.0131, 0.0018,  ..., 0.0000, 0.0659, 0.0000],\n",
            "        [0.0024, 0.0004, 0.0000,  ..., 0.0000, 0.0798, 0.0000],\n",
            "        [0.0031, 0.0000, 0.0017,  ..., 0.0509, 0.0604, 0.0027],\n",
            "        ...,\n",
            "        [0.0018, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.0007],\n",
            "        [0.0000, 0.0000, 0.0009,  ..., 0.0010, 0.0004, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0007,  ..., 0.0006, 0.0000, 0.0031]],\n",
            "       device='cuda:0', grad_fn=<CloneBackward0>)\n",
            "outputNow: tensor([[-1.0832, -1.1801, -0.6986,  ..., -0.9776, -1.2152, -1.3197],\n",
            "        [-0.4378, -0.4508, -1.0667,  ..., -1.0288,  7.9437, -0.8195],\n",
            "        [-0.7598,  3.4552, -1.3036,  ..., -1.2878,  5.5496, -0.5904],\n",
            "        ...,\n",
            "        [-1.2738, -1.2434, -0.7754,  ..., -0.8762, -1.0965, -1.2124],\n",
            "        [-1.2987, -1.1456, -0.8463,  ..., -0.7089, -1.0954, -1.2595],\n",
            "        [-1.1910, -1.0655, -0.7273,  ..., -1.1934, -1.0467, -1.0906]],\n",
            "       device='cuda:0', grad_fn=<CloneBackward0>)\n",
            "predicted: tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6, 7, 0, 4, 9,\n",
            "        5, 2, 4, 0, 9, 6, 6, 5, 4, 5, 9, 2, 4, 1, 9, 5, 4, 6, 5, 6, 0, 9, 3, 9,\n",
            "        7, 6, 9, 8, 0, 3, 8, 8, 7, 7, 5, 3, 7, 5, 6, 3, 6, 2, 1, 2, 3, 7, 2, 6,\n",
            "        8, 8, 0, 2, 9, 3, 3, 8, 8, 1, 1, 7, 2, 5, 2, 7, 8, 9, 0, 3, 8, 6, 4, 6,\n",
            "        6, 0, 0, 7, 4, 5, 6, 3, 1, 1, 3, 6, 8, 7, 4, 0, 6, 2, 1, 3, 0, 4, 2, 7,\n",
            "        8, 3, 1, 2, 8, 0, 8, 3, 3, 2, 4, 1, 8, 9, 1, 2, 9, 7, 2, 8, 6, 5, 6, 3,\n",
            "        8, 7, 6, 5, 5, 2, 8, 9, 6, 0, 0, 5, 2, 9, 5, 4, 2, 1, 6, 6, 0, 4, 8, 4,\n",
            "        5, 0, 9, 9, 9, 8, 9, 9, 3, 7, 2, 0, 0, 5, 2, 3, 3, 8, 6, 3, 4, 0, 5, 8,\n",
            "        0, 1, 7, 2, 8, 8, 7, 8, 5, 1, 8, 7, 1, 3, 0, 5, 7, 9, 7, 4, 5, 9, 8, 0,\n",
            "        7, 9, 8, 2, 7, 6, 9, 4, 3, 9, 0, 4, 7, 6, 5, 1, 3, 8, 8, 0, 4, 0, 5, 5,\n",
            "        1, 1, 8, 9, 0, 3, 1, 9, 2, 2, 5, 3, 9, 9, 4, 0, 3, 0, 0, 9, 8, 1, 5, 7,\n",
            "        0, 8, 2, 4, 7, 0, 2, 3, 6, 3, 8, 5, 0, 2, 4, 3, 9, 0, 6, 1, 0, 9, 1, 4,\n",
            "        7, 9, 1, 2, 6, 9, 3, 4, 6, 0, 0, 6, 6, 6, 3, 2, 6, 1, 8, 2, 1, 2, 8, 6,\n",
            "        0, 0, 4, 0, 7, 7, 5, 5, 3, 5, 2, 3, 4, 1, 7, 5, 4, 6, 1, 9, 3, 6, 6, 9,\n",
            "        3, 8, 0, 7, 2, 6, 2, 5, 8, 5, 4, 6, 8, 9, 9, 1, 0, 2, 2, 2, 3, 2, 8, 0,\n",
            "        9, 5, 8, 1, 9, 4, 1, 8, 4, 1, 4, 7, 9, 4, 2, 7, 0, 7, 8, 6, 6, 9, 0, 9,\n",
            "        5, 8, 7, 2, 2, 5, 1, 2, 6, 2, 1, 6, 2, 3, 0, 3, 9, 8, 7, 8, 8, 5, 0, 1,\n",
            "        8, 2, 7, 9, 3, 6, 1, 9, 0, 7, 3, 7, 4, 5, 8, 0, 2, 9, 3, 4, 0, 6, 2, 5,\n",
            "        3, 3, 3, 7, 2, 5, 3, 1, 1, 4, 9, 9, 5, 7, 5, 0, 2, 2, 2, 9, 7, 3, 9, 4,\n",
            "        4, 5, 4, 6, 5, 6, 1, 4, 3, 4, 4, 3, 7, 8, 3, 7, 8, 0, 5, 7, 6, 0, 3, 4,\n",
            "        8, 6, 8, 3, 5, 9, 9, 9, 5, 0, 1, 0, 8, 1, 1, 8, 0, 2, 2, 0, 4, 6, 5, 4,\n",
            "        9, 4, 7, 9, 9, 4, 5, 6], device='cuda:0')\n",
            "labels: tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6, 7, 0, 4, 9,\n",
            "        5, 2, 4, 0, 9, 6, 6, 5, 4, 5, 9, 2, 4, 1, 9, 5, 4, 6, 5, 6, 0, 9, 3, 9,\n",
            "        7, 6, 9, 8, 0, 3, 8, 8, 7, 7, 4, 6, 7, 3, 6, 3, 6, 2, 1, 2, 3, 7, 2, 6,\n",
            "        8, 8, 0, 2, 9, 3, 3, 8, 8, 1, 1, 7, 2, 5, 2, 7, 8, 9, 0, 3, 8, 6, 4, 6,\n",
            "        6, 0, 0, 7, 4, 5, 6, 3, 1, 1, 3, 6, 8, 7, 4, 0, 6, 2, 1, 3, 0, 4, 2, 7,\n",
            "        8, 3, 1, 2, 8, 0, 8, 3, 5, 2, 4, 1, 8, 9, 1, 2, 9, 7, 2, 9, 6, 5, 6, 3,\n",
            "        8, 7, 6, 2, 5, 2, 8, 9, 6, 0, 0, 5, 2, 9, 5, 4, 2, 1, 6, 6, 8, 4, 8, 4,\n",
            "        5, 0, 9, 9, 9, 8, 9, 9, 3, 7, 5, 0, 0, 5, 2, 2, 3, 8, 6, 3, 4, 0, 5, 8,\n",
            "        0, 1, 7, 2, 8, 8, 7, 8, 5, 1, 8, 7, 1, 3, 0, 5, 7, 9, 7, 4, 5, 9, 8, 0,\n",
            "        7, 9, 8, 2, 7, 6, 9, 4, 3, 9, 6, 4, 7, 6, 5, 1, 5, 8, 8, 0, 4, 0, 5, 5,\n",
            "        1, 1, 8, 9, 0, 3, 1, 9, 2, 2, 5, 3, 9, 9, 4, 0, 3, 0, 0, 9, 8, 1, 5, 7,\n",
            "        0, 8, 2, 4, 7, 0, 2, 3, 6, 3, 8, 5, 0, 3, 4, 3, 9, 0, 6, 1, 0, 9, 1, 0,\n",
            "        7, 9, 1, 2, 6, 9, 3, 4, 6, 0, 0, 6, 6, 6, 3, 2, 6, 1, 8, 2, 1, 6, 8, 6,\n",
            "        8, 0, 4, 0, 7, 7, 5, 5, 3, 5, 2, 3, 4, 1, 7, 5, 4, 6, 1, 9, 3, 6, 6, 9,\n",
            "        3, 8, 0, 7, 2, 6, 2, 5, 8, 5, 4, 6, 8, 9, 9, 1, 0, 2, 2, 7, 3, 2, 8, 0,\n",
            "        9, 5, 8, 1, 9, 4, 1, 3, 8, 1, 4, 7, 9, 4, 2, 7, 0, 7, 0, 6, 6, 9, 0, 9,\n",
            "        2, 8, 7, 2, 2, 5, 1, 2, 6, 2, 9, 6, 2, 3, 0, 3, 9, 8, 7, 8, 8, 4, 0, 1,\n",
            "        8, 2, 7, 9, 3, 6, 1, 9, 0, 7, 3, 7, 4, 5, 0, 0, 2, 9, 3, 4, 0, 6, 2, 5,\n",
            "        3, 7, 3, 7, 2, 5, 3, 1, 1, 4, 9, 9, 5, 7, 5, 0, 2, 2, 2, 9, 7, 3, 9, 4,\n",
            "        3, 5, 4, 6, 5, 6, 1, 4, 3, 4, 4, 3, 7, 8, 3, 7, 8, 0, 5, 7, 6, 0, 5, 4,\n",
            "        8, 6, 8, 5, 5, 9, 9, 9, 5, 0, 1, 0, 8, 1, 1, 8, 0, 2, 2, 0, 4, 6, 5, 4,\n",
            "        9, 4, 7, 9, 9, 4, 5, 6], device='cuda:0')\n",
            "myLabels: tensor([1, 2, 2, 0, 1, 1, 2, 1, 1, 2, 0, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 0, 1, 2,\n",
            "        1, 0, 1, 0, 2, 1, 1, 1, 1, 1, 2, 0, 1, 2, 2, 1, 1, 1, 1, 1, 0, 2, 1, 2,\n",
            "        1, 1, 2, 2, 0, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 0, 1, 1, 0, 1,\n",
            "        2, 2, 0, 0, 2, 1, 1, 2, 2, 2, 2, 1, 0, 1, 0, 1, 2, 2, 0, 1, 2, 1, 1, 1,\n",
            "        1, 0, 0, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 0, 1, 0, 2, 1, 0, 1, 0, 1,\n",
            "        2, 1, 2, 0, 2, 0, 2, 1, 1, 0, 1, 2, 2, 2, 2, 0, 2, 1, 0, 2, 1, 1, 1, 1,\n",
            "        2, 1, 1, 0, 1, 0, 2, 2, 1, 0, 0, 1, 0, 2, 1, 1, 0, 2, 1, 1, 2, 1, 2, 1,\n",
            "        1, 0, 2, 2, 2, 2, 2, 2, 1, 1, 1, 0, 0, 1, 0, 0, 1, 2, 1, 1, 1, 0, 1, 2,\n",
            "        0, 2, 1, 0, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 0, 1, 1, 2, 1, 1, 1, 2, 2, 0,\n",
            "        1, 2, 2, 0, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 0, 1, 0, 1, 1,\n",
            "        2, 2, 2, 2, 0, 1, 2, 2, 0, 0, 1, 1, 2, 2, 1, 0, 1, 0, 0, 2, 2, 2, 1, 1,\n",
            "        0, 2, 0, 1, 1, 0, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 2, 0, 1, 2, 0, 2, 2, 0,\n",
            "        1, 2, 2, 0, 1, 2, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 2, 2, 0, 2, 1, 2, 1,\n",
            "        2, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2,\n",
            "        1, 2, 0, 1, 0, 1, 0, 1, 2, 1, 1, 1, 2, 2, 2, 2, 0, 0, 0, 1, 1, 0, 2, 0,\n",
            "        2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 0, 1, 0, 1, 0, 1, 1, 2, 0, 2,\n",
            "        0, 2, 1, 0, 0, 1, 2, 0, 1, 0, 2, 1, 0, 1, 0, 1, 2, 2, 1, 2, 2, 1, 0, 2,\n",
            "        2, 0, 1, 2, 1, 1, 2, 2, 0, 1, 1, 1, 1, 1, 0, 0, 0, 2, 1, 1, 0, 1, 0, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 0, 0, 0, 0, 2, 1, 1, 2, 1,\n",
            "        1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 0, 1, 1, 1, 0, 1, 1,\n",
            "        2, 1, 2, 1, 1, 2, 2, 2, 1, 0, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 1,\n",
            "        2, 1, 1, 2, 2, 1, 1, 1], device='cuda:0')\n",
            "loss: tensor(0.2213, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "myLoss: tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "##开始训练################################################################################\n",
            "cuda\n",
            "start training\n",
            "one 20 batch totally time cost 7.391\n",
            "batchIndex 19 |trainLen 98 | Loss: 0.018 | Acc: 100.000 | correct, total: (10240,10240)\n",
            "one 20 batch totally time cost 14.244\n",
            "batchIndex 39 |trainLen 98 | Loss: 0.016 | Acc: 100.000 | correct, total: (20480,20480)\n",
            "one 20 batch totally time cost 21.128\n",
            "batchIndex 59 |trainLen 98 | Loss: 0.015 | Acc: 100.000 | correct, total: (30720,30720)\n",
            "one 20 batch totally time cost 28.071\n",
            "batchIndex 79 |trainLen 98 | Loss: 0.014 | Acc: 100.000 | correct, total: (40960,40960)\n",
            "epoch 268 totally time cost 34.186\n",
            "one 20 batch totally time cost 7.193\n",
            "batchIndex 19 |trainLen 98 | Loss: 0.010 | Acc: 100.000 | correct, total: (10240,10240)\n",
            "one 20 batch totally time cost 14.055\n",
            "batchIndex 39 |trainLen 98 | Loss: 0.010 | Acc: 100.000 | correct, total: (20480,20480)\n",
            "one 20 batch totally time cost 20.905\n",
            "batchIndex 59 |trainLen 98 | Loss: 0.010 | Acc: 100.000 | correct, total: (30720,30720)\n",
            "one 20 batch totally time cost 27.754\n",
            "batchIndex 79 |trainLen 98 | Loss: 0.009 | Acc: 100.000 | correct, total: (40960,40960)\n",
            "epoch 269 totally time cost 33.883\n",
            "one 20 batch totally time cost 7.184\n",
            "batchIndex 19 |trainLen 98 | Loss: 0.008 | Acc: 100.000 | correct, total: (10240,10240)\n",
            "one 20 batch totally time cost 14.074\n",
            "batchIndex 39 |trainLen 98 | Loss: 0.008 | Acc: 99.995 | correct, total: (20479,20480)\n",
            "one 20 batch totally time cost 20.920\n",
            "batchIndex 59 |trainLen 98 | Loss: 0.008 | Acc: 99.997 | correct, total: (30719,30720)\n",
            "one 20 batch totally time cost 27.777\n",
            "batchIndex 79 |trainLen 98 | Loss: 0.008 | Acc: 99.998 | correct, total: (40959,40960)\n",
            "epoch 270 totally time cost 33.877\n",
            "one 20 batch totally time cost 7.235\n",
            "batchIndex 19 |trainLen 98 | Loss: 0.007 | Acc: 100.000 | correct, total: (10240,10240)\n",
            "one 20 batch totally time cost 14.076\n",
            "batchIndex 39 |trainLen 98 | Loss: 0.007 | Acc: 100.000 | correct, total: (20480,20480)\n",
            "one 20 batch totally time cost 20.930\n",
            "batchIndex 59 |trainLen 98 | Loss: 0.007 | Acc: 100.000 | correct, total: (30720,30720)\n",
            "one 20 batch totally time cost 27.795\n",
            "batchIndex 79 |trainLen 98 | Loss: 0.007 | Acc: 100.000 | correct, total: (40960,40960)\n",
            "epoch 271 totally time cost 33.909\n",
            "one 20 batch totally time cost 7.175\n",
            "batchIndex 19 |trainLen 98 | Loss: 0.006 | Acc: 100.000 | correct, total: (10240,10240)\n",
            "one 20 batch totally time cost 14.102\n",
            "batchIndex 39 |trainLen 98 | Loss: 0.006 | Acc: 99.995 | correct, total: (20479,20480)\n",
            "one 20 batch totally time cost 20.970\n",
            "batchIndex 59 |trainLen 98 | Loss: 0.006 | Acc: 99.993 | correct, total: (30718,30720)\n",
            "one 20 batch totally time cost 27.824\n",
            "batchIndex 79 |trainLen 98 | Loss: 0.006 | Acc: 99.995 | correct, total: (40958,40960)\n",
            "epoch 272 totally time cost 33.914\n",
            "one 20 batch totally time cost 7.173\n",
            "batchIndex 19 |trainLen 98 | Loss: 0.006 | Acc: 100.000 | correct, total: (10240,10240)\n",
            "one 20 batch totally time cost 14.066\n",
            "batchIndex 39 |trainLen 98 | Loss: 0.005 | Acc: 100.000 | correct, total: (20480,20480)\n",
            "one 20 batch totally time cost 20.924\n",
            "batchIndex 59 |trainLen 98 | Loss: 0.005 | Acc: 100.000 | correct, total: (30720,30720)\n",
            "one 20 batch totally time cost 27.775\n",
            "batchIndex 79 |trainLen 98 | Loss: 0.005 | Acc: 100.000 | correct, total: (40960,40960)\n",
            "epoch 273 totally time cost 33.872\n",
            "one 20 batch totally time cost 7.174\n",
            "batchIndex 19 |trainLen 98 | Loss: 0.005 | Acc: 100.000 | correct, total: (10240,10240)\n",
            "one 20 batch totally time cost 14.042\n",
            "batchIndex 39 |trainLen 98 | Loss: 0.005 | Acc: 100.000 | correct, total: (20480,20480)\n",
            "one 20 batch totally time cost 20.879\n",
            "batchIndex 59 |trainLen 98 | Loss: 0.005 | Acc: 100.000 | correct, total: (30720,30720)\n",
            "one 20 batch totally time cost 27.709\n",
            "batchIndex 79 |trainLen 98 | Loss: 0.005 | Acc: 100.000 | correct, total: (40960,40960)\n",
            "epoch 274 totally time cost 33.809\n",
            "one 20 batch totally time cost 7.195\n",
            "batchIndex 19 |trainLen 98 | Loss: 0.005 | Acc: 100.000 | correct, total: (10240,10240)\n",
            "one 20 batch totally time cost 14.038\n",
            "batchIndex 39 |trainLen 98 | Loss: 0.005 | Acc: 100.000 | correct, total: (20480,20480)\n",
            "one 20 batch totally time cost 20.854\n",
            "batchIndex 59 |trainLen 98 | Loss: 0.005 | Acc: 99.997 | correct, total: (30719,30720)\n",
            "one 20 batch totally time cost 27.688\n",
            "batchIndex 79 |trainLen 98 | Loss: 0.004 | Acc: 99.998 | correct, total: (40959,40960)\n",
            "epoch 275 totally time cost 33.808\n",
            "one 20 batch totally time cost 7.189\n",
            "batchIndex 19 |trainLen 98 | Loss: 0.004 | Acc: 100.000 | correct, total: (10240,10240)\n",
            "one 20 batch totally time cost 14.071\n",
            "batchIndex 39 |trainLen 98 | Loss: 0.004 | Acc: 100.000 | correct, total: (20480,20480)\n",
            "one 20 batch totally time cost 20.931\n",
            "batchIndex 59 |trainLen 98 | Loss: 0.004 | Acc: 100.000 | correct, total: (30720,30720)\n",
            "one 20 batch totally time cost 27.790\n",
            "batchIndex 79 |trainLen 98 | Loss: 0.004 | Acc: 100.000 | correct, total: (40960,40960)\n",
            "epoch 276 totally time cost 33.890\n",
            "one 20 batch totally time cost 7.234\n",
            "batchIndex 19 |trainLen 98 | Loss: 0.004 | Acc: 100.000 | correct, total: (10240,10240)\n",
            "one 20 batch totally time cost 14.094\n",
            "batchIndex 39 |trainLen 98 | Loss: 0.004 | Acc: 100.000 | correct, total: (20480,20480)\n",
            "one 20 batch totally time cost 20.941\n",
            "batchIndex 59 |trainLen 98 | Loss: 0.004 | Acc: 100.000 | correct, total: (30720,30720)\n",
            "one 20 batch totally time cost 27.796\n",
            "batchIndex 79 |trainLen 98 | Loss: 0.004 | Acc: 100.000 | correct, total: (40960,40960)\n",
            "epoch 277 totally time cost 33.900\n",
            "one 20 batch totally time cost 7.228\n",
            "batchIndex 19 |trainLen 98 | Loss: 0.004 | Acc: 100.000 | correct, total: (10240,10240)\n",
            "one 20 batch totally time cost 14.092\n",
            "batchIndex 39 |trainLen 98 | Loss: 0.004 | Acc: 100.000 | correct, total: (20480,20480)\n",
            "one 20 batch totally time cost 20.943\n",
            "batchIndex 59 |trainLen 98 | Loss: 0.004 | Acc: 100.000 | correct, total: (30720,30720)\n",
            "one 20 batch totally time cost 27.775\n",
            "batchIndex 79 |trainLen 98 | Loss: 0.004 | Acc: 100.000 | correct, total: (40960,40960)\n",
            "epoch 278 totally time cost 33.864\n",
            "one 20 batch totally time cost 7.248\n",
            "batchIndex 19 |trainLen 98 | Loss: 0.004 | Acc: 100.000 | correct, total: (10240,10240)\n",
            "one 20 batch totally time cost 14.138\n",
            "batchIndex 39 |trainLen 98 | Loss: 0.004 | Acc: 100.000 | correct, total: (20480,20480)\n",
            "one 20 batch totally time cost 21.011\n",
            "batchIndex 59 |trainLen 98 | Loss: 0.004 | Acc: 100.000 | correct, total: (30720,30720)\n",
            "one 20 batch totally time cost 27.873\n",
            "batchIndex 79 |trainLen 98 | Loss: 0.004 | Acc: 100.000 | correct, total: (40960,40960)\n",
            "epoch 279 totally time cost 33.995\n",
            "one 20 batch totally time cost 7.275\n",
            "batchIndex 19 |trainLen 98 | Loss: 0.003 | Acc: 100.000 | correct, total: (10240,10240)\n",
            "one 20 batch totally time cost 14.177\n",
            "batchIndex 39 |trainLen 98 | Loss: 0.003 | Acc: 100.000 | correct, total: (20480,20480)\n",
            "one 20 batch totally time cost 21.015\n",
            "batchIndex 59 |trainLen 98 | Loss: 0.003 | Acc: 100.000 | correct, total: (30720,30720)\n",
            "one 20 batch totally time cost 27.911\n",
            "batchIndex 79 |trainLen 98 | Loss: 0.003 | Acc: 100.000 | correct, total: (40960,40960)\n",
            "epoch 280 totally time cost 34.010\n",
            "one 20 batch totally time cost 7.277\n",
            "batchIndex 19 |trainLen 98 | Loss: 0.003 | Acc: 100.000 | correct, total: (10240,10240)\n",
            "one 20 batch totally time cost 14.132\n",
            "batchIndex 39 |trainLen 98 | Loss: 0.003 | Acc: 100.000 | correct, total: (20480,20480)\n",
            "one 20 batch totally time cost 20.972\n",
            "batchIndex 59 |trainLen 98 | Loss: 0.003 | Acc: 100.000 | correct, total: (30720,30720)\n",
            "one 20 batch totally time cost 27.832\n",
            "batchIndex 79 |trainLen 98 | Loss: 0.003 | Acc: 100.000 | correct, total: (40960,40960)\n",
            "epoch 281 totally time cost 33.920\n",
            "one 20 batch totally time cost 7.248\n",
            "batchIndex 19 |trainLen 98 | Loss: 0.003 | Acc: 100.000 | correct, total: (10240,10240)\n",
            "one 20 batch totally time cost 14.157\n",
            "batchIndex 39 |trainLen 98 | Loss: 0.003 | Acc: 100.000 | correct, total: (20480,20480)\n",
            "one 20 batch totally time cost 20.979\n",
            "batchIndex 59 |trainLen 98 | Loss: 0.003 | Acc: 100.000 | correct, total: (30720,30720)\n",
            "one 20 batch totally time cost 27.828\n",
            "batchIndex 79 |trainLen 98 | Loss: 0.003 | Acc: 100.000 | correct, total: (40960,40960)\n",
            "epoch 282 totally time cost 33.913\n",
            "one 20 batch totally time cost 7.174\n",
            "batchIndex 19 |trainLen 98 | Loss: 0.003 | Acc: 100.000 | correct, total: (10240,10240)\n",
            "one 20 batch totally time cost 14.024\n",
            "batchIndex 39 |trainLen 98 | Loss: 0.003 | Acc: 100.000 | correct, total: (20480,20480)\n",
            "one 20 batch totally time cost 20.891\n",
            "batchIndex 59 |trainLen 98 | Loss: 0.003 | Acc: 100.000 | correct, total: (30720,30720)\n",
            "one 20 batch totally time cost 27.714\n",
            "batchIndex 79 |trainLen 98 | Loss: 0.003 | Acc: 100.000 | correct, total: (40960,40960)\n",
            "epoch 283 totally time cost 33.812\n"
          ]
        }
      ]
    }
  ]
}