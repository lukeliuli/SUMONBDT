{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f98b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data\n",
      "x.shape: (844538, 22) yOneHot.shape: (844538, 19)\n"
     ]
    }
   ],
   "source": [
    "#用于测试oneHot\n",
    "#也是第一步，读取数据\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "#[2,3,5,9]\n",
    "x1 = [0,0,0,0]\n",
    "x2 = [0,0,0,1]\n",
    "\n",
    "x3 = [1,1,1,2]\n",
    "x4 = [1,1,1,3]\n",
    "x5 = [1,1,2,4]\n",
    "x6 = [1,1,2,5]\n",
    "x7 = [1,2,3,6]\n",
    "x8 = [1,2,3,7]\n",
    "x9 = [1,2,4,8]\n",
    "X = [x1, x2, x3,x4,x5,x6,x7,x8,x9]\n",
    "enc.fit(X)\n",
    "#print(enc.transform(X).toarray())\n",
    "\n",
    "\n",
    "########################读写CSV,并转为oneHot\n",
    "file1 = \"./trainData/dataAllSim.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "\n",
    "y = enc.transform(y).toarray()\n",
    "\n",
    "print(\"x.shape:\",x.shape,\"yOneHot.shape:\",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03c551ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuli/miniconda3/envs/keras220CpuJupyter/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/liuli/miniconda3/envs/keras220CpuJupyter/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/liuli/miniconda3/envs/keras220CpuJupyter/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/liuli/miniconda3/envs/keras220CpuJupyter/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/liuli/miniconda3/envs/keras220CpuJupyter/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/liuli/miniconda3/envs/keras220CpuJupyter/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "560640/844538 [==================>...........] - ETA: 22s - loss: 0.1343 - mean_absolute_error: 0.0793"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-50bf99f0bb74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2560\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hmcnf.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras220CpuJupyter/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1361\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/envs/keras220CpuJupyter/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    262\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras220CpuJupyter/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2912\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2914\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2915\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2916\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras220CpuJupyter/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#1. 核心为keras220不是pytorch\n",
    "#2. 基于hmcnf\n",
    "#第二步，训练\n",
    "import model_hmcnf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "#hierarchy = [18, 80, 178, 142, 77, 4]\n",
    "hierarchy = [2,3,5,9]\n",
    "features_size = x.shape[1]\n",
    "label_size = y.shape[1]\n",
    "beta = 0.2\n",
    "dropout_rate=0.1\n",
    "relu_size=384\n",
    "\n",
    "\n",
    "\n",
    "def local_model(num_labels, dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(num_labels, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_model(dropout_rate, relu_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(relu_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    return model\n",
    "\n",
    "\n",
    "def sigmoid_model(label_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(label_size, activation='sigmoid',name=\"global\"))\n",
    "    return model\n",
    "\n",
    "features = layers.Input(shape=(features_size,))\n",
    "global_models = []\n",
    "local_models = []\n",
    "\n",
    "\n",
    "for i in range(len(hierarchy)):\n",
    "    if i == 0:\n",
    "        global_models.append(global_model(dropout_rate, relu_size)(features))\n",
    "    else:\n",
    "        global_models.append(global_model(dropout_rate, relu_size)(layers.concatenate([global_models[i-1], features])))\n",
    "\n",
    "p_glob = sigmoid_model(label_size)(global_models[-1])\n",
    "\n",
    "\n",
    "#显示只有全局模型的情况\n",
    "#modelTmp1 = tf.keras.Model(inputs=[features], outputs=[p_glob])\n",
    "#modelTmp1.summary()#\n",
    "#plot_model(modelTmp1, to_file='Flatten1.png', show_shapes=True)\n",
    "\n",
    "\n",
    "for i in range(len(hierarchy)):\n",
    "    local_models.append(local_model(hierarchy[i], dropout_rate, relu_size)(global_models[i]))\n",
    "    \n",
    "#显示只有局部局模型的情况(部分全局)\n",
    "p_loc = layers.concatenate(local_models)\n",
    "#modelTmp2 = tf.keras.Model(inputs=[features], outputs=[p_loc])\n",
    "#modelTmp2.summary()#\n",
    "#plot_model(modelTmp2, to_file='Flatten2.png', show_shapes=True)\n",
    "p_glob1 = layers.Lambda(lambda x: x*beta,name=\"global\")(p_glob)\n",
    "p_loc1 = layers.Lambda(lambda x: x*(1-beta),name=\"local\")(p_loc)\n",
    "\n",
    "labels = layers.add([p_glob1, p_loc1])\n",
    "\n",
    "model = tf.keras.Model(inputs=[features], outputs=[labels])\n",
    "plot_model(model, to_file='FlattenAll.png', show_shapes=True)\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['mae'])\n",
    "\n",
    "\n",
    "\n",
    "model.fit([x],[y],epochs=100, batch_size=2560)\n",
    "\n",
    "model.save(\"hmcnf.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7beeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#第三步，验证\n",
    "import model_hmcnf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#######################0.准备onehot\n",
    "enc = OneHotEncoder()\n",
    "#[2,3,5,9]\n",
    "x1 = [0,0,0,0]\n",
    "x2 = [0,0,0,1]\n",
    "\n",
    "x3 = [1,1,1,2]\n",
    "x4 = [1,1,1,3]\n",
    "x5 = [1,1,2,4]\n",
    "x6 = [1,1,2,5]\n",
    "x7 = [1,2,3,6]\n",
    "x8 = [1,2,3,7]\n",
    "x9 = [1,2,4,8]\n",
    "X = [x1, x2, x3,x4,x5,x6,x7,x8,x9]\n",
    "enc.fit(X)\n",
    "\n",
    "#######################2.准备数据\n",
    "        \n",
    "file1 = \"./trainData/dataAllSim.csv\"\n",
    "print(\"reading data\")\n",
    "xyDataTmp = pd.read_csv(file1)\n",
    "#print(xyDataTmp.info())\n",
    "xyData = np.array(xyDataTmp)\n",
    "\n",
    "x = xyData[:,0:22]\n",
    "y = xyData[:,22:26]\n",
    "ylabel = y\n",
    "y = enc.transform(y).toarray()\n",
    "\n",
    "\n",
    "\n",
    "#######################3.预测模型\n",
    "\n",
    "hierarchy = [2,3,5,9]\n",
    "features_size = x.shape[1]\n",
    "label_size = y.shape[1]\n",
    "beta = 0.2\n",
    "\n",
    "model_name =\"hmcnf.h5\" \n",
    "\n",
    "model = keras.models.load_model(model_name)\n",
    "y_out = model.predict([x], batch_size=2560)\n",
    "y_predict = np.where(y_out > 0.5, 1, 0)\n",
    "\n",
    "predict_ok = np.where(np.sum(y_predict - y, axis=1) == 0, 1, 0)\n",
    "\n",
    "\n",
    "print(\"validated {} , {} good out of {} samples\".format(model_name, np.sum(predict_ok), predict_ok.shape[0]))\n",
    "#######################3.层次预测预测模型\n",
    "print(\"3.层次预测预测模型\")\n",
    "y1 = np.where(y_out[:,0:2] > 0.5, 1, 0)\n",
    "y2 = np.where(y_out[:,2:5] > 0.5, 1, 0)\n",
    "y3 = np.where(y_out[:,5:10] > 0.5, 1, 0)\n",
    "y4 = np.where(y_out[:,10:19] > 0.5, 1, 0)\n",
    "for i in range(y4.shape[0]):\n",
    "    tmp1 = y1[i]\n",
    "    tmp2 = y2[i]\n",
    "    tmp3 = y3[i]\n",
    "    tmp4 = y4[i]\n",
    "    if sum(tmp1) == 0:\n",
    "        index=  np.argmax(tmp1)\n",
    "        y1[i,index]=1\n",
    "        \n",
    "    if sum(tmp2) == 0:\n",
    "        index=  np.argmax(tmp2)\n",
    "        y2[i,index]=1\n",
    "        \n",
    "    if sum(tmp3) == 0:\n",
    "        index=  np.argmax(tmp3)\n",
    "        y3[i,index]=1\n",
    "    \n",
    "    if sum(tmp4) == 0:\n",
    "        index=  np.argmax(tmp4)\n",
    "        y4[i,index]=1\n",
    "        #print(i,y4[i],index)\n",
    "y_predict = np.concatenate([y1,y2,y3,y4],axis=1)\n",
    "predict_ok = np.where(np.sum(y_predict - y, axis=1) == 0, 1, 0)\n",
    "print(\"validated {} , {} good out of {} samples\".format(model_name, np.sum(predict_ok), predict_ok.shape[0]))\n",
    "\n",
    "#######################4.评估层次模型\n",
    "#hierarchy = [2,3,5,9]\n",
    "ypredict = enc.inverse_transform(y_predict)\n",
    "\n",
    "##第一层，2\n",
    "print(\"###################################第一层，2\")\n",
    "h1_yp = ypredict[:,0]\n",
    "h1_yl = ylabel[:,0]\n",
    "tmp1 = classification_report(h1_yl,h1_yp)\n",
    "tmp2 = confusion_matrix(h1_yl,h1_yp,normalize='true')\n",
    "tmp3 = confusion_matrix(h1_yl,h1_yp,normalize='pred')\n",
    "print(tmp1)\n",
    "print(np.around(tmp2, decimals=3))\n",
    "print(np.around(tmp3, decimals=3))\n",
    "\n",
    "\n",
    "##第二层，3\n",
    "print(\"################################第二层，3\")\n",
    "h2_yp = ypredict[:,1]\n",
    "h2_yl = ylabel[:,1]\n",
    "tmp1 = classification_report(h2_yl,h2_yp)\n",
    "tmp2 = confusion_matrix(h2_yl,h2_yp,normalize='true')\n",
    "tmp3 = confusion_matrix(h2_yl,h2_yp,normalize='pred')\n",
    "print(tmp1)\n",
    "print(np.around(tmp2, decimals=3))\n",
    "print(np.around(tmp3, decimals=3))\n",
    "\n",
    "\n",
    "\n",
    "##第三层，5\n",
    "print(\"#############################第三层，5\")\n",
    "h3_yp = ypredict[:,2]\n",
    "h3_yl = ylabel[:,2]\n",
    "tmp1 = classification_report(h3_yl,h3_yp)\n",
    "tmp2 = confusion_matrix(h3_yl,h3_yp,normalize='true')\n",
    "tmp3 = confusion_matrix(h3_yl,h3_yp,normalize='pred')\n",
    "print(tmp1)\n",
    "print(np.around(tmp2, decimals=3))\n",
    "print(np.around(tmp3, decimals=3))\n",
    "\n",
    "\n",
    "##第四层，9\n",
    "print(\"#############################第四层，9\")\n",
    "h4_yp = ypredict[:,3]\n",
    "h4_yl = ylabel[:,3]\n",
    "tmp1 = classification_report(h4_yl,h4_yp)\n",
    "tmp2 = confusion_matrix(h4_yl,h4_yp,normalize='true')\n",
    "tmp3 = confusion_matrix(h4_yl,h4_yp,normalize='pred')\n",
    "print(tmp1)\n",
    "print(np.around(tmp2, decimals=3))\n",
    "print(np.around(tmp3, decimals=3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f76df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras220CpuJupyter]",
   "language": "python",
   "name": "conda-env-keras220CpuJupyter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
